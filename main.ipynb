{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbd9740e-606f-488c-adc6-450f320dae6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.12/site-packages (from seaborn) (2.3.1)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/site-packages (from seaborn) (2.3.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/site-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cae94c70-d297-474f-904a-ba2ebc62a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Union, Tuple, Set\n",
    "from enum import Enum\n",
    "import math\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "\n",
    "data_folder = \"data/\"\n",
    "jsons_path = \"data/*.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a807f4-ada1-46e4-82ac-d78a74271261",
   "metadata": {},
   "source": [
    "Création de la classe FullCellInfo récupérant l'ensmeble des informations d'une cellule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cfa8d38-bca5-4b48-9f44-50e11f331653",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FullCellInfo:\n",
    "    \"\"\"Structure complète d'une cellule Excel avec les informations disponibles dans le JSON Univer\"\"\"\n",
    "    # Contenu (disponible dans le JSON)\n",
    "    raw_value: Any = \"\"\n",
    "    cell_type: int = 0  # 1=text, 2=number, 3=formula (inféré)\n",
    "    formula: str = \"\"   # \"f\" - formule si présente (avec = au début)\n",
    "    \n",
    "    # Position\n",
    "    row: int = 0\n",
    "    col: int = 0\n",
    "    sheet_id: str = \"default\"\n",
    "    sheet_name: str = \"\"\n",
    "    \n",
    "    # Style complet (basé sur le format Univer optimisé)\n",
    "    style_id: str = \"\"  # Peut être vide si pas de style\n",
    "    \n",
    "    # Formatage de texte\n",
    "    bold: bool = False          # \"bl\": 1\n",
    "    italic: bool = False        # \"it\": 1  \n",
    "    underline: bool = False     # \"ul\": {\"s\": 1}\n",
    "    strike: bool = False        # \"st\": {\"s\": 1}\n",
    "    font_size: float = 11.0     # \"fs\": size (défaut Calibri 11)\n",
    "    font_family: str = \"Calibri\" # \"ff\": family\n",
    "    \n",
    "    # Couleurs\n",
    "    text_color: str = \"#000000\"     # \"cl\": {\"rgb\": \"#color\"}\n",
    "    background_color: str = \"#FFFFFF\"  # \"bg\": {\"rgb\": \"#color\"}\n",
    "    \n",
    "    # Bordures\n",
    "    border_top: int = 0      # \"bd\": {\"t\": {\"s\": value, \"cl\": {\"rgb\": \"#color\"}}}\n",
    "    border_bottom: int = 0   # \"bd\": {\"b\": {\"s\": value, \"cl\": {\"rgb\": \"#color\"}}}\n",
    "    border_left: int = 0     # \"bd\": {\"l\": {\"s\": value, \"cl\": {\"rgb\": \"#color\"}}}\n",
    "    border_right: int = 0    # \"bd\": {\"r\": {\"s\": value, \"cl\": {\"rgb\": \"#color\"}}}\n",
    "    border_color: str = \"#000000\"\n",
    "    \n",
    "    # Alignement\n",
    "    horizontal_align: int = 0  # \"ht\": 1=left, 2=center, 3=right, 4=justify\n",
    "    vertical_align: int = 0    # \"vt\": 1=top, 2=center, 3=bottom\n",
    "    text_wrap: bool = False    # \"tb\": 3\n",
    "    \n",
    "    # Rotation/Transformation\n",
    "    text_rotation: int = 0     # \"tr\": {\"a\": angle, \"v\": 0}\n",
    "    \n",
    "    # Format de nombre\n",
    "    number_format: str = \"General\"  # \"n\": {\"pattern\": \"format\"}\n",
    "    \n",
    "    # Fusion de cellules (disponible via mergeData)\n",
    "    is_merged: bool = False\n",
    "    merge_range: Tuple[int, int, int, int] = (0, 0, 0, 0)  # (startRow, endRow, startCol, endCol)\n",
    "    \n",
    "    # Métadonnées de feuille\n",
    "    sheet_hidden: bool = False\n",
    "    sheet_tab_color: str = \"\"\n",
    "    sheet_zoom: float = 1.0\n",
    "    sheet_show_gridlines: bool = True\n",
    "    \n",
    "    # Métadonnées de ligne/colonne\n",
    "    row_height: Optional[float] = None\n",
    "    row_hidden: bool = False\n",
    "    col_width: Optional[int] = None\n",
    "    col_hidden: bool = False\n",
    "    \n",
    "    # Volets figés\n",
    "    freeze_start_row: int = -1\n",
    "    freeze_start_col: int = -1\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Post-processing après création\"\"\"\n",
    "        # Le type est défini par \"t\" dans le JSON ou inféré du contenu\n",
    "        if self.cell_type == 0:  # Si pas de type défini\n",
    "            if self.formula:\n",
    "                self.cell_type = 3\n",
    "            elif isinstance(self.raw_value, (int, float)):\n",
    "                self.cell_type = 2\n",
    "            elif isinstance(self.raw_value, str) and self.raw_value.strip():\n",
    "                self.cell_type = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef97d7-8054-4630-afd8-51c40872a5f2",
   "metadata": {},
   "source": [
    "Classe pour générer les FullCellInfo à partir d'un json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "325735e2-438e-4887-8588-8e2575063cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExcelParser:\n",
    "    \"\"\"Parse les fichiers Excel JSON Univer en structures FullCellInfo\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_excel_json(excel_data: Dict) -> List['FullCellInfo']:\n",
    "        \"\"\"Convertit JSON Excel Univer en liste de FullCellInfo\"\"\"\n",
    "        cells = []  \n",
    "        \n",
    "        styles = excel_data.get('styles', {})\n",
    "        \n",
    "        for sheet_id, sheet_info in excel_data.get('sheets', {}).items():\n",
    "            sheet_name = sheet_info.get('name', \"\")\n",
    "            cell_data = sheet_info.get('cellData', {})\n",
    "            merge_data = sheet_info.get('mergeData', [])\n",
    "            row_data = sheet_info.get('rowData', {})\n",
    "            column_data = sheet_info.get('columnData', {})\n",
    "            freeze_info = sheet_info.get('freeze', {})\n",
    "            \n",
    "            # Métadonnées de feuille\n",
    "            sheet_hidden = bool(sheet_info.get('hidden', 0))\n",
    "            sheet_tab_color = sheet_info.get('tabColor', \"\")\n",
    "            sheet_zoom = sheet_info.get('zoomRatio', 1.0)\n",
    "            sheet_show_gridlines = bool(sheet_info.get('showGridlines', 1))\n",
    "            \n",
    "            # Informations de volets figés\n",
    "            freeze_start_row = freeze_info.get('startRow', -1)\n",
    "            freeze_start_col = freeze_info.get('startColumn', -1)\n",
    "            \n",
    "            # Créer un mapping des cellules fusionnées\n",
    "            merge_map = ExcelParser._create_merge_map(merge_data)\n",
    "            \n",
    "            for row_str, row_cells in cell_data.items():\n",
    "                row = int(row_str)\n",
    "                \n",
    "                # Informations de ligne\n",
    "                row_info = row_data.get(row_str, {})\n",
    "                row_height = row_info.get('h')\n",
    "                row_hidden = bool(row_info.get('hd', 0))\n",
    "                \n",
    "                for col_str, cell_info in row_cells.items():\n",
    "                    col = int(col_str)\n",
    "                    \n",
    "                    # Informations de colonne\n",
    "                    col_info = column_data.get(col_str, {})\n",
    "                    col_width = col_info.get('w')\n",
    "                    col_hidden = bool(col_info.get('hd', 0))\n",
    "                    \n",
    "                    # Extraire les informations de style\n",
    "                    style_id = cell_info.get('s', '')\n",
    "                    style = styles.get(style_id, {}) if style_id else {}\n",
    "                    \n",
    "                    # Vérifier si cette cellule fait partie d'une fusion\n",
    "                    merge_info = merge_map.get((row, col), None)\n",
    "                    \n",
    "                    cell = FullCellInfo(\n",
    "                        raw_value=cell_info.get('v', ''),\n",
    "                        cell_type=cell_info.get('t', 0),\n",
    "                        formula=cell_info.get('f', ''),\n",
    "                        row=row,\n",
    "                        col=col,\n",
    "                        sheet_id=sheet_id,\n",
    "                        sheet_name=sheet_name,\n",
    "                        style_id=style_id,\n",
    "                        is_merged=merge_info is not None,\n",
    "                        merge_range=merge_info if merge_info else (0, 0, 0, 0),\n",
    "                        sheet_hidden=sheet_hidden,\n",
    "                        sheet_tab_color=sheet_tab_color,\n",
    "                        sheet_zoom=sheet_zoom,\n",
    "                        sheet_show_gridlines=sheet_show_gridlines,\n",
    "                        row_height=row_height,\n",
    "                        row_hidden=row_hidden,\n",
    "                        col_width=col_width,\n",
    "                        col_hidden=col_hidden,\n",
    "                        freeze_start_row=freeze_start_row,\n",
    "                        freeze_start_col=freeze_start_col,\n",
    "                        **ExcelParser._parse_style(style)\n",
    "                    )\n",
    "                    \n",
    "                    cells.append(cell)  \n",
    "        \n",
    "        return cells\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_merge_map(merge_data: List[Dict]) -> Dict[Tuple[int, int], Tuple[int, int, int, int]]:\n",
    "        \"\"\"Crée un mapping des cellules fusionnées\"\"\"\n",
    "        merge_map = {}\n",
    "        \n",
    "        for merge in merge_data:\n",
    "            start_row = merge['startRow']\n",
    "            end_row = merge['endRow'] \n",
    "            start_col = merge['startColumn']\n",
    "            end_col = merge['endColumn']\n",
    "            \n",
    "            # Marquer toutes les cellules dans cette plage comme fusionnées\n",
    "            for row in range(start_row, end_row + 1):\n",
    "                for col in range(start_col, end_col + 1):\n",
    "                    merge_map[(row, col)] = (start_row, end_row, start_col, end_col)\n",
    "        \n",
    "        return merge_map\n",
    "    \n",
    "    @staticmethod\n",
    "    def _parse_style(style: Dict) -> Dict:\n",
    "        \"\"\"Parse les informations de style basé sur le format Univer optimisé\"\"\"\n",
    "        parsed = {\n",
    "            'bold': bool(style.get('bl', 0)),\n",
    "            'italic': bool(style.get('it', 0)),\n",
    "            'font_size': float(style.get('fs', 11.0)),\n",
    "            'font_family': style.get('ff', 'Calibri'),\n",
    "            'text_wrap': bool(style.get('tb', 0) == 3),  # tb: 3 = wrap\n",
    "        }\n",
    "        \n",
    "        # Underline - structure: \"ul\": {\"s\": 1}\n",
    "        ul_info = style.get('ul', {})\n",
    "        if isinstance(ul_info, dict):\n",
    "            parsed['underline'] = bool(ul_info.get('s', 0))\n",
    "        else:\n",
    "            parsed['underline'] = bool(ul_info)\n",
    "        \n",
    "        # Strike - structure: \"st\": {\"s\": 1}\n",
    "        st_info = style.get('st', {})\n",
    "        if isinstance(st_info, dict):\n",
    "            parsed['strike'] = bool(st_info.get('s', 0))\n",
    "        else:\n",
    "            parsed['strike'] = bool(st_info)\n",
    "        \n",
    "        # Couleurs\n",
    "        if 'cl' in style:  # text color\n",
    "            parsed['text_color'] = style['cl'].get('rgb', '#000000')\n",
    "        if 'bg' in style:  # background color\n",
    "            parsed['background_color'] = style['bg'].get('rgb', '#FFFFFF')\n",
    "            \n",
    "        # Bordures - structure: \"bd\": {\"t\": {\"s\": 8, \"cl\": {\"rgb\": \"#000000\"}}}\n",
    "        borders = style.get('bd', {})\n",
    "        parsed.update({\n",
    "            'border_top': borders.get('t', {}).get('s', 0),\n",
    "            'border_bottom': borders.get('b', {}).get('s', 0),\n",
    "            'border_left': borders.get('l', {}).get('s', 0),\n",
    "            'border_right': borders.get('r', {}).get('s', 0),\n",
    "        })\n",
    "        \n",
    "        # Couleur de bordure (prendre la première trouvée)\n",
    "        border_color = \"#000000\"\n",
    "        for border_side in ['t', 'b', 'l', 'r']:\n",
    "            if border_side in borders and 'cl' in borders[border_side]:\n",
    "                border_color = borders[border_side]['cl'].get('rgb', '#000000')\n",
    "                break\n",
    "        parsed['border_color'] = border_color\n",
    "        \n",
    "        # Alignement (mapping exact du convertisseur)\n",
    "        parsed.update({\n",
    "            'horizontal_align': style.get('ht', 0),  # 1=left, 2=center, 3=right, 4=justify\n",
    "            'vertical_align': style.get('vt', 0),    # 1=top, 2=center, 3=bottom\n",
    "        })\n",
    "        \n",
    "        # Rotation/Transformation - \"tr\": {\"a\": angle, \"v\": 0}\n",
    "        tr_info = style.get('tr', {})\n",
    "        if isinstance(tr_info, dict):\n",
    "            parsed['text_rotation'] = tr_info.get('a', 0)  # angle\n",
    "        else:\n",
    "            parsed['text_rotation'] = 0\n",
    "        \n",
    "        # Format de nombre - structure: \"n\": {\"pattern\": \"format\"}\n",
    "        number_info = style.get('n', {})\n",
    "        if isinstance(number_info, dict):\n",
    "            parsed['number_format'] = number_info.get('pattern', 'General')\n",
    "        else:\n",
    "            parsed['number_format'] = 'General'\n",
    "        \n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "401e09af-a884-4155-b3dd-c0bc83ebbe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExcelDataProcessor:\n",
    "    \"\"\"Classe pour préparer les données Excel pour l'entraînement de transformers\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cells_to_text_sequence(cells: List[FullCellInfo], include_empty_cells: bool = False) -> str:\n",
    "        \"\"\"Convertit une liste de cellules en séquence de texte pour l'entraînement\"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        # Regrouper par feuille et position\n",
    "        sheets = {}\n",
    "        for cell in cells:\n",
    "            if cell.sheet_name not in sheets:\n",
    "                sheets[cell.sheet_name] = []\n",
    "            sheets[cell.sheet_name].append(cell)\n",
    "        \n",
    "        for sheet_name, sheet_cells in sheets.items():\n",
    "            # Trier par position (row, col)\n",
    "            sheet_cells.sort(key=lambda c: (c.row, c.col))\n",
    "            \n",
    "            # Métadonnées de feuille enrichies\n",
    "            sheet_meta = f\"[SHEET:{sheet_name}\"\n",
    "            if sheet_cells:\n",
    "                first_cell = sheet_cells[0]\n",
    "                if first_cell.sheet_hidden:\n",
    "                    sheet_meta += \",HIDDEN\"\n",
    "                if first_cell.sheet_tab_color:\n",
    "                    sheet_meta += f\",TAB:{first_cell.sheet_tab_color}\"\n",
    "                if first_cell.sheet_zoom != 1.0:\n",
    "                    sheet_meta += f\",ZOOM:{first_cell.sheet_zoom}\"\n",
    "                if not first_cell.sheet_show_gridlines:\n",
    "                    sheet_meta += \",NO_GRID\"\n",
    "                if first_cell.freeze_start_row >= 0 or first_cell.freeze_start_col >= 0:\n",
    "                    sheet_meta += f\",FREEZE:{first_cell.freeze_start_row},{first_cell.freeze_start_col}\"\n",
    "            sheet_meta += \"]\"\n",
    "            \n",
    "            sheet_text = sheet_meta\n",
    "            for cell in sheet_cells:\n",
    "                if not include_empty_cells and not cell.raw_value and not cell.is_merged and not cell.style_id:\n",
    "                    continue\n",
    "                cell_repr = ExcelDataProcessor._cell_to_token(cell)\n",
    "                sheet_text += f\" {cell_repr}\"\n",
    "            \n",
    "            sequences.append(sheet_text)\n",
    "        \n",
    "        return \" [SHEET_END] \".join(sequences)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _cell_to_token(cell: FullCellInfo) -> str:\n",
    "        \"\"\"Convertit une cellule en token enrichi pour le transformer\"\"\"\n",
    "        # Format: [POS:row,col][TYPE:t][STYLE:...][VALUE:...][MERGE:...][ROW/COL:...]\n",
    "        pos = f\"[POS:{cell.row},{cell.col}]\"\n",
    "        \n",
    "        # Type de cellule\n",
    "        type_map = {0: \"EMPTY\", 1: \"TEXT\", 2: \"NUMBER\", 3: \"FORMULA\"}\n",
    "        cell_type = f\"[TYPE:{type_map.get(cell.cell_type, 'UNKNOWN')}]\"\n",
    "        \n",
    "        # Style simplifié pour le token\n",
    "        style_parts = []\n",
    "        if cell.bold: style_parts.append(\"B\")\n",
    "        if cell.italic: style_parts.append(\"I\") \n",
    "        if cell.underline: style_parts.append(\"U\")\n",
    "        if cell.strike: style_parts.append(\"S\")\n",
    "        if cell.background_color != \"#FFFFFF\": \n",
    "            style_parts.append(f\"BG:{cell.background_color}\")\n",
    "        if cell.text_rotation != 0: \n",
    "            style_parts.append(f\"ROT:{cell.text_rotation}\")\n",
    "        if any([cell.border_top, cell.border_bottom, cell.border_left, cell.border_right]):\n",
    "            style_parts.append(\"BORDER\")\n",
    "        if cell.horizontal_align != 0:\n",
    "            align_map = {1: \"LEFT\", 2: \"CENTER\", 3: \"RIGHT\", 4: \"JUSTIFY\"}\n",
    "            style_parts.append(f\"ALIGN:{align_map.get(cell.horizontal_align, 'UNKNOWN')}\")\n",
    "        if cell.text_wrap:\n",
    "            style_parts.append(\"WRAP\")\n",
    "        if cell.font_size != 11.0:\n",
    "            style_parts.append(f\"SIZE:{cell.font_size}\")\n",
    "        if cell.font_family != \"Calibri\":\n",
    "            style_parts.append(f\"FONT:{cell.font_family}\")\n",
    "        \n",
    "        style = f\"[STYLE:{','.join(style_parts)}]\" if style_parts else \"[STYLE:NONE]\"\n",
    "        \n",
    "        # Valeur (formule ou valeur)\n",
    "        if cell.formula:\n",
    "            # Formule avec = au début selon le convertisseur\n",
    "            formula_clean = cell.formula.lstrip('=')\n",
    "            value = f\"[FORMULA:={formula_clean}]\"\n",
    "        else:\n",
    "            value = f\"[VALUE:{cell.raw_value}]\"\n",
    "        \n",
    "        # Ajout information de fusion\n",
    "        merge = \"\"\n",
    "        if cell.is_merged:\n",
    "            sr, er, sc, ec = cell.merge_range\n",
    "            merge = f\"[MERGE:{sr},{er},{sc},{ec}]\"\n",
    "        \n",
    "        # Informations de ligne/colonne si non-standard\n",
    "        layout = \"\"\n",
    "        layout_parts = []\n",
    "        if cell.row_height is not None:\n",
    "            layout_parts.append(f\"RH:{cell.row_height}\")\n",
    "        if cell.row_hidden:\n",
    "            layout_parts.append(\"ROW_HIDDEN\")\n",
    "        if cell.col_width is not None:\n",
    "            layout_parts.append(f\"CW:{cell.col_width}\")\n",
    "        if cell.col_hidden:\n",
    "            layout_parts.append(\"COL_HIDDEN\")\n",
    "        \n",
    "        if layout_parts:\n",
    "            layout = f\"[LAYOUT:{','.join(layout_parts)}]\"\n",
    "        \n",
    "        return f\"{pos}{cell_type}{style}{value}{merge}{layout}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_workbook_metadata(cells: List[FullCellInfo]) -> Dict[str, Any]:\n",
    "        \"\"\"Extrait les métadonnées complètes du classeur\"\"\"\n",
    "        if not cells:\n",
    "            return {}\n",
    "        \n",
    "        sheet_metadata = {}\n",
    "        \n",
    "        for cell in cells:\n",
    "            if cell.sheet_name not in sheet_metadata:\n",
    "                sheet_metadata[cell.sheet_name] = {\n",
    "                    'sheet_id': cell.sheet_id,\n",
    "                    'hidden': cell.sheet_hidden,\n",
    "                    'tab_color': cell.sheet_tab_color,\n",
    "                    'zoom': cell.sheet_zoom,\n",
    "                    'show_gridlines': cell.sheet_show_gridlines,\n",
    "                    'freeze_panes': (cell.freeze_start_row, cell.freeze_start_col) if cell.freeze_start_row >= 0 or cell.freeze_start_col >= 0 else None,\n",
    "                    'cell_count': 0,\n",
    "                    'merged_count': 0,\n",
    "                    'formula_count': 0,\n",
    "                    'styles_used': set(),\n",
    "                    'max_row': 0,\n",
    "                    'max_col': 0,\n",
    "                    'custom_row_heights': 0,\n",
    "                    'custom_col_widths': 0,\n",
    "                    'hidden_rows': 0,\n",
    "                    'hidden_cols': 0\n",
    "                }\n",
    "            \n",
    "            meta = sheet_metadata[cell.sheet_name]\n",
    "            meta['cell_count'] += 1\n",
    "            meta['max_row'] = max(meta['max_row'], cell.row)\n",
    "            meta['max_col'] = max(meta['max_col'], cell.col)\n",
    "            \n",
    "            if cell.is_merged:\n",
    "                meta['merged_count'] += 1\n",
    "            if cell.formula:\n",
    "                meta['formula_count'] += 1\n",
    "            if cell.style_id:\n",
    "                meta['styles_used'].add(cell.style_id)\n",
    "            if cell.row_height is not None:\n",
    "                meta['custom_row_heights'] += 1\n",
    "            if cell.col_width is not None:\n",
    "                meta['custom_col_widths'] += 1\n",
    "            if cell.row_hidden:\n",
    "                meta['hidden_rows'] += 1\n",
    "            if cell.col_hidden:\n",
    "                meta['hidden_cols'] += 1\n",
    "        \n",
    "        # Convertir les sets en listes pour la sérialisation\n",
    "        for meta in sheet_metadata.values():\n",
    "            meta['styles_used'] = list(meta['styles_used'])\n",
    "        \n",
    "        return {\n",
    "            'sheets': sheet_metadata,\n",
    "            'total_cells': len(cells),\n",
    "            'total_sheets': len(sheet_metadata),\n",
    "            'total_merged_cells': sum(meta['merged_count'] for meta in sheet_metadata.values()),\n",
    "            'total_formulas': sum(meta['formula_count'] for meta in sheet_metadata.values()),\n",
    "            'total_styles': len(set(cell.style_id for cell in cells if cell.style_id))\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0748e56d-a71b-4c4f-b48f-ce17391fc9a0",
   "metadata": {},
   "source": [
    "Embedder depuis FullCellInfo : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c13001e0-ba4b-42fd-bccb-06ab8dcd3ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dimension avant projection: 616\n",
      "Embedding shape: torch.Size([256])\n",
      "Embedding (premiers 10 éléments): tensor([-0.2624,  0.6277, -0.1341, -0.0217, -1.5651, -1.6880, -0.9782,  1.3257,\n",
      "        -1.0761,  0.7506])\n",
      "Batch embedding shape: torch.Size([3, 256])\n",
      "Nombre total de paramètres: 562,320\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class EmbeddingConfig:\n",
    "    \"\"\"Configuration pour l'embedder Excel\"\"\"\n",
    "    # Dimensions\n",
    "    embedding_dim: int = 256\n",
    "    position_embedding_dim: int = 24\n",
    "    type_embedding_dim: int = 16\n",
    "    \n",
    "    # Vocabulaires fixes\n",
    "    max_position: int = 1000  # pour row/col\n",
    "    max_value_length: int = 100\n",
    "    max_font_size: int = 72\n",
    "    \n",
    "    # Couleurs (nombre de couleurs possibles)\n",
    "    color_vocab_size: int = 100\n",
    "    \n",
    "    # Alignements, bordures, etc.\n",
    "    align_vocab_size: int = 5\n",
    "    border_vocab_size: int = 10\n",
    "    font_vocab_size: int = 20\n",
    "\n",
    "class ExcelCellEmbedder(nn.Module):\n",
    "    \"\"\"Embedder direct pour FullCellInfo avec dimensions corrigées\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EmbeddingConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Position et Type\n",
    "        self.row_embedding = nn.Embedding(config.max_position, config.position_embedding_dim)\n",
    "        self.col_embedding = nn.Embedding(config.max_position, config.position_embedding_dim)\n",
    "        self.type_embedding = nn.Embedding(4, config.type_embedding_dim)\n",
    "        \n",
    "        # Contenu (valeur/formule)\n",
    "        self.value_encoder = ValueEncoder(config)\n",
    "        \n",
    "        # Formatage booléen\n",
    "        self.bool_embedding = nn.Embedding(2, 8)\n",
    "        \n",
    "        # Police\n",
    "        self.font_size_embedding = nn.Embedding(config.max_font_size + 1, 16)\n",
    "        self.font_family_embedding = nn.Embedding(config.font_vocab_size, 32)\n",
    "        \n",
    "        # Couleurs\n",
    "        self.color_embedding = nn.Embedding(config.color_vocab_size, 24)\n",
    "        \n",
    "        # Alignement\n",
    "        self.align_h_embedding = nn.Embedding(config.align_vocab_size, 16)\n",
    "        self.align_v_embedding = nn.Embedding(config.align_vocab_size, 16)\n",
    "        self.rotation_embedding = nn.Embedding(361, 16)\n",
    "        \n",
    "        # Bordures\n",
    "        self.border_embedding = nn.Embedding(config.border_vocab_size, 16)\n",
    "        \n",
    "        # Fusion\n",
    "        self.merge_encoder = MergeEncoder(config)\n",
    "        \n",
    "        # CALCUL CORRECT DES DIMENSIONS\n",
    "        total_dim = (\n",
    "            2 * config.position_embedding_dim +    # row(32) + col(32) = 64\n",
    "            config.type_embedding_dim +            # type = 16\n",
    "            self.value_encoder.output_dim +        # value = 256\n",
    "            4 * 8 +                                # 4 bools = 32\n",
    "            16 + 32 +                              # font_size + font_family = 48\n",
    "            2 * 24 +                               # 2 colors = 48\n",
    "            2 * 16 + 8 + 16 +                     # align_h + align_v + wrap + rotation = 56\n",
    "            4 * 16 +                               # 4 borders = 64\n",
    "            self.merge_encoder.output_dim          # merge = 32\n",
    "        )  # Total = 64+16+256+32+48+48+56+64+32 = 616\n",
    "        \n",
    "        print(f\"Total dimension avant projection: {total_dim}\")\n",
    "        \n",
    "        # Projection finale\n",
    "        self.projection = nn.Linear(total_dim, config.embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(config.embedding_dim)\n",
    "        \n",
    "        # Vocabulaires pour la conversion\n",
    "        self._build_vocabularies()\n",
    "    \n",
    "    def _calculate_total_dim(self) -> int:\n",
    "        \"\"\"Calcule la dimension totale avant projection\"\"\"\n",
    "        return (\n",
    "            2 * self.config.position_embedding_dim +\n",
    "            self.config.type_embedding_dim +\n",
    "            self.value_encoder.output_dim +\n",
    "            4 * 8 +                    # bools\n",
    "            16 + 32 +                  # font\n",
    "            2 * 24 +                   # colors\n",
    "            2 * 16 + 8 + 16 +         # alignment\n",
    "            4 * 16 +                   # borders\n",
    "            32                         # merge (fixed size)\n",
    "        )\n",
    "    \n",
    "    def _build_vocabularies(self):\n",
    "        \"\"\"Construit les vocabulaires de conversion\"\"\"\n",
    "        self.font_families = [\n",
    "            \"Calibri\", \"Arial\", \"Times New Roman\", \"Helvetica\", \"Verdana\",\n",
    "            \"Georgia\", \"Courier New\", \"Tahoma\", \"Comic Sans MS\", \"Impact\"\n",
    "        ]\n",
    "        self.font_family_to_id = {font: i for i, font in enumerate(self.font_families)}\n",
    "    \n",
    "    def forward(self, cells: Union['FullCellInfo', List['FullCellInfo']]) -> torch.Tensor:\n",
    "        if not isinstance(cells, list):\n",
    "            cells = [cells]\n",
    "        \n",
    "        embeddings = []\n",
    "        \n",
    "        for cell in cells:\n",
    "            # Position\n",
    "            row_emb = self.row_embedding(torch.clamp(torch.tensor(cell.row), 0, self.config.max_position - 1))\n",
    "            col_emb = self.col_embedding(torch.clamp(torch.tensor(cell.col), 0, self.config.max_position - 1))\n",
    "            \n",
    "            # Type\n",
    "            type_emb = self.type_embedding(torch.tensor(cell.cell_type))\n",
    "            \n",
    "            # Contenu\n",
    "            value_emb = self.value_encoder(cell)\n",
    "            \n",
    "            # Formatage booléen\n",
    "            bold_emb = self.bool_embedding(torch.tensor(int(cell.bold)))\n",
    "            italic_emb = self.bool_embedding(torch.tensor(int(cell.italic)))\n",
    "            underline_emb = self.bool_embedding(torch.tensor(int(cell.underline)))\n",
    "            strike_emb = self.bool_embedding(torch.tensor(int(cell.strike)))\n",
    "            \n",
    "            # Police\n",
    "            font_size = min(int(cell.font_size), self.config.max_font_size)\n",
    "            font_size_emb = self.font_size_embedding(torch.tensor(font_size))\n",
    "            \n",
    "            font_id = self.font_family_to_id.get(cell.font_family, 0)\n",
    "            font_family_emb = self.font_family_embedding(torch.tensor(font_id))\n",
    "            \n",
    "            # Couleurs\n",
    "            text_color_id = self._color_to_id(cell.text_color)\n",
    "            bg_color_id = self._color_to_id(cell.background_color)\n",
    "            text_color_emb = self.color_embedding(torch.tensor(text_color_id))\n",
    "            bg_color_emb = self.color_embedding(torch.tensor(bg_color_id))\n",
    "            \n",
    "            # Alignement\n",
    "            align_h_emb = self.align_h_embedding(torch.tensor(cell.horizontal_align))\n",
    "            align_v_emb = self.align_v_embedding(torch.tensor(cell.vertical_align))\n",
    "            wrap_emb = self.bool_embedding(torch.tensor(int(cell.text_wrap)))\n",
    "            rotation = min(abs(cell.text_rotation), 360)\n",
    "            rotation_emb = self.rotation_embedding(torch.tensor(rotation))\n",
    "            \n",
    "            # Bordures\n",
    "            border_top_emb = self.border_embedding(torch.tensor(self._border_to_id(cell.border_top)))\n",
    "            border_bottom_emb = self.border_embedding(torch.tensor(self._border_to_id(cell.border_bottom)))\n",
    "            border_left_emb = self.border_embedding(torch.tensor(self._border_to_id(cell.border_left)))\n",
    "            border_right_emb = self.border_embedding(torch.tensor(self._border_to_id(cell.border_right)))\n",
    "            \n",
    "            # Fusion\n",
    "            merge_emb = self.merge_encoder(cell)\n",
    "            \n",
    "            # Concaténer tous les embeddings\n",
    "            cell_embedding = torch.cat([\n",
    "                row_emb, col_emb, type_emb, value_emb,\n",
    "                bold_emb, italic_emb, underline_emb, strike_emb,\n",
    "                font_size_emb, font_family_emb,\n",
    "                text_color_emb, bg_color_emb,\n",
    "                align_h_emb, align_v_emb, wrap_emb, rotation_emb,\n",
    "                border_top_emb, border_bottom_emb, border_left_emb, border_right_emb,\n",
    "                merge_emb\n",
    "            ], dim=0)\n",
    "            \n",
    "            embeddings.append(cell_embedding)\n",
    "        \n",
    "        # Stack et projeter\n",
    "        batch_embeddings = torch.stack(embeddings)\n",
    "        projected = self.projection(batch_embeddings)\n",
    "        normalized = self.layer_norm(projected)\n",
    "        \n",
    "        return normalized.squeeze(0) if len(cells) == 1 else normalized\n",
    "    \n",
    "    def _color_to_id(self, color: str) -> int:\n",
    "        \"\"\"Convertit une couleur hex en ID\"\"\"\n",
    "        if not color or color == \"#FFFFFF\":\n",
    "            return 0\n",
    "        try:\n",
    "            hex_val = int(color.replace(\"#\", \"\"), 16)\n",
    "            return (hex_val % (self.config.color_vocab_size - 1)) + 1\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def _border_to_id(self, border_style: int) -> int:\n",
    "        \"\"\"Convertit un style de bordure en ID\"\"\"\n",
    "        border_map = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 7: 5, 8: 6, 13: 7}\n",
    "        return border_map.get(border_style, 0)\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingConfig:\n",
    "    \"\"\"Configuration pour l'embedder Excel - CORRIGÉE\"\"\"\n",
    "    # Dimensions principales\n",
    "    embedding_dim: int = 256\n",
    "    position_embedding_dim: int = 32\n",
    "    type_embedding_dim: int = 16\n",
    "    \n",
    "    # Vocabulaires fixes\n",
    "    max_position: int = 1000\n",
    "    max_font_size: int = 72\n",
    "    color_vocab_size: int = 100\n",
    "    align_vocab_size: int = 5\n",
    "    border_vocab_size: int = 10\n",
    "    font_vocab_size: int = 20\n",
    "    \n",
    "    # AJOUT : Paramètres pour ValueEncoder\n",
    "    max_value_tokens: int = 8\n",
    "    value_token_dim: int = 32\n",
    "    value_vocab_size: int = 10000\n",
    "\n",
    "class ValueEncoder(nn.Module):\n",
    "    \"\"\"Encodeur spécialisé pour les valeurs de cellules - CORRIGÉ\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EmbeddingConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.max_tokens = config.max_value_tokens  # FIXÉ\n",
    "        self.token_dim = config.value_token_dim    # FIXÉ\n",
    "        self.output_dim = self.max_tokens * self.token_dim  # 8 * 32 = 256D\n",
    "        \n",
    "        # Tokenizer simple\n",
    "        self.vocab_size = config.value_vocab_size  # FIXÉ\n",
    "        self.token_embedding = nn.Embedding(self.vocab_size, self.token_dim)\n",
    "        \n",
    "        # Embeddings spéciaux\n",
    "        self.mask_token_id = 1000\n",
    "        self.pad_token_id = 0\n",
    "        self.unk_token_id = 1\n",
    "        self.number_token_id = 2\n",
    "        self.formula_start_id = 3\n",
    "        \n",
    "        # Encodeur de position pour les tokens\n",
    "        self.token_position_embedding = nn.Embedding(self.max_tokens, self.token_dim)\n",
    "        \n",
    "        # Classification du type de contenu\n",
    "        self.content_type_embedding = nn.Embedding(4, self.token_dim)\n",
    "        \n",
    "    def forward(self, cell: 'FullCellInfo', mask_content: bool = False) -> torch.Tensor:\n",
    "        \"\"\"Encode la valeur en séquence de tokens\"\"\"\n",
    "        if mask_content:\n",
    "            tokens = [self.mask_token_id] * self.max_tokens\n",
    "            content_type = 0\n",
    "        elif cell.formula:\n",
    "            tokens = self._tokenize_formula(cell.formula)\n",
    "            content_type = 3  # FORMULA\n",
    "        elif cell.raw_value and cell.cell_type == 2:  # Number\n",
    "            tokens = self._tokenize_number(cell.raw_value)\n",
    "            content_type = 2  # NUMBER\n",
    "        elif cell.raw_value and cell.cell_type == 1:  # Text\n",
    "            tokens = self._tokenize_text(str(cell.raw_value))\n",
    "            content_type = 1  # TEXT\n",
    "        else:\n",
    "            tokens = [self.pad_token_id] * self.max_tokens\n",
    "            content_type = 0  # EMPTY\n",
    "        \n",
    "        # Padding/truncation à max_tokens\n",
    "        if len(tokens) > self.max_tokens:\n",
    "            tokens = tokens[:self.max_tokens]\n",
    "        else:\n",
    "            tokens.extend([self.pad_token_id] * (self.max_tokens - len(tokens)))\n",
    "        \n",
    "        # Convertir en embeddings\n",
    "        token_ids = torch.tensor(tokens)\n",
    "        token_embs = self.token_embedding(token_ids)  # [max_tokens, token_dim]\n",
    "        \n",
    "        # Ajouter encodage positionnel\n",
    "        positions = torch.arange(self.max_tokens)\n",
    "        pos_embs = self.token_position_embedding(positions)\n",
    "        \n",
    "        # Ajouter type de contenu à chaque token\n",
    "        content_type_emb = self.content_type_embedding(torch.tensor(content_type))\n",
    "        content_type_emb = content_type_emb.unsqueeze(0).expand(self.max_tokens, -1)\n",
    "        \n",
    "        # Combiner\n",
    "        combined_embs = token_embs + pos_embs + content_type_emb  # [max_tokens, token_dim]\n",
    "        \n",
    "        # Aplatir pour la sortie\n",
    "        return combined_embs.flatten()  # [max_tokens * token_dim] = [256]\n",
    "    \n",
    "    def _tokenize_text(self, text: str) -> List[int]:\n",
    "        \"\"\"Tokenise le texte (version simplifiée)\"\"\"\n",
    "        words = text.lower().split()[:self.max_tokens]\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            token_id = (hash(word) % (self.vocab_size - 10)) + 10\n",
    "            tokens.append(token_id)\n",
    "        return tokens\n",
    "    \n",
    "    def _tokenize_number(self, value: Any) -> List[int]:\n",
    "        \"\"\"Tokenise un nombre\"\"\"\n",
    "        try:\n",
    "            num_str = str(float(value))\n",
    "            tokens = [self.number_token_id]\n",
    "            for char in num_str[:self.max_tokens-1]:\n",
    "                if char.isdigit():\n",
    "                    tokens.append(ord(char) - ord('0') + 4)\n",
    "                elif char == '.':\n",
    "                    tokens.append(14)\n",
    "                elif char == '-':\n",
    "                    tokens.append(15)\n",
    "            return tokens\n",
    "        except:\n",
    "            return [self.unk_token_id]\n",
    "    \n",
    "    def _tokenize_formula(self, formula: str) -> List[int]:\n",
    "        \"\"\"Tokenise une formule Excel\"\"\"\n",
    "        tokens = [self.formula_start_id]\n",
    "        # Simplification pour l'exemple\n",
    "        char_tokens = [ord(c) % 100 + 100 for c in formula[:self.max_tokens-1]]\n",
    "        tokens.extend(char_tokens)\n",
    "        return tokens\n",
    "\n",
    "        \n",
    "class MergeEncoder(nn.Module):\n",
    "    \"\"\"Encodeur pour les informations de fusion\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EmbeddingConfig):\n",
    "        super().__init__()\n",
    "        self.output_dim = 32\n",
    "        \n",
    "        # Embeddings pour les coordonnées de fusion\n",
    "        self.merge_coord_embedding = nn.Embedding(config.max_position, 8)\n",
    "        self.merge_projection = nn.Linear(4 * 8, self.output_dim)\n",
    "        self.no_merge_embedding = nn.Parameter(torch.randn(self.output_dim))\n",
    "    \n",
    "    def forward(self, cell: 'FullCellInfo') -> torch.Tensor:\n",
    "        \"\"\"Encode les informations de fusion\"\"\"\n",
    "        if not cell.is_merged:\n",
    "            return self.no_merge_embedding\n",
    "        \n",
    "        sr, er, sc, ec = cell.merge_range\n",
    "        \n",
    "        # Limiter les coordonnées\n",
    "        sr = min(sr, self.merge_coord_embedding.num_embeddings - 1)\n",
    "        er = min(er, self.merge_coord_embedding.num_embeddings - 1)\n",
    "        sc = min(sc, self.merge_coord_embedding.num_embeddings - 1)\n",
    "        ec = min(ec, self.merge_coord_embedding.num_embeddings - 1)\n",
    "        \n",
    "        # Embeddings des coordonnées\n",
    "        sr_emb = self.merge_coord_embedding(torch.tensor(sr))\n",
    "        er_emb = self.merge_coord_embedding(torch.tensor(er))\n",
    "        sc_emb = self.merge_coord_embedding(torch.tensor(sc))\n",
    "        ec_emb = self.merge_coord_embedding(torch.tensor(ec))\n",
    "        \n",
    "        # Concaténer et projeter\n",
    "        merge_vec = torch.cat([sr_emb, er_emb, sc_emb, ec_emb])\n",
    "        return self.merge_projection(merge_vec)\n",
    "\n",
    "class ExcelSheetEmbedder(nn.Module):\n",
    "    \"\"\"Embedder pour des feuilles entières\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EmbeddingConfig):\n",
    "        super().__init__()\n",
    "        self.cell_embedder = ExcelCellEmbedder(config)\n",
    "        self.position_encoder = PositionalEncoder(config.embedding_dim)\n",
    "        \n",
    "    def forward(self, cells: List['FullCellInfo'], max_cells: Optional[int] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Embed une feuille entière\n",
    "        \n",
    "        Args:\n",
    "            cells: Liste de cellules\n",
    "            max_cells: Nombre maximum de cellules (pour padding)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor [num_cells, embedding_dim] ou [max_cells, embedding_dim]\n",
    "        \"\"\"\n",
    "        # Trier par position\n",
    "        sorted_cells = sorted(cells, key=lambda c: (c.row, c.col))\n",
    "        \n",
    "        if max_cells:\n",
    "            # Padding ou troncature\n",
    "            if len(sorted_cells) > max_cells:\n",
    "                sorted_cells = sorted_cells[:max_cells]\n",
    "            elif len(sorted_cells) < max_cells:\n",
    "                # Créer des cellules vides pour le padding\n",
    "                empty_cell = self._create_empty_cell()\n",
    "                sorted_cells.extend([empty_cell] * (max_cells - len(sorted_cells)))\n",
    "        \n",
    "        # Embedder toutes les cellules\n",
    "        cell_embeddings = self.cell_embedder(sorted_cells)\n",
    "        \n",
    "        # Ajouter l'encodage positionnel\n",
    "        positioned_embeddings = self.position_encoder(cell_embeddings)\n",
    "        \n",
    "        return positioned_embeddings\n",
    "    \n",
    "    def _create_empty_cell(self) -> 'FullCellInfo':\n",
    "        \"\"\"Crée une cellule vide pour le padding\"\"\"\n",
    "        # Retourner une cellule avec toutes les valeurs par défaut\n",
    "        # Cette implémentation dépend de votre classe FullCellInfo\n",
    "        pass\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    \"\"\"Encodage positionnel pour les séquences de cellules\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int, max_length: int = 10000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_length, embedding_dim)\n",
    "        position = torch.arange(0, max_length).unsqueeze(1).float()\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() *\n",
    "                           -(np.log(10000.0) / embedding_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Ajoute l'encodage positionnel\"\"\"\n",
    "        seq_len = x.size(0)\n",
    "        return x + self.pe[:seq_len]\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = EmbeddingConfig(\n",
    "        embedding_dim=256,\n",
    "        position_embedding_dim=32\n",
    "    )\n",
    "    \n",
    "    # Créer l'embedder\n",
    "    embedder = ExcelCellEmbedder(config)\n",
    "    \n",
    "    # Simuler une cellule (remplacer par votre vraie classe FullCellInfo)\n",
    "    from dataclasses import dataclass\n",
    "    from typing import Tuple\n",
    "    \n",
    "    @dataclass \n",
    "    class MockFullCellInfo:\n",
    "        raw_value: str = \"Hello\"\n",
    "        cell_type: int = 1\n",
    "        formula: str = \"\"\n",
    "        row: int = 5\n",
    "        col: int = 3\n",
    "        bold: bool = True\n",
    "        italic: bool = False\n",
    "        underline: bool = False\n",
    "        strike: bool = False\n",
    "        font_size: float = 12.0\n",
    "        font_family: str = \"Arial\"\n",
    "        text_color: str = \"#FF0000\"\n",
    "        background_color: str = \"#FFFFFF\"\n",
    "        horizontal_align: int = 1\n",
    "        vertical_align: int = 0\n",
    "        text_wrap: bool = False\n",
    "        text_rotation: int = 0\n",
    "        border_top: int = 1\n",
    "        border_bottom: int = 0\n",
    "        border_left: int = 0\n",
    "        border_right: int = 0\n",
    "        is_merged: bool = False\n",
    "        merge_range: Tuple[int, int, int, int] = (0, 0, 0, 0)\n",
    "    \n",
    "    # Créer une cellule test\n",
    "    cell = MockFullCellInfo()\n",
    "    \n",
    "    # Obtenir l'embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = embedder(cell)\n",
    "        print(f\"Embedding shape: {embedding.shape}\")\n",
    "        print(f\"Embedding (premiers 10 éléments): {embedding[:10]}\")\n",
    "        \n",
    "        # Test avec plusieurs cellules\n",
    "        cells = [cell, cell, cell]\n",
    "        batch_embedding = embedder(cells)\n",
    "        print(f\"Batch embedding shape: {batch_embedding.shape}\")\n",
    "        \n",
    "        # Nombre de paramètres\n",
    "        total_params = sum(p.numel() for p in embedder.parameters())\n",
    "        print(f\"Nombre total de paramètres: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21174b58-7057-4add-9cb3-96aff6f70f20",
   "metadata": {},
   "source": [
    "Création du Graph Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "71749465-3df3-4bc5-8858-c6b5ed01a08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de cellules: 8\n",
      "Nombre d'arêtes: 52\n",
      "Nombre de feuilles: 3\n",
      "\n",
      "Position mapping (sheet, row, col) -> index:\n",
      "  ('Sheet1', 0, 0) -> 0\n",
      "  ('Sheet1', 0, 1) -> 1\n",
      "  ('Sheet1', 1, 0) -> 2\n",
      "  ('Sheet1', 1, 1) -> 3\n",
      "  ('Sheet2', 0, 0) -> 4\n",
      "\n",
      "Arêtes par type:\n",
      "  adjacent_diag: 3\n",
      "  adjacent_down: 3\n",
      "  adjacent_right: 3\n",
      "  cross_sheet: 19\n",
      "  formula_ref: 2\n",
      "  same_col: 3\n",
      "  same_row: 3\n",
      "  same_sheet: 9\n",
      "  same_value_type: 7\n",
      "\n",
      "Exemples d'arêtes de feuille:\n",
      "  same_sheet: Sheet1!(0,0) -> Sheet1!(0,1) [weight: 1.00]\n",
      "  same_sheet: Sheet1!(0,0) -> Sheet1!(1,0) [weight: 1.00]\n",
      "  same_sheet: Sheet1!(0,0) -> Sheet1!(1,1) [weight: 1.00]\n",
      "  same_sheet: Sheet1!(0,1) -> Sheet1!(1,0) [weight: 1.00]\n",
      "  same_sheet: Sheet1!(0,1) -> Sheet1!(1,1) [weight: 1.00]\n",
      "\n",
      "Exemples de références cross-sheet:\n"
     ]
    }
   ],
   "source": [
    "class EdgeType(Enum):\n",
    "    \"\"\"Types d'arêtes dans le graphe Excel\"\"\"\n",
    "    # Relations spatiales\n",
    "    SAME_ROW = \"same_row\"\n",
    "    SAME_COL = \"same_col\"\n",
    "    ADJACENT_RIGHT = \"adjacent_right\"\n",
    "    ADJACENT_DOWN = \"adjacent_down\"\n",
    "    ADJACENT_DIAG = \"adjacent_diag\"\n",
    "    \n",
    "    # Relations de distance\n",
    "    NEAR_2 = \"near_2\"      # Distance 2\n",
    "    NEAR_3 = \"near_3\"      # Distance 3\n",
    "    NEAR_5 = \"near_5\"      # Distance 5\n",
    "    FAR = \"far\"            # Distance > 5\n",
    "    \n",
    "    # Relations de dépendance\n",
    "    FORMULA_REF = \"formula_ref\"           # A1 référence B1 dans une formule\n",
    "    FORMULA_RANGE = \"formula_range\"       # Formule utilise une plage (A1:B5)\n",
    "    FORMULA_INDIRECT = \"formula_indirect\" # Référence indirecte (INDIRECT, etc.)\n",
    "    CROSS_SHEET_REF = \"cross_sheet_ref\"   # Référence entre feuilles (Sheet2!A1)\n",
    "    \n",
    "    # Relations structurelles\n",
    "    MERGED_CELL = \"merged_cell\"      # Cellules fusionnées\n",
    "    SAME_STYLE = \"same_style\"        # Même style appliqué\n",
    "    SAME_VALUE_TYPE = \"same_value_type\"  # Même type de valeur\n",
    "    \n",
    "    # Relations de feuille\n",
    "    SAME_SHEET = \"same_sheet\"        # Appartiennent à la même feuille\n",
    "    CROSS_SHEET = \"cross_sheet\"      # Appartiennent à des feuilles différentes\n",
    "    \n",
    "    # Relations de plage\n",
    "    RANGE_START = \"range_start\"      # Début de plage\n",
    "    RANGE_END = \"range_end\"          # Fin de plage\n",
    "    RANGE_MEMBER = \"range_member\"    # Membre d'une plage\n",
    "\n",
    "@dataclass\n",
    "class GraphEdge:\n",
    "    \"\"\"Représente une arête dans le graphe Excel\"\"\"\n",
    "    source_idx: int           # Index de la cellule source\n",
    "    target_idx: int           # Index de la cellule cible\n",
    "    edge_type: EdgeType       # Type de relation\n",
    "    weight: float = 1.0       # Poids de l'arête\n",
    "    metadata: Dict[str, Any] = None  # Métadonnées additionnelles\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "@dataclass\n",
    "class ExcelGraph:\n",
    "    \"\"\"Représente le graphe Excel avec cellules et arêtes\"\"\"\n",
    "    cell_embeddings: torch.Tensor      # [num_cells, embedding_dim]\n",
    "    edge_embeddings: torch.Tensor      # [num_edges, edge_embedding_dim]\n",
    "    edge_indices: torch.Tensor         # [2, num_edges] - format PyG\n",
    "    edge_types: List[EdgeType]         # Type de chaque arête\n",
    "    edge_weights: torch.Tensor         # [num_edges] - poids des arêtes\n",
    "    cell_positions: List[Tuple[int, int]]  # [(row, col)] positions originales\n",
    "    \n",
    "    @property\n",
    "    def num_nodes(self) -> int:\n",
    "        return self.cell_embeddings.size(0)\n",
    "    \n",
    "    @property\n",
    "    def num_edges(self) -> int:\n",
    "        return self.edge_embeddings.size(0)\n",
    "\n",
    "class ExcelGraphBuilder:\n",
    "    \"\"\"Construit le graphe de relations entre cellules Excel\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_distance: int = 5,\n",
    "                 include_spatial: bool = True,\n",
    "                 include_formula_deps: bool = True,\n",
    "                 include_structural: bool = True,\n",
    "                 include_sheet_relations: bool = True,\n",
    "                 same_style_threshold: float = 0.8,\n",
    "                 cross_sheet_weight: float = 0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_distance: Distance maximale pour les arêtes spatiales\n",
    "            include_spatial: Inclure les relations spatiales\n",
    "            include_formula_deps: Inclure les dépendances de formules\n",
    "            include_structural: Inclure les relations structurelles\n",
    "            include_sheet_relations: Inclure les relations de feuille\n",
    "            same_style_threshold: Seuil pour considérer des styles similaires\n",
    "            cross_sheet_weight: Poids pour les relations inter-feuilles\n",
    "        \"\"\"\n",
    "        self.max_distance = max_distance\n",
    "        self.include_spatial = include_spatial\n",
    "        self.include_formula_deps = include_formula_deps\n",
    "        self.include_structural = include_structural\n",
    "        self.include_sheet_relations = include_sheet_relations\n",
    "        self.same_style_threshold = same_style_threshold\n",
    "        self.cross_sheet_weight = cross_sheet_weight\n",
    "    \n",
    "    def build_graph(self, cells: List['FullCellInfo']) -> Tuple[List[GraphEdge], Dict[Tuple[str, int, int], int]]:\n",
    "        \"\"\"\n",
    "        Construit le graphe de relations entre cellules\n",
    "        \n",
    "        Args:\n",
    "            cells: Liste des cellules Excel\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[edges, position_to_index_map] - position inclut maintenant sheet_name\n",
    "        \"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        # Créer un mapping (sheet, row, col) -> index\n",
    "        pos_to_idx = {(cell.sheet_name, cell.row, cell.col): i for i, cell in enumerate(cells)}\n",
    "        \n",
    "        # 1. Relations de feuille (en premier pour établir le contexte)\n",
    "        if self.include_sheet_relations:\n",
    "            edges.extend(self._build_sheet_relation_edges(cells, pos_to_idx))\n",
    "        \n",
    "        # 2. Relations spatiales (seulement dans la même feuille)\n",
    "        if self.include_spatial:\n",
    "            edges.extend(self._build_spatial_edges(cells, pos_to_idx))\n",
    "        \n",
    "        # 3. Relations de dépendance (formules - incluant cross-sheet)\n",
    "        if self.include_formula_deps:\n",
    "            edges.extend(self._build_formula_dependency_edges(cells, pos_to_idx))\n",
    "        \n",
    "        # 4. Relations structurelles\n",
    "        if self.include_structural:\n",
    "            edges.extend(self._build_structural_edges(cells, pos_to_idx))\n",
    "        \n",
    "        return edges, pos_to_idx\n",
    "    \n",
    "    def _build_sheet_relation_edges(self, cells: List['FullCellInfo'], pos_to_idx: Dict) -> List[GraphEdge]:\n",
    "        \"\"\"Construit les arêtes basées sur l'appartenance aux feuilles\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        # Grouper les cellules par feuille\n",
    "        sheets = {}\n",
    "        for i, cell in enumerate(cells):\n",
    "            if cell.sheet_name not in sheets:\n",
    "                sheets[cell.sheet_name] = []\n",
    "            sheets[cell.sheet_name].append(i)\n",
    "        \n",
    "        # Relations intra-feuille (SAME_SHEET)\n",
    "        for sheet_name, cell_indices in sheets.items():\n",
    "            for i in range(len(cell_indices)):\n",
    "                for j in range(i + 1, len(cell_indices)):\n",
    "                    idx_i, idx_j = cell_indices[i], cell_indices[j]\n",
    "                    edges.append(GraphEdge(\n",
    "                        idx_i, idx_j, \n",
    "                        EdgeType.SAME_SHEET, \n",
    "                        weight=1.0,\n",
    "                        metadata={'sheet_name': sheet_name}\n",
    "                    ))\n",
    "        \n",
    "        # Relations inter-feuilles (CROSS_SHEET)\n",
    "        sheet_names = list(sheets.keys())\n",
    "        for i in range(len(sheet_names)):\n",
    "            for j in range(i + 1, len(sheet_names)):\n",
    "                sheet_i, sheet_j = sheet_names[i], sheet_names[j]\n",
    "                cells_i, cells_j = sheets[sheet_i], sheets[sheet_j]\n",
    "                \n",
    "                # Connecter toutes les cellules entre feuilles différentes\n",
    "                # (peut être coûteux - limiter si nécessaire)\n",
    "                for cell_idx_i in cells_i[:10]:  # Limiter à 10 cellules par feuille\n",
    "                    for cell_idx_j in cells_j[:10]:\n",
    "                        edges.append(GraphEdge(\n",
    "                            cell_idx_i, cell_idx_j,\n",
    "                            EdgeType.CROSS_SHEET,\n",
    "                            weight=self.cross_sheet_weight,\n",
    "                            metadata={\n",
    "                                'sheet_i': sheet_i,\n",
    "                                'sheet_j': sheet_j\n",
    "                            }\n",
    "                        ))\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def _build_spatial_edges(self, cells: List['FullCellInfo'], pos_to_idx: Dict) -> List[GraphEdge]:\n",
    "        \"\"\"Construit les arêtes basées sur les relations spatiales (dans la même feuille uniquement)\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        # Grouper par feuille pour éviter les relations spatiales inter-feuilles\n",
    "        sheets = {}\n",
    "        for i, cell in enumerate(cells):\n",
    "            if cell.sheet_name not in sheets:\n",
    "                sheets[cell.sheet_name] = []\n",
    "            sheets[cell.sheet_name].append((i, cell))\n",
    "        \n",
    "        # Construire les relations spatiales pour chaque feuille séparément\n",
    "        for sheet_name, sheet_cells in sheets.items():\n",
    "            for i, (idx_i, cell_i) in enumerate(sheet_cells):\n",
    "                for j, (idx_j, cell_j) in enumerate(sheet_cells):\n",
    "                    if i >= j:  # Éviter les doublons\n",
    "                        continue\n",
    "                    \n",
    "                    row_i, col_i = cell_i.row, cell_i.col\n",
    "                    row_j, col_j = cell_j.row, cell_j.col\n",
    "                    \n",
    "                    # Distance Manhattan\n",
    "                    distance = abs(row_i - row_j) + abs(col_i - col_j)\n",
    "                    \n",
    "                    if distance > self.max_distance:\n",
    "                        # Relation \"far\" (même feuille)\n",
    "                        edges.append(GraphEdge(\n",
    "                            idx_i, idx_j, EdgeType.FAR, \n",
    "                            weight=1.0/distance,\n",
    "                            metadata={'sheet_name': sheet_name}\n",
    "                        ))\n",
    "                        continue\n",
    "                    \n",
    "                    # Relations spécifiques\n",
    "                    if row_i == row_j and col_i != col_j:\n",
    "                        # Même ligne\n",
    "                        edges.append(GraphEdge(\n",
    "                            idx_i, idx_j, EdgeType.SAME_ROW, \n",
    "                            weight=1.0,\n",
    "                            metadata={'sheet_name': sheet_name, 'row': row_i}\n",
    "                        ))\n",
    "                        \n",
    "                        # Adjacent horizontal\n",
    "                        if abs(col_i - col_j) == 1:\n",
    "                            edges.append(GraphEdge(idx_i, idx_j, EdgeType.ADJACENT_RIGHT, weight=1.0))\n",
    "                    \n",
    "                    elif col_i == col_j and row_i != row_j:\n",
    "                        # Même colonne\n",
    "                        edges.append(GraphEdge(\n",
    "                            idx_i, idx_j, EdgeType.SAME_COL, \n",
    "                            weight=1.0,\n",
    "                            metadata={'sheet_name': sheet_name, 'col': col_i}\n",
    "                        ))\n",
    "                        \n",
    "                        # Adjacent vertical\n",
    "                        if abs(row_i - row_j) == 1:\n",
    "                            edges.append(GraphEdge(idx_i, idx_j, EdgeType.ADJACENT_DOWN, weight=1.0))\n",
    "                    \n",
    "                    elif distance == 2 and abs(row_i - row_j) == 1 and abs(col_i - col_j) == 1:\n",
    "                        # Adjacent diagonal\n",
    "                        edges.append(GraphEdge(idx_i, idx_j, EdgeType.ADJACENT_DIAG, weight=0.7))\n",
    "                    \n",
    "                    # Relations de distance\n",
    "                    elif distance == 2:\n",
    "                        edges.append(GraphEdge(idx_i, idx_j, EdgeType.NEAR_2, weight=0.5))\n",
    "                    elif distance == 3:\n",
    "                        edges.append(GraphEdge(idx_i, idx_j, EdgeType.NEAR_3, weight=0.3))\n",
    "                    elif distance <= 5:\n",
    "                        edges.append(GraphEdge(idx_i, idx_j, EdgeType.NEAR_5, weight=0.1))\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def _build_formula_dependency_edges(self, cells: List['FullCellInfo'], pos_to_idx: Dict) -> List[GraphEdge]:\n",
    "        \"\"\"Construit les arêtes basées sur les dépendances de formules (incluant cross-sheet)\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        for i, cell in enumerate(cells):\n",
    "            if not cell.formula:\n",
    "                continue\n",
    "            \n",
    "            # Parser les références dans la formule\n",
    "            references = self._parse_formula_references(cell.formula, cell.sheet_name)\n",
    "            \n",
    "            for ref in references:\n",
    "                if ref['type'] == 'cell':\n",
    "                    # Référence à une cellule (même feuille ou autre)\n",
    "                    target_pos = (ref['sheet'], ref['row'], ref['col'])\n",
    "                    if target_pos in pos_to_idx:\n",
    "                        j = pos_to_idx[target_pos]\n",
    "                        edge_type = EdgeType.CROSS_SHEET_REF if ref['sheet'] != cell.sheet_name else EdgeType.FORMULA_REF\n",
    "                        weight = 0.8 if ref['sheet'] != cell.sheet_name else 1.0\n",
    "                        \n",
    "                        edges.append(GraphEdge(\n",
    "                            i, j, edge_type, \n",
    "                            weight=weight,\n",
    "                            metadata={\n",
    "                                'formula_ref': ref['text'],\n",
    "                                'source_sheet': cell.sheet_name,\n",
    "                                'target_sheet': ref['sheet']\n",
    "                            }\n",
    "                        ))\n",
    "                \n",
    "                elif ref['type'] == 'range':\n",
    "                    # Référence à une plage\n",
    "                    start_row, start_col = ref['start_row'], ref['start_col']\n",
    "                    end_row, end_col = ref['end_row'], ref['end_col']\n",
    "                    \n",
    "                    for row in range(start_row, end_row + 1):\n",
    "                        for col in range(start_col, end_col + 1):\n",
    "                            target_pos = (ref['sheet'], row, col)\n",
    "                            if target_pos in pos_to_idx:\n",
    "                                j = pos_to_idx[target_pos]\n",
    "                                edge_type = EdgeType.CROSS_SHEET_REF if ref['sheet'] != cell.sheet_name else EdgeType.FORMULA_RANGE\n",
    "                                weight = 0.4 if ref['sheet'] != cell.sheet_name else 0.5\n",
    "                                \n",
    "                                edges.append(GraphEdge(\n",
    "                                    i, j, edge_type,\n",
    "                                    weight=weight,\n",
    "                                    metadata={\n",
    "                                        'formula_range': ref['text'],\n",
    "                                        'source_sheet': cell.sheet_name,\n",
    "                                        'target_sheet': ref['sheet']\n",
    "                                    }\n",
    "                                ))\n",
    "                \n",
    "                elif ref['type'] == 'indirect':\n",
    "                    # Référence indirecte (plus complexe à analyser)\n",
    "                    edges.append(GraphEdge(\n",
    "                        i, i, EdgeType.FORMULA_INDIRECT, \n",
    "                        weight=0.3,\n",
    "                        metadata={'indirect_ref': ref['text']}\n",
    "                    ))\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def _parse_formula_references(self, formula: str, current_sheet: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Parse les références dans une formule Excel (incluant cross-sheet)\"\"\"\n",
    "        references = []\n",
    "        \n",
    "        # Pattern pour les références cross-sheet (Sheet1!A1, 'Sheet Name'!A1)\n",
    "        cross_sheet_pattern = r\"(?:'([^']+)'|([^!\\s]+))!(\\$?[A-Z]+\\$?\\d+)\"\n",
    "        cross_sheet_matches = re.finditer(cross_sheet_pattern, formula.upper())\n",
    "        \n",
    "        for match in cross_sheet_matches:\n",
    "            sheet_name_quoted, sheet_name_simple, cell_ref = match.groups()\n",
    "            sheet_name = sheet_name_quoted or sheet_name_simple\n",
    "            \n",
    "            # Parser la référence de cellule\n",
    "            cell_match = re.match(r'\\$?([A-Z]+)\\$?(\\d+)', cell_ref)\n",
    "            if cell_match:\n",
    "                col_str, row_str = cell_match.groups()\n",
    "                col = self._col_str_to_num(col_str)\n",
    "                row = int(row_str) - 1\n",
    "                \n",
    "                references.append({\n",
    "                    'type': 'cell',\n",
    "                    'sheet': sheet_name,\n",
    "                    'row': row,\n",
    "                    'col': col,\n",
    "                    'text': match.group(0)\n",
    "                })\n",
    "        \n",
    "        # Pattern pour les plages cross-sheet (Sheet1!A1:B5)\n",
    "        cross_sheet_range_pattern = r\"(?:'([^']+)'|([^!\\s]+))!(\\$?[A-Z]+\\$?\\d+:\\$?[A-Z]+\\$?\\d+)\"\n",
    "        cross_sheet_range_matches = re.finditer(cross_sheet_range_pattern, formula.upper())\n",
    "        \n",
    "        for match in cross_sheet_range_matches:\n",
    "            sheet_name_quoted, sheet_name_simple, range_ref = match.groups()\n",
    "            sheet_name = sheet_name_quoted or sheet_name_simple\n",
    "            \n",
    "            # Parser la plage\n",
    "            range_match = re.match(r'\\$?([A-Z]+)\\$?(\\d+):\\$?([A-Z]+)\\$?(\\d+)', range_ref)\n",
    "            if range_match:\n",
    "                start_col_str, start_row_str, end_col_str, end_row_str = range_match.groups()\n",
    "                start_col = self._col_str_to_num(start_col_str)\n",
    "                start_row = int(start_row_str) - 1\n",
    "                end_col = self._col_str_to_num(end_col_str)\n",
    "                end_row = int(end_row_str) - 1\n",
    "                \n",
    "                references.append({\n",
    "                    'type': 'range',\n",
    "                    'sheet': sheet_name,\n",
    "                    'start_row': start_row,\n",
    "                    'start_col': start_col,\n",
    "                    'end_row': end_row,\n",
    "                    'end_col': end_col,\n",
    "                    'text': match.group(0)\n",
    "                })\n",
    "        \n",
    "        # Pattern pour les références locales (A1, A1:B5 sans nom de feuille)\n",
    "        # Retirer d'abord les références cross-sheet pour éviter les doublons\n",
    "        formula_local = formula.upper()\n",
    "        for match in re.finditer(cross_sheet_pattern, formula_local):\n",
    "            formula_local = formula_local.replace(match.group(0), \"\")\n",
    "        for match in re.finditer(cross_sheet_range_pattern, formula_local):\n",
    "            formula_local = formula_local.replace(match.group(0), \"\")\n",
    "        \n",
    "        # Références de cellules locales\n",
    "        cell_pattern = r'\\$?([A-Z]+)\\$?(\\d+)'\n",
    "        cell_matches = re.finditer(cell_pattern, formula_local)\n",
    "        \n",
    "        for match in cell_matches:\n",
    "            col_str, row_str = match.groups()\n",
    "            col = self._col_str_to_num(col_str)\n",
    "            row = int(row_str) - 1\n",
    "            references.append({\n",
    "                'type': 'cell',\n",
    "                'sheet': current_sheet,\n",
    "                'row': row,\n",
    "                'col': col,\n",
    "                'text': match.group(0)\n",
    "            })\n",
    "        \n",
    "        # Plages locales\n",
    "        range_pattern = r'\\$?([A-Z]+)\\$?(\\d+):\\$?([A-Z]+)\\$?(\\d+)'\n",
    "        range_matches = re.finditer(range_pattern, formula_local)\n",
    "        \n",
    "        for match in range_matches:\n",
    "            start_col_str, start_row_str, end_col_str, end_row_str = match.groups()\n",
    "            start_col = self._col_str_to_num(start_col_str)\n",
    "            start_row = int(start_row_str) - 1\n",
    "            end_col = self._col_str_to_num(end_col_str)\n",
    "            end_row = int(end_row_str) - 1\n",
    "            \n",
    "            references.append({\n",
    "                'type': 'range',\n",
    "                'sheet': current_sheet,\n",
    "                'start_row': start_row,\n",
    "                'start_col': start_col,\n",
    "                'end_row': end_row,\n",
    "                'end_col': end_col,\n",
    "                'text': match.group(0)\n",
    "            })\n",
    "        \n",
    "        # Fonctions indirectes\n",
    "        indirect_pattern = r'INDIRECT\\s*\\([^)]+\\)'\n",
    "        indirect_matches = re.finditer(indirect_pattern, formula.upper())\n",
    "        \n",
    "        for match in indirect_matches:\n",
    "            references.append({\n",
    "                'type': 'indirect',\n",
    "                'text': match.group(0)\n",
    "            })\n",
    "        \n",
    "        return references\n",
    "    \n",
    "    def _col_str_to_num(self, col_str: str) -> int:\n",
    "        \"\"\"Convertit une colonne string (A, B, ..., AA, AB) en nombre\"\"\"\n",
    "        result = 0\n",
    "        for char in col_str:\n",
    "            result = result * 26 + (ord(char) - ord('A') + 1)\n",
    "        return result - 1  # Convert to 0-based\n",
    "    \n",
    "    def _build_structural_edges(self, cells: List['FullCellInfo'], pos_to_idx: Dict) -> List[GraphEdge]:\n",
    "        \"\"\"Construit les arêtes basées sur les relations structurelles\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        for i, cell_i in enumerate(cells):\n",
    "            for j, cell_j in enumerate(cells):\n",
    "                if i >= j:\n",
    "                    continue\n",
    "                \n",
    "                # Cellules fusionnées\n",
    "                if cell_i.is_merged and cell_j.is_merged:\n",
    "                    if cell_i.merge_range == cell_j.merge_range:\n",
    "                        edges.append(GraphEdge(i, j, EdgeType.MERGED_CELL, weight=1.0))\n",
    "                \n",
    "                # Même style (simplifié - comparer style_id)\n",
    "                if cell_i.style_id and cell_j.style_id and cell_i.style_id == cell_j.style_id:\n",
    "                    edges.append(GraphEdge(i, j, EdgeType.SAME_STYLE, weight=0.8))\n",
    "                \n",
    "                # Même type de valeur\n",
    "                if cell_i.cell_type == cell_j.cell_type and cell_i.cell_type != 0:  # Pas vide\n",
    "                    edges.append(GraphEdge(i, j, EdgeType.SAME_VALUE_TYPE, weight=0.4))\n",
    "        \n",
    "        return edges\n",
    "\n",
    "class EdgeEmbedder(nn.Module):\n",
    "    \"\"\"Embedder pour les arêtes du graphe\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Embedding par type d'arête\n",
    "        num_edge_types = len(EdgeType)\n",
    "        self.edge_type_embedding = nn.Embedding(num_edge_types, embedding_dim)\n",
    "        \n",
    "        # Embedding pour les poids (discrétisés)\n",
    "        self.weight_bins = 20\n",
    "        self.weight_embedding = nn.Embedding(self.weight_bins, embedding_dim // 4)\n",
    "        \n",
    "        # Projection finale\n",
    "        self.projection = nn.Linear(embedding_dim + embedding_dim // 4, embedding_dim)\n",
    "        \n",
    "        # Mapping des types vers des IDs\n",
    "        self.edge_type_to_id = {edge_type: i for i, edge_type in enumerate(EdgeType)}\n",
    "    \n",
    "    def forward(self, edge_types: List[EdgeType], edge_weights: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Embed les arêtes\n",
    "        \n",
    "        Args:\n",
    "            edge_types: Liste des types d'arêtes\n",
    "            edge_weights: Poids des arêtes [num_edges]\n",
    "            \n",
    "        Returns:\n",
    "            Embeddings des arêtes [num_edges, embedding_dim]\n",
    "        \"\"\"\n",
    "        # Convertir types en IDs\n",
    "        type_ids = torch.tensor([self.edge_type_to_id[et] for et in edge_types])\n",
    "        type_embeddings = self.edge_type_embedding(type_ids)\n",
    "        \n",
    "        # Discrétiser les poids\n",
    "        weight_bins = torch.clamp(\n",
    "            (edge_weights * self.weight_bins).long(), \n",
    "            0, self.weight_bins - 1\n",
    "        )\n",
    "        weight_embeddings = self.weight_embedding(weight_bins)\n",
    "        \n",
    "        # Combiner\n",
    "        combined = torch.cat([type_embeddings, weight_embeddings], dim=1)\n",
    "        return self.projection(combined)\n",
    "\n",
    "class ExcelGraphEmbedder(nn.Module):\n",
    "    \"\"\"Embedder complet pour les graphes Excel\"\"\"\n",
    "    \n",
    "    def __init__(self, cell_embedder: ExcelCellEmbedder, edge_embedding_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.cell_embedder = cell_embedder\n",
    "        self.edge_embedder = EdgeEmbedder(edge_embedding_dim)\n",
    "        self.graph_builder = ExcelGraphBuilder()\n",
    "        \n",
    "    def forward(self, cells: List['FullCellInfo']) -> ExcelGraph:\n",
    "        \"\"\"\n",
    "        Convertit une liste de cellules en graphe embedé\n",
    "        \n",
    "        Args:\n",
    "            cells: Liste des cellules Excel\n",
    "            \n",
    "        Returns:\n",
    "            ExcelGraph avec embeddings\n",
    "        \"\"\"\n",
    "        # 1. Embed les cellules\n",
    "        cell_embeddings = self.cell_embedder(cells)\n",
    "        \n",
    "        # 2. Construire le graphe\n",
    "        edges, pos_to_idx = self.graph_builder.build_graph(cells)\n",
    "        \n",
    "        if not edges:\n",
    "            # Graphe vide\n",
    "            return ExcelGraph(\n",
    "                cell_embeddings=cell_embeddings,\n",
    "                edge_embeddings=torch.empty(0, self.edge_embedder.embedding_dim),\n",
    "                edge_indices=torch.empty(2, 0, dtype=torch.long),\n",
    "                edge_types=[],\n",
    "                edge_weights=torch.empty(0),\n",
    "                cell_positions=[(cell.row, cell.col) for cell in cells]\n",
    "            )\n",
    "        \n",
    "        # 3. Préparer les données d'arêtes\n",
    "        edge_indices = torch.tensor([[e.source_idx, e.target_idx] for e in edges]).T\n",
    "        edge_types = [e.edge_type for e in edges]\n",
    "        edge_weights = torch.tensor([e.weight for e in edges])\n",
    "        \n",
    "        # 4. Embed les arêtes\n",
    "        edge_embeddings = self.edge_embedder(edge_types, edge_weights)\n",
    "        \n",
    "        # 5. Créer le graphe final\n",
    "        return ExcelGraph(\n",
    "            cell_embeddings=cell_embeddings,\n",
    "            edge_embeddings=edge_embeddings,\n",
    "            edge_indices=edge_indices,\n",
    "            edge_types=edge_types,\n",
    "            edge_weights=edge_weights,\n",
    "            cell_positions=[(cell.row, cell.col) for cell in cells]\n",
    "        )\n",
    "    \n",
    "    def get_edge_statistics(self, graph: ExcelGraph) -> Dict[str, int]:\n",
    "        \"\"\"Retourne des statistiques sur les types d'arêtes\"\"\"\n",
    "        stats = {}\n",
    "        for edge_type in graph.edge_types:\n",
    "            stats[edge_type.value] = stats.get(edge_type.value, 0) + 1\n",
    "        return stats\n",
    "\n",
    "# Exemple d'utilisation et tests\n",
    "if __name__ == \"__main__\":\n",
    "    # Simuler des cellules pour tester\n",
    "    from dataclasses import dataclass\n",
    "    from typing import Tuple\n",
    "    \n",
    "    @dataclass\n",
    "    class MockFullCellInfo:\n",
    "        raw_value: str = \"\"\n",
    "        cell_type: int = 0\n",
    "        formula: str = \"\"\n",
    "        row: int = 0\n",
    "        col: int = 0\n",
    "        sheet_name: str = \"Sheet1\"\n",
    "        style_id: str = \"\"\n",
    "        is_merged: bool = False\n",
    "        merge_range: Tuple[int, int, int, int] = (0, 0, 0, 0)\n",
    "        # Autres attributs...\n",
    "        bold: bool = False\n",
    "        italic: bool = False\n",
    "        underline: bool = False\n",
    "        strike: bool = False\n",
    "        font_size: float = 11.0\n",
    "        font_family: str = \"Calibri\"\n",
    "        text_color: str = \"#000000\"\n",
    "        background_color: str = \"#FFFFFF\"\n",
    "        horizontal_align: int = 0\n",
    "        vertical_align: int = 0\n",
    "        text_wrap: bool = False\n",
    "        text_rotation: int = 0\n",
    "        border_top: int = 0\n",
    "        border_bottom: int = 0\n",
    "        border_left: int = 0\n",
    "        border_right: int = 0\n",
    "    \n",
    "    # Créer des cellules de test avec multiple feuilles\n",
    "    cells = [\n",
    "        # Feuille 1\n",
    "        MockFullCellInfo(raw_value=\"A1\", row=0, col=0, cell_type=1, sheet_name=\"Sheet1\"),\n",
    "        MockFullCellInfo(raw_value=\"10\", row=0, col=1, cell_type=2, sheet_name=\"Sheet1\"),\n",
    "        MockFullCellInfo(raw_value=\"B1\", row=1, col=0, cell_type=1, sheet_name=\"Sheet1\"),\n",
    "        MockFullCellInfo(formula=\"=A1+B1\", row=1, col=1, cell_type=3, sheet_name=\"Sheet1\"),\n",
    "        \n",
    "        # Feuille 2\n",
    "        MockFullCellInfo(raw_value=\"Data\", row=0, col=0, cell_type=1, sheet_name=\"Sheet2\"),\n",
    "        MockFullCellInfo(raw_value=\"100\", row=0, col=1, cell_type=2, sheet_name=\"Sheet2\"),\n",
    "        MockFullCellInfo(formula=\"=Sheet1!A1*2\", row=1, col=0, cell_type=3, sheet_name=\"Sheet2\"),\n",
    "        \n",
    "        # Feuille 3 \n",
    "        MockFullCellInfo(formula=\"=SUM(Sheet1!A1:B2)\", row=0, col=0, cell_type=3, sheet_name=\"Sheet3\"),\n",
    "    ]\n",
    "    \n",
    "    # Créer le graph builder et tester\n",
    "    builder = ExcelGraphBuilder(include_sheet_relations=True)\n",
    "    edges, pos_to_idx = builder.build_graph(cells)\n",
    "    \n",
    "    print(f\"Nombre de cellules: {len(cells)}\")\n",
    "    print(f\"Nombre d'arêtes: {len(edges)}\")\n",
    "    print(f\"Nombre de feuilles: {len(set(cell.sheet_name for cell in cells))}\")\n",
    "    \n",
    "    print(f\"\\nPosition mapping (sheet, row, col) -> index:\")\n",
    "    for pos, idx in list(pos_to_idx.items())[:5]:\n",
    "        print(f\"  {pos} -> {idx}\")\n",
    "    \n",
    "    print(\"\\nArêtes par type:\")\n",
    "    edge_type_counts = {}\n",
    "    for edge in edges:\n",
    "        edge_type_counts[edge.edge_type.value] = edge_type_counts.get(edge.edge_type.value, 0) + 1\n",
    "    \n",
    "    for edge_type, count in sorted(edge_type_counts.items()):\n",
    "        print(f\"  {edge_type}: {count}\")\n",
    "    \n",
    "    print(\"\\nExemples d'arêtes de feuille:\")\n",
    "    sheet_edges = [e for e in edges if e.edge_type in [EdgeType.SAME_SHEET, EdgeType.CROSS_SHEET]]\n",
    "    for edge in sheet_edges[:5]:\n",
    "        source_cell = cells[edge.source_idx]\n",
    "        target_cell = cells[edge.target_idx]\n",
    "        print(f\"  {edge.edge_type.value}: \"\n",
    "              f\"{source_cell.sheet_name}!({source_cell.row},{source_cell.col}) -> \"\n",
    "              f\"{target_cell.sheet_name}!({target_cell.row},{target_cell.col}) \"\n",
    "              f\"[weight: {edge.weight:.2f}]\")\n",
    "    \n",
    "    print(\"\\nExemples de références cross-sheet:\")\n",
    "    cross_ref_edges = [e for e in edges if e.edge_type == EdgeType.CROSS_SHEET_REF]\n",
    "    for edge in cross_ref_edges:\n",
    "        source_cell = cells[edge.source_idx]\n",
    "        target_cell = cells[edge.target_idx]\n",
    "        metadata = edge.metadata or {}\n",
    "        print(f\"  {edge.edge_type.value}: \"\n",
    "              f\"{source_cell.sheet_name}!({source_cell.row},{source_cell.col}) -> \"\n",
    "              f\"{target_cell.sheet_name}!({target_cell.row},{target_cell.col}) \"\n",
    "              f\"[formula: {metadata.get('formula_ref', 'N/A')}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e0749-d488-44df-84d6-f2564967dfc7",
   "metadata": {},
   "source": [
    "Fonction pour transformer un/des json en GraphEmbedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f48e8db-f591-413d-95f7-d005879625b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dimension avant projection: 616\n",
      "Statistiques du graphe:\n",
      "  Nombre de nœuds: 6\n",
      "  Nombre d'arêtes: 43\n",
      "  Degré moyen: 14.33\n",
      "  Nombre de feuilles: 2\n",
      "  Feuilles: [0, 1]\n",
      "  Types d'arêtes:\n",
      "    same_sheet: 7\n",
      "    cross_sheet: 8\n",
      "    same_col: 2\n",
      "    adjacent_down: 2\n",
      "    same_row: 3\n",
      "    adjacent_right: 3\n",
      "    adjacent_diag: 2\n",
      "    formula_ref: 2\n",
      "    same_style: 10\n",
      "    same_value_type: 4\n",
      "\n",
      "Dimensions des embeddings:\n",
      "  Cellules: torch.Size([6, 256])\n",
      "  Arêtes: torch.Size([43, 64])\n",
      "  Indices d'arêtes: torch.Size([2, 43])\n",
      "\n",
      "Batch transformé: 2 graphes\n",
      "Total dimension avant projection: 616\n",
      "\n",
      "Graphe rapide: 6 nœuds, 43 arêtes\n"
     ]
    }
   ],
   "source": [
    "class JSONToGraphTransformer:\n",
    "    \"\"\"Transforme un JSON Excel en GraphEmbedded\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_config: Optional['EmbeddingConfig'] = None,\n",
    "                 max_cells_per_sheet: int = 1000,\n",
    "                 include_empty_cells: bool = False,\n",
    "                 graph_config: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_config: Configuration pour l'embedder\n",
    "            max_cells_per_sheet: Limite de cellules par feuille\n",
    "            include_empty_cells: Inclure les cellules vides\n",
    "            graph_config: Configuration pour le graph builder\n",
    "        \"\"\"\n",
    "        # Configuration par défaut\n",
    "        if embedding_config is None:\n",
    "            embedding_config = EmbeddingConfig(\n",
    "                embedding_dim=256,\n",
    "                position_embedding_dim=32,\n",
    "                max_position=10000,\n",
    "                color_vocab_size=1000\n",
    "            )\n",
    "        \n",
    "        if graph_config is None:\n",
    "            graph_config = {\n",
    "                'max_distance': 5,\n",
    "                'include_spatial': True,\n",
    "                'include_formula_deps': True,\n",
    "                'include_structural': True,\n",
    "                'include_sheet_relations': True,\n",
    "                'cross_sheet_weight': 0.3\n",
    "            }\n",
    "        \n",
    "        self.embedding_config = embedding_config\n",
    "        self.max_cells_per_sheet = max_cells_per_sheet\n",
    "        self.include_empty_cells = include_empty_cells\n",
    "        self.graph_config = graph_config\n",
    "        \n",
    "        # Initialiser les composants\n",
    "        self.cell_embedder = ExcelCellEmbedder(embedding_config)\n",
    "        self.graph_embedder = ExcelGraphEmbedder(\n",
    "            self.cell_embedder, \n",
    "            edge_embedding_dim=64\n",
    "        )\n",
    "        \n",
    "        # Configurer le graph builder\n",
    "        self.graph_embedder.graph_builder = ExcelGraphBuilder(**graph_config)\n",
    "    \n",
    "    def transform(self, \n",
    "                  json_data: Union[str, Dict[str, Any]], \n",
    "                  filter_sheets: Optional[List[str]] = None,\n",
    "                  max_total_cells: Optional[int] = None) -> 'ExcelGraph':\n",
    "        \"\"\"\n",
    "        Transforme un JSON Excel en ExcelGraph\n",
    "        \n",
    "        Args:\n",
    "            json_data: JSON string ou dict contenant les données Excel\n",
    "            filter_sheets: Liste des noms de feuilles à inclure (None = toutes)\n",
    "            max_total_cells: Limite totale de cellules (None = pas de limite)\n",
    "            \n",
    "        Returns:\n",
    "            ExcelGraph avec embeddings\n",
    "        \"\"\"\n",
    "        # 1. Parser le JSON\n",
    "        if isinstance(json_data, str):\n",
    "            excel_data = json.loads(json_data)\n",
    "        else:\n",
    "            excel_data = json_data\n",
    "        \n",
    "        # 2. Extraire les cellules\n",
    "        all_cells = self._extract_cells_from_json(excel_data, filter_sheets)\n",
    "        \n",
    "        # 3. Filtrer et limiter les cellules\n",
    "        filtered_cells = self._filter_cells(all_cells, max_total_cells)\n",
    "        \n",
    "        # 4. Transformer en graphe embedé\n",
    "        excel_graph = self.graph_embedder(filtered_cells)\n",
    "        \n",
    "        return excel_graph\n",
    "    \n",
    "    def _extract_cells_from_json(self, \n",
    "                                 excel_data: Dict[str, Any], \n",
    "                                 filter_sheets: Optional[List[str]] = None) -> List['FullCellInfo']:\n",
    "        \"\"\"Extrait les cellules du JSON Excel\"\"\"\n",
    "        try:\n",
    "            # Utiliser le parser existant\n",
    "            all_cells = ExcelParser.parse_excel_json(excel_data)\n",
    "            \n",
    "            # S'assurer qu'on a une liste\n",
    "            if not isinstance(all_cells, list):\n",
    "                if all_cells is None:\n",
    "                    return []\n",
    "                # Si c'est un seul objet FullCellInfo, le mettre dans une liste\n",
    "                return [all_cells]\n",
    "            \n",
    "            # Filtrer par feuilles si spécifié\n",
    "            if filter_sheets:\n",
    "                all_cells = [cell for cell in all_cells if cell.sheet_name in filter_sheets]\n",
    "            \n",
    "            return all_cells\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du parsing JSON: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _filter_cells(self, \n",
    "                      cells: List['FullCellInfo'], \n",
    "                      max_total_cells: Optional[int] = None) -> List['FullCellInfo']:\n",
    "        \"\"\"Filtre et limite les cellules selon les critères\"\"\"\n",
    "        # Vérifier que cells est bien une liste\n",
    "        if not isinstance(cells, list):\n",
    "            if cells is None:\n",
    "                return []\n",
    "            return [cells]  # Convertir un seul objet en liste\n",
    "        \n",
    "        if not cells:  # Liste vide\n",
    "            return []\n",
    "        \n",
    "        filtered_cells = []\n",
    "        \n",
    "        # Grouper par feuille\n",
    "        sheets = {}\n",
    "        for cell in cells:\n",
    "            if cell.sheet_name not in sheets:\n",
    "                sheets[cell.sheet_name] = []\n",
    "            sheets[cell.sheet_name].append(cell)\n",
    "        \n",
    "        # Filtrer par feuille\n",
    "        for sheet_name, sheet_cells in sheets.items():\n",
    "            # Filtrer les cellules vides si nécessaire\n",
    "            if not self.include_empty_cells:\n",
    "                sheet_cells = [\n",
    "                    cell for cell in sheet_cells \n",
    "                    if cell.raw_value or cell.formula or cell.style_id or cell.is_merged\n",
    "                ]\n",
    "            \n",
    "            # Trier par priorité (formules > valeurs > style > vides)\n",
    "            sheet_cells.sort(key=self._cell_priority, reverse=True)\n",
    "            \n",
    "            # Limiter par feuille\n",
    "            if len(sheet_cells) > self.max_cells_per_sheet:\n",
    "                sheet_cells = sheet_cells[:self.max_cells_per_sheet]\n",
    "            \n",
    "            filtered_cells.extend(sheet_cells)\n",
    "        \n",
    "        # Limiter le total si spécifié\n",
    "        if max_total_cells and len(filtered_cells) > max_total_cells:\n",
    "            # Trier par priorité globale et prendre les plus importantes\n",
    "            filtered_cells.sort(key=self._cell_priority, reverse=True)\n",
    "            filtered_cells = filtered_cells[:max_total_cells]\n",
    "        \n",
    "        return filtered_cells\n",
    "    \n",
    "    def _cell_priority(self, cell: 'FullCellInfo') -> int:\n",
    "        \"\"\"Calcule la priorité d'une cellule pour le filtrage\"\"\"\n",
    "        priority = 0\n",
    "        \n",
    "        # Formules ont la plus haute priorité\n",
    "        if cell.formula:\n",
    "            priority += 1000\n",
    "        \n",
    "        # Cellules avec valeurs\n",
    "        if cell.raw_value:\n",
    "            priority += 500\n",
    "            \n",
    "            # Bonus selon le type\n",
    "            if cell.cell_type == 2:  # Nombre\n",
    "                priority += 100\n",
    "            elif cell.cell_type == 1:  # Texte\n",
    "                priority += 50\n",
    "        \n",
    "        # Cellules avec style\n",
    "        if cell.style_id and cell.style_id != \"s0\":\n",
    "            priority += 200\n",
    "        \n",
    "        # Cellules fusionnées\n",
    "        if cell.is_merged:\n",
    "            priority += 150\n",
    "        \n",
    "        # Proximité du coin supérieur gauche (cellules importantes souvent en haut à gauche)\n",
    "        distance_from_origin = cell.row + cell.col\n",
    "        priority += max(0, 100 - distance_from_origin)\n",
    "        \n",
    "        return priority\n",
    "    \n",
    "    def transform_batch(self, \n",
    "                        json_files: List[Union[str, Dict[str, Any]]],\n",
    "                        batch_size: int = 32) -> List['ExcelGraph']:\n",
    "        \"\"\"\n",
    "        Transforme plusieurs JSON en batch\n",
    "        \n",
    "        Args:\n",
    "            json_files: Liste de JSON (strings ou dicts)\n",
    "            batch_size: Taille des batches pour l'embedding\n",
    "            \n",
    "        Returns:\n",
    "            Liste d'ExcelGraph\n",
    "        \"\"\"\n",
    "        graphs = []\n",
    "        \n",
    "        for i in range(0, len(json_files), batch_size):\n",
    "            batch = json_files[i:i + batch_size]\n",
    "            batch_graphs = []\n",
    "            \n",
    "            for json_data in batch:\n",
    "                try:\n",
    "                    graph = self.transform(json_data)\n",
    "                    batch_graphs.append(graph)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors de la transformation: {e}\")\n",
    "                    # Créer un graphe vide en cas d'erreur\n",
    "                    empty_graph = self._create_empty_graph()\n",
    "                    batch_graphs.append(empty_graph)\n",
    "            \n",
    "            graphs.extend(batch_graphs)\n",
    "        \n",
    "        return graphs\n",
    "    \n",
    "    def _create_empty_graph(self) -> 'ExcelGraph':\n",
    "        \"\"\"Crée un graphe vide en cas d'erreur\"\"\"\n",
    "        return ExcelGraph(\n",
    "            cell_embeddings=torch.empty(0, self.embedding_config.embedding_dim),\n",
    "            edge_embeddings=torch.empty(0, 64),\n",
    "            edge_indices=torch.empty(2, 0, dtype=torch.long),\n",
    "            edge_types=[],\n",
    "            edge_weights=torch.empty(0),\n",
    "            cell_positions=[]\n",
    "        )\n",
    "    \n",
    "    def get_graph_statistics(self, graph: 'ExcelGraph') -> Dict[str, Any]:\n",
    "        \"\"\"Retourne des statistiques détaillées sur le graphe\"\"\"\n",
    "        if graph.num_nodes == 0:\n",
    "            return {'empty_graph': True}\n",
    "        \n",
    "        # Statistiques de base\n",
    "        stats = {\n",
    "            'num_nodes': graph.num_nodes,\n",
    "            'num_edges': graph.num_edges,\n",
    "            'avg_degree': graph.num_edges * 2 / graph.num_nodes if graph.num_nodes > 0 else 0\n",
    "        }\n",
    "        \n",
    "        # Statistiques par type d'arête\n",
    "        edge_type_counts = {}\n",
    "        for edge_type in graph.edge_types:\n",
    "            edge_type_counts[edge_type.value] = edge_type_counts.get(edge_type.value, 0) + 1\n",
    "        stats['edge_types'] = edge_type_counts\n",
    "        \n",
    "        # Statistiques des feuilles\n",
    "        sheets = set(pos[0] if isinstance(pos, tuple) and len(pos) >= 1 else \"unknown\" \n",
    "                    for pos in graph.cell_positions)\n",
    "        stats['num_sheets'] = len(sheets)\n",
    "        stats['sheet_names'] = list(sheets)\n",
    "        \n",
    "        # Statistiques des positions\n",
    "        if graph.cell_positions:\n",
    "            positions = [(pos[1], pos[2]) if isinstance(pos, tuple) and len(pos) >= 3 \n",
    "                        else pos for pos in graph.cell_positions]\n",
    "            if positions and all(isinstance(p, tuple) and len(p) == 2 for p in positions):\n",
    "                rows = [p[0] for p in positions]\n",
    "                cols = [p[1] for p in positions]\n",
    "                stats['position_range'] = {\n",
    "                    'min_row': min(rows),\n",
    "                    'max_row': max(rows),\n",
    "                    'min_col': min(cols),\n",
    "                    'max_col': max(cols)\n",
    "                }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def save_graph(self, graph: 'ExcelGraph', filepath: str):\n",
    "        \"\"\"Sauvegarde un graphe au format PyTorch\"\"\"\n",
    "        torch.save({\n",
    "            'cell_embeddings': graph.cell_embeddings,\n",
    "            'edge_embeddings': graph.edge_embeddings,\n",
    "            'edge_indices': graph.edge_indices,\n",
    "            'edge_types': [et.value for et in graph.edge_types],\n",
    "            'edge_weights': graph.edge_weights,\n",
    "            'cell_positions': graph.cell_positions,\n",
    "            'config': self.embedding_config,\n",
    "            'graph_config': self.graph_config\n",
    "        }, filepath)\n",
    "    \n",
    "    def load_graph(self, filepath: str) -> 'ExcelGraph':\n",
    "        \"\"\"Charge un graphe depuis un fichier\"\"\"\n",
    "        data = torch.load(filepath)\n",
    "        \n",
    "        # Reconstituer les edge_types\n",
    "        edge_types = [EdgeType(et) for et in data['edge_types']]\n",
    "        \n",
    "        return ExcelGraph(\n",
    "            cell_embeddings=data['cell_embeddings'],\n",
    "            edge_embeddings=data['edge_embeddings'],\n",
    "            edge_indices=data['edge_indices'],\n",
    "            edge_types=edge_types,\n",
    "            edge_weights=data['edge_weights'],\n",
    "            cell_positions=data['cell_positions']\n",
    "        )\n",
    "\n",
    "# Fonction utilitaire standalone\n",
    "def json_to_excel_graph(json_data: Union[str, Dict[str, Any]], \n",
    "                        **kwargs) -> 'ExcelGraph':\n",
    "    \"\"\"\n",
    "    Fonction utilitaire pour transformer rapidement un JSON en ExcelGraph\n",
    "    \n",
    "    Args:\n",
    "        json_data: JSON Excel (string ou dict)\n",
    "        **kwargs: Arguments pour JSONToGraphTransformer\n",
    "        \n",
    "    Returns:\n",
    "        ExcelGraph\n",
    "    \"\"\"\n",
    "    transformer = JSONToGraphTransformer(**kwargs)\n",
    "    return transformer.transform(json_data)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # JSON d'exemple (format Univer)\n",
    "    example_json = {\n",
    "        \"styles\": {\n",
    "            \"s0\": {\"fs\": 12.0},\n",
    "            \"s1\": {\"fs\": 16.0, \"bl\": 1, \"cl\": {\"rgb\": \"#FFFFFF\"}, \"bg\": {\"rgb\": \"#4470C4\"}}\n",
    "        },\n",
    "        \"sheets\": {\n",
    "            \"sheet1\": {\n",
    "                \"id\": \"sheet1\",\n",
    "                \"name\": \"Sheet1\",\n",
    "                \"hidden\": 0,\n",
    "                \"rowCount\": 10,\n",
    "                \"columnCount\": 10,\n",
    "                \"mergeData\": [],\n",
    "                \"cellData\": {\n",
    "                    \"0\": {\n",
    "                        \"0\": {\"v\": \"Hello\", \"t\": 1, \"s\": \"s0\"},\n",
    "                        \"1\": {\"v\": \"World\", \"t\": 1, \"s\": \"s1\"}\n",
    "                    },\n",
    "                    \"1\": {\n",
    "                        \"0\": {\"v\": 42, \"t\": 2, \"s\": \"s0\"},\n",
    "                        \"1\": {\"f\": \"=A1&B1\", \"t\": 3, \"s\": \"s0\"}\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"sheet2\": {\n",
    "                \"id\": \"sheet2\", \n",
    "                \"name\": \"Sheet2\",\n",
    "                \"hidden\": 0,\n",
    "                \"rowCount\": 5,\n",
    "                \"columnCount\": 5,\n",
    "                \"mergeData\": [],\n",
    "                \"cellData\": {\n",
    "                    \"0\": {\n",
    "                        \"0\": {\"v\": \"Data\", \"t\": 1, \"s\": \"s0\"},\n",
    "                        \"1\": {\"f\": \"=Sheet1!A1\", \"t\": 3, \"s\": \"s0\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Créer le transformer\n",
    "    transformer = JSONToGraphTransformer(\n",
    "        max_cells_per_sheet=100,\n",
    "        include_empty_cells=False\n",
    "    )\n",
    "    \n",
    "    # Transformer le JSON\n",
    "    excel_graph = transformer.transform(example_json)\n",
    "    \n",
    "    # Afficher les statistiques\n",
    "    stats = transformer.get_graph_statistics(excel_graph)\n",
    "    print(\"Statistiques du graphe:\")\n",
    "    print(f\"  Nombre de nœuds: {stats['num_nodes']}\")\n",
    "    print(f\"  Nombre d'arêtes: {stats['num_edges']}\")\n",
    "    print(f\"  Degré moyen: {stats['avg_degree']:.2f}\")\n",
    "    print(f\"  Nombre de feuilles: {stats['num_sheets']}\")\n",
    "    print(f\"  Feuilles: {stats['sheet_names']}\")\n",
    "    \n",
    "    if 'edge_types' in stats:\n",
    "        print(\"  Types d'arêtes:\")\n",
    "        for edge_type, count in stats['edge_types'].items():\n",
    "            print(f\"    {edge_type}: {count}\")\n",
    "    \n",
    "    print(f\"\\nDimensions des embeddings:\")\n",
    "    print(f\"  Cellules: {excel_graph.cell_embeddings.shape}\")\n",
    "    print(f\"  Arêtes: {excel_graph.edge_embeddings.shape}\")\n",
    "    print(f\"  Indices d'arêtes: {excel_graph.edge_indices.shape}\")\n",
    "    \n",
    "    # Test avec une liste de JSON\n",
    "    json_list = [example_json, example_json]\n",
    "    graphs = transformer.transform_batch(json_list)\n",
    "    print(f\"\\nBatch transformé: {len(graphs)} graphes\")\n",
    "    \n",
    "    # Sauvegarde (optionnel)\n",
    "    # transformer.save_graph(excel_graph, \"example_graph.pt\")\n",
    "    \n",
    "    # Test de la fonction utilitaire\n",
    "    quick_graph = json_to_excel_graph(example_json, max_cells_per_sheet=50)\n",
    "    print(f\"\\nGraphe rapide: {quick_graph.num_nodes} nœuds, {quick_graph.num_edges} arêtes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5abb5e0a-c538-4d1a-a942-ed9c213e8a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTransformerLayer(nn.Module):\n",
    "    \"Couche Transformer adaptée aux graphes Excel\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        # Multi-head attention spatiale\n",
    "        self.spatial_attention = nn.MultiheadAttention(\n",
    "            d_model, n_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Multi-head attention sur les arêtes\n",
    "        self.edge_attention = nn.MultiheadAttention(\n",
    "            d_model, n_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feed forward\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Gate pour combiner spatial et edge attention\n",
    "        self.gate = nn.Linear(d_model * 2, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, \n",
    "                node_features: torch.Tensor,\n",
    "                edge_indices: torch.Tensor,\n",
    "                edge_features: torch.Tensor,\n",
    "                attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            node_features: [batch_size, num_nodes, d_model]\n",
    "            edge_indices: [2, num_edges] \n",
    "            edge_features: [num_edges, d_model]\n",
    "            attention_mask: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes, d_model = node_features.shape\n",
    "        \n",
    "        # 1. Attention spatiale globale (toutes les cellules)\n",
    "        spatial_out, spatial_weights = self.spatial_attention(\n",
    "            node_features, node_features, node_features,\n",
    "            attn_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # 2. Attention basée sur les arêtes du graphe\n",
    "        if edge_indices.size(1) > 0:\n",
    "            # Construire les features pour l'attention sur arêtes\n",
    "            edge_source_features = node_features[:, edge_indices[0]]  # [batch, num_edges, d_model]\n",
    "            edge_target_features = node_features[:, edge_indices[1]]  # [batch, num_edges, d_model]\n",
    "            \n",
    "            # Combiner avec les features d'arêtes\n",
    "            edge_combined = edge_source_features + edge_features.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            \n",
    "            # Attention sur les arêtes\n",
    "            edge_out, edge_weights = self.edge_attention(\n",
    "                edge_combined, edge_combined, edge_target_features\n",
    "            )\n",
    "            \n",
    "            # Agréger vers les nœuds (scatter_add simulation)\n",
    "            edge_aggregated = torch.zeros_like(node_features)\n",
    "            for i in range(edge_indices.size(1)):\n",
    "                target_idx = edge_indices[1, i]\n",
    "                edge_aggregated[:, target_idx] += edge_out[:, i]\n",
    "        else:\n",
    "            edge_aggregated = torch.zeros_like(node_features)\n",
    "        \n",
    "        # 3. Combiner spatial et edge attention avec gating\n",
    "        combined_attention = torch.cat([spatial_out, edge_aggregated], dim=-1)\n",
    "        gated_attention = torch.sigmoid(self.gate(combined_attention))\n",
    "        attended = gated_attention * spatial_out + (1 - gated_attention) * edge_aggregated\n",
    "        \n",
    "        # 4. Résiduelle + LayerNorm\n",
    "        x = self.norm1(node_features + self.dropout(attended))\n",
    "        \n",
    "        # 5. Feed Forward\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ExcelGraphTransformer(nn.Module):\n",
    "    \"\"\"Transformer principal pour les graphes Excel\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config: EmbeddingConfig,\n",
    "                 num_layers: int = 6,\n",
    "                 n_heads: int = 8,\n",
    "                 d_ff: int = 1024,\n",
    "                 dropout: float = 0.1,\n",
    "                 max_nodes: int = 512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.d_model = config.embedding_dim\n",
    "        self.max_nodes = max_nodes\n",
    "        \n",
    "        # Embedder de cellules optimisé\n",
    "        self.cell_embedder = self._create_cell_embedder(config)\n",
    "        \n",
    "        # Embedder d'arêtes\n",
    "        self.edge_embedder = EdgeEmbedder(config.embedding_dim)\n",
    "        \n",
    "        # Encodage positionnel 2D (row, col)\n",
    "        self.pos_encoding_2d = PositionalEncoding2D(config.embedding_dim)\n",
    "        \n",
    "        # Couches Transformer\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            GraphTransformerLayer(self.d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Projection finale\n",
    "        self.output_projection = nn.Linear(self.d_model, self.d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def _create_cell_embedder(self, config):\n",
    "        \"\"\"Crée un embedder de cellules optimisé\"\"\"\n",
    "        # Version simplifiée avec les dimensions optimisées\n",
    "        class CellEmbedder(nn.Module):\n",
    "            def __init__(self, config):\n",
    "                super().__init__()\n",
    "                self.config = config\n",
    "                \n",
    "                # Position\n",
    "                self.row_embedding = nn.Embedding(config.max_position, config.position_embedding_dim)\n",
    "                self.col_embedding = nn.Embedding(config.max_position, config.position_embedding_dim)\n",
    "                \n",
    "                # Type et contenu\n",
    "                self.type_embedding = nn.Embedding(4, config.type_embedding_dim)\n",
    "                self.value_encoder = ValueEncoder(config)\n",
    "                \n",
    "                # Style simplifié\n",
    "                self.bool_embedding = nn.Embedding(2, 8)\n",
    "                self.font_size_embedding = nn.Embedding(73, 16)\n",
    "                self.color_embedding = nn.Embedding(config.color_vocab_size, 24)\n",
    "                \n",
    "                # Projection finale optimisée\n",
    "                total_dim = (\n",
    "                    2 * config.position_embedding_dim +  # row, col\n",
    "                    config.type_embedding_dim +          # type\n",
    "                    self.value_encoder.output_dim +      # value (256D)\n",
    "                    4 * 8 +                              # bools\n",
    "                    16 + 24 + 48                         # font, colors, etc.\n",
    "                )\n",
    "                \n",
    "                self.projection = nn.Linear(total_dim, config.embedding_dim)\n",
    "                self.layer_norm = nn.LayerNorm(config.embedding_dim)\n",
    "                \n",
    "            def forward(self, cells, mask_indices=None):\n",
    "                if not isinstance(cells, list):\n",
    "                    cells = [cells]\n",
    "                \n",
    "                embeddings = []\n",
    "                for i, cell in enumerate(cells):\n",
    "                    # Position\n",
    "                    row_emb = self.row_embedding(torch.clamp(torch.tensor(cell.row), 0, self.config.max_position - 1))\n",
    "                    col_emb = self.col_embedding(torch.clamp(torch.tensor(cell.col), 0, self.config.max_position - 1))\n",
    "                    \n",
    "                    # Type\n",
    "                    type_emb = self.type_embedding(torch.tensor(cell.cell_type))\n",
    "                    \n",
    "                    # Contenu (avec masking possible)\n",
    "                    mask_content = mask_indices is not None and i in mask_indices\n",
    "                    value_emb = self.value_encoder(cell, mask_content=mask_content)\n",
    "                    \n",
    "                    # Style simplifié\n",
    "                    bold_emb = self.bool_embedding(torch.tensor(int(cell.bold)))\n",
    "                    italic_emb = self.bool_embedding(torch.tensor(int(cell.italic)))\n",
    "                    underline_emb = self.bool_embedding(torch.tensor(int(cell.underline)))\n",
    "                    strike_emb = self.bool_embedding(torch.tensor(int(cell.strike)))\n",
    "                    \n",
    "                    font_size = min(int(cell.font_size), 72)\n",
    "                    font_size_emb = self.font_size_embedding(torch.tensor(font_size))\n",
    "                    \n",
    "                    # Couleurs simplifiées\n",
    "                    text_color_id = self._color_to_id(cell.text_color)\n",
    "                    bg_color_id = self._color_to_id(cell.background_color)\n",
    "                    text_color_emb = self.color_embedding(torch.tensor(text_color_id))\n",
    "                    bg_color_emb = self.color_embedding(torch.tensor(bg_color_id))\n",
    "                    \n",
    "                    # Concaténer\n",
    "                    cell_embedding = torch.cat([\n",
    "                        row_emb, col_emb, type_emb, value_emb,\n",
    "                        bold_emb, italic_emb, underline_emb, strike_emb,\n",
    "                        font_size_emb, text_color_emb, bg_color_emb\n",
    "                    ], dim=0)\n",
    "                    \n",
    "                    embeddings.append(cell_embedding)\n",
    "                \n",
    "                # Stack et projeter\n",
    "                batch_embeddings = torch.stack(embeddings)\n",
    "                projected = self.projection(batch_embeddings)\n",
    "                normalized = self.layer_norm(projected)\n",
    "                \n",
    "                return normalized.squeeze(0) if len(cells) == 1 else normalized\n",
    "            \n",
    "            def _color_to_id(self, color: str) -> int:\n",
    "                if not color or color == \"#FFFFFF\":\n",
    "                    return 0\n",
    "                try:\n",
    "                    hex_val = int(color.replace(\"#\", \"\"), 16)\n",
    "                    return (hex_val % (self.config.color_vocab_size - 1)) + 1\n",
    "                except:\n",
    "                    return 0\n",
    "        \n",
    "        return CellEmbedder(config)\n",
    "    \n",
    "    def forward(self, \n",
    "                excel_graph: 'ExcelGraph',\n",
    "                mask_indices: Optional[List[int]] = None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass du transformer\n",
    "        \n",
    "        Args:\n",
    "            excel_graph: Graphe Excel avec embeddings\n",
    "            mask_indices: Indices des cellules à masquer\n",
    "            \n",
    "        Returns:\n",
    "            Dict avec les embeddings et logits\n",
    "        \"\"\"\n",
    "        # Récupérer les features\n",
    "        node_features = excel_graph.cell_embeddings.unsqueeze(0)  # [1, num_nodes, d_model]\n",
    "        edge_indices = excel_graph.edge_indices\n",
    "        edge_features = excel_graph.edge_embeddings\n",
    "        \n",
    "        # Ajouter l'encodage positionnel 2D\n",
    "        positions = [(pos[0], pos[1]) for pos in excel_graph.cell_positions]\n",
    "        pos_encoding = self.pos_encoding_2d(positions, node_features.device)\n",
    "        node_features = node_features + pos_encoding.unsqueeze(0)\n",
    "        \n",
    "        # Appliquer les couches Transformer\n",
    "        x = node_features\n",
    "        attention_weights = []\n",
    "        \n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x, edge_indices, edge_features)\n",
    "        \n",
    "        # Projection finale\n",
    "        output_embeddings = self.output_projection(x)\n",
    "        \n",
    "        return {\n",
    "            'embeddings': output_embeddings,\n",
    "            'hidden_states': x,\n",
    "            'masked_indices': mask_indices\n",
    "        }\n",
    "\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 1000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # S'assurer que d_model est divisible par 4 pour row/col encoding\n",
    "        if d_model % 4 != 0:\n",
    "            d_model_adj = ((d_model // 4) + 1) * 4\n",
    "            self.projection = nn.Linear(d_model_adj, d_model)\n",
    "        else:\n",
    "            d_model_adj = d_model\n",
    "            self.projection = None\n",
    "        \n",
    "        # Créer un tensor pour stocker les encodages\n",
    "        pe = torch.zeros(max_len, max_len, d_model_adj)\n",
    "        dim_per_component = d_model_adj // 2\n",
    "        \n",
    "        # Position encoding pour les lignes (première moitié des dimensions)\n",
    "        for row in range(max_len):\n",
    "            for pos_dim in range(0, dim_per_component, 2):\n",
    "                div_term = 10000.0 ** (pos_dim / dim_per_component)\n",
    "                pe[row, :, pos_dim] = torch.sin(row / div_term)\n",
    "                if pos_dim + 1 < dim_per_component:\n",
    "                    pe[row, :, pos_dim + 1] = torch.cos(row / div_term)\n",
    "        \n",
    "        # Position encoding pour les colonnes (deuxième moitié des dimensions)\n",
    "        for col in range(max_len):\n",
    "            for pos_dim in range(dim_per_component, d_model_adj, 2):\n",
    "                rel_dim = pos_dim - dim_per_component\n",
    "                div_term = 10000.0 ** (rel_dim / dim_per_component)\n",
    "                pe[:, col, pos_dim] = torch.sin(col / div_term)\n",
    "                if pos_dim + 1 < d_model_adj:\n",
    "                    pe[:, col, pos_dim + 1] = torch.cos(col / div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, positions: List[Tuple[int, int]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            positions: Liste de (row, col)\n",
    "            device: Device PyTorch\n",
    "        \"\"\"\n",
    "        batch_pos = []\n",
    "        for row, col in positions:\n",
    "            row = min(row, self.pe.size(0) - 1)\n",
    "            col = min(col, self.pe.size(1) - 1)\n",
    "            pos_encoding = self.pe[row, col]\n",
    "            \n",
    "            # Projeter si nécessaire\n",
    "            if self.projection is not None:\n",
    "                pos_encoding = self.projection(pos_encoding)\n",
    "            \n",
    "            batch_pos.append(pos_encoding)\n",
    "        \n",
    "        return torch.stack(batch_pos).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3f1a3-44bf-4430-a354-5fa8f4f464b7",
   "metadata": {},
   "source": [
    "Tache d'apprentissage : Masked Cell Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "814eb582-77a4-4d3e-b47b-a1589e5112fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes manquantes pour l'entraînement\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "class ExcelMaskedDataset(Dataset):\n",
    "    \"\"\"Dataset pour l'entraînement avec masquage de cellules\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 json_files: List[Dict[str, Any]],\n",
    "                 transformer_builder: 'JSONToGraphTransformer',\n",
    "                 mask_ratio: float = 0.15,\n",
    "                 num_candidates: int = 10):\n",
    "        self.json_files = json_files\n",
    "        self.transformer_builder = transformer_builder\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.num_candidates = num_candidates\n",
    "        \n",
    "        # Pré-traiter les données\n",
    "        self.samples = []\n",
    "        self._prepare_samples()\n",
    "    \n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"Prépare les échantillons d'entraînement\"\"\"\n",
    "        for i, json_data in enumerate(self.json_files):\n",
    "            try:\n",
    "                # Extraire les cellules\n",
    "                cells = self.transformer_builder._extract_cells_from_json(json_data)\n",
    "                cells = self.transformer_builder._filter_cells(cells, max_total_cells=200)\n",
    "                \n",
    "                if len(cells) < 3:  # Besoin d'au moins 3 cellules\n",
    "                    continue\n",
    "                \n",
    "                # Créer le graphe\n",
    "                graph = self.transformer_builder.graph_embedder(cells)\n",
    "                \n",
    "                # Choisir les cellules à masquer\n",
    "                mask_indices = ExcelMaskingStrategy.random_masking(cells, self.mask_ratio)\n",
    "                if not mask_indices:\n",
    "                    continue\n",
    "                \n",
    "                # Générer les candidats\n",
    "                candidates = []\n",
    "                labels = []  # Position du bon candidat\n",
    "                \n",
    "                for mask_idx in mask_indices:\n",
    "                    cell = cells[mask_idx]\n",
    "                    cell_candidates = generate_candidates(cell, self.num_candidates)\n",
    "                    \n",
    "                    # S'assurer que la vraie valeur est dans les candidats\n",
    "                    true_value = str(cell.raw_value) if cell.raw_value else str(cell.formula) if cell.formula else \"\"\n",
    "                    \n",
    "                    if true_value and true_value not in cell_candidates:\n",
    "                        # Remplacer un candidat aléatoire par la vraie valeur\n",
    "                        random_idx = random.randint(0, len(cell_candidates) - 1)\n",
    "                        cell_candidates[random_idx] = true_value\n",
    "                    \n",
    "                    # Trouver la position de la vraie valeur\n",
    "                    try:\n",
    "                        label = cell_candidates.index(true_value)\n",
    "                    except ValueError:\n",
    "                        label = 0  # Par défaut\n",
    "                    \n",
    "                    candidates.append(cell_candidates)\n",
    "                    labels.append(label)\n",
    "                \n",
    "                self.samples.append({\n",
    "                    'graph': graph,\n",
    "                    'mask_indices': mask_indices,\n",
    "                    'candidates': candidates,\n",
    "                    'labels': labels,\n",
    "                    'file_id': i\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erreur échantillon {i}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "class MaskedPredictionTrainer:\n",
    "    \"\"\"Entraîneur pour la tâche de masked prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model: 'MaskedCellPredictor',\n",
    "                 learning_rate: float = 1e-4,\n",
    "                 weight_decay: float = 1e-5):\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=learning_rate, \n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=100, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Métriques d'entraînement\n",
    "        self.training_history = []\n",
    "    \n",
    "    def train_epoch(self, dataloader: DataLoader, epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"Entraîne le modèle sur une époque\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_predictions = 0\n",
    "        top3_correct = 0\n",
    "        top5_correct = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            try:\n",
    "                # Forward pass\n",
    "                graph = batch['graph']\n",
    "                mask_indices = batch['mask_indices']\n",
    "                candidates = batch['candidates']\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                # Prédiction\n",
    "                output = self.model(graph, mask_indices, candidates)\n",
    "                logits = output['logits']  # [num_masked, num_candidates]\n",
    "                \n",
    "                # Loss (Cross-entropy)\n",
    "                # Flatten pour le calcul de loss\n",
    "                labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "                loss = F.cross_entropy(logits, labels_tensor)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Métriques\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Accuracy\n",
    "                probabilities = F.softmax(logits, dim=-1)\n",
    "                predictions = torch.argmax(probabilities, dim=-1)\n",
    "                \n",
    "                correct = (predictions == labels_tensor).sum().item()\n",
    "                total_correct += correct\n",
    "                total_predictions += len(labels)\n",
    "                \n",
    "                # Top-k accuracy\n",
    "                _, top_k = torch.topk(probabilities, k=5, dim=-1)\n",
    "                labels_expanded = labels_tensor.unsqueeze(1).expand_as(top_k)\n",
    "                \n",
    "                top3_correct += (labels_expanded[:, :3] == top_k[:, :3]).any(dim=1).sum().item()\n",
    "                top5_correct += (labels_expanded == top_k).any(dim=1).sum().item()\n",
    "                \n",
    "                # Logging\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"  Batch {batch_idx}: Loss {loss.item():.4f}, Acc {correct/len(labels):.1%}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erreur batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Scheduler step\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        # Métriques d'époque\n",
    "        epoch_metrics = {\n",
    "            'loss': total_loss / len(dataloader) if len(dataloader) > 0 else 0,\n",
    "            'accuracy': total_correct / total_predictions if total_predictions > 0 else 0,\n",
    "            'top3_accuracy': top3_correct / total_predictions if total_predictions > 0 else 0,\n",
    "            'top5_accuracy': top5_correct / total_predictions if total_predictions > 0 else 0,\n",
    "            'total_predictions': total_predictions,\n",
    "            'epoch_time': time.time() - start_time,\n",
    "            'learning_rate': self.optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "        \n",
    "        self.training_history.append(epoch_metrics)\n",
    "        return epoch_metrics\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Fonction de collation pour le DataLoader\"\"\"\n",
    "    # Pour l'instant, on ne supporte que batch_size=1\n",
    "    if len(batch) == 1:\n",
    "        return batch[0]\n",
    "    else:\n",
    "        # Pour les batches plus grands, il faudrait implémenter\n",
    "        # le padding/batching des graphes\n",
    "        raise NotImplementedError(\"Batch size > 1 pas encore supporté\")\n",
    "\n",
    "def load_json_files(data_folder: str) -> tuple:\n",
    "    \"\"\"Charge tous les fichiers JSON du dossier\"\"\"\n",
    "    json_files = []\n",
    "    file_paths = []\n",
    "    \n",
    "    patterns = [\n",
    "        os.path.join(data_folder, \"*.json\"),\n",
    "        os.path.join(data_folder, \"**\", \"*.json\")\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        found_files = glob.glob(pattern, recursive=True)\n",
    "        for file_path in found_files:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                json_files.append(data)\n",
    "                file_paths.append(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur chargement {file_path}: {e}\")\n",
    "    \n",
    "    return json_files, file_paths\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "def run_full_training_on_your_data():\n",
    "    \"\"\"Lance l'entraînement complet sur vos données\"\"\"\n",
    "    \n",
    "    print(\"🚀 ENTRAÎNEMENT COMPLET SUR VOS DONNÉES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Configuration adaptée\n",
    "    config = EmbeddingConfig(\n",
    "        embedding_dim=256,  # Divisible par 4\n",
    "        position_embedding_dim=32,\n",
    "        max_position=2000,\n",
    "        color_vocab_size=300\n",
    "    )\n",
    "    \n",
    "    # Transformer builder\n",
    "    transformer_builder = JSONToGraphTransformer(\n",
    "        embedding_config=config,\n",
    "        max_cells_per_sheet=500,\n",
    "        include_empty_cells=False\n",
    "    )\n",
    "    \n",
    "    # Modèle plus robuste\n",
    "    excel_transformer = ExcelGraphTransformer(\n",
    "        config, \n",
    "        num_layers=6,\n",
    "        n_heads=8, \n",
    "        d_ff=1024\n",
    "    )\n",
    "    \n",
    "    predictor = MaskedCellPredictor(excel_transformer, num_candidates=10)\n",
    "    \n",
    "    # Charger vos données\n",
    "    json_files, file_paths = load_json_files(\"data\")\n",
    "    print(f\"Fichiers chargés: {len(json_files)}\")\n",
    "    \n",
    "    # Limiter pour le test\n",
    "    if len(json_files) > 10:\n",
    "        json_files = json_files[:10]\n",
    "    \n",
    "    # Dataset\n",
    "    dataset = ExcelMaskedDataset(\n",
    "        json_files,\n",
    "        transformer_builder,\n",
    "        mask_ratio=0.15,\n",
    "        num_candidates=10\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset créé: {len(dataset)} échantillons\")\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"❌ Dataset vide!\")\n",
    "        return None, None\n",
    "    \n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Entraîneur\n",
    "    trainer = MaskedPredictionTrainer(predictor)\n",
    "    \n",
    "    # Entraînement\n",
    "    num_epochs = 10\n",
    "    print(f\"Entraînement sur {num_epochs} époques...\")\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nÉpoque {epoch + 1}/{num_epochs}:\")\n",
    "        \n",
    "        epoch_metrics = trainer.train_epoch(dataloader, epoch)\n",
    "        \n",
    "        print(f\"  Loss: {epoch_metrics['loss']:.4f}\")\n",
    "        print(f\"  Accuracy: {epoch_metrics['accuracy']:.1%}\")\n",
    "        print(f\"  Top-3: {epoch_metrics['top3_accuracy']:.1%}\")\n",
    "        print(f\"  Top-5: {epoch_metrics['top5_accuracy']:.1%}\")\n",
    "        \n",
    "        if epoch_metrics['accuracy'] > best_accuracy:\n",
    "            best_accuracy = epoch_metrics['accuracy']\n",
    "            torch.save(predictor.state_dict(), \"best_model.pt\")\n",
    "            print(f\"  ⭐ Nouveau record: {best_accuracy:.1%}\")\n",
    "    \n",
    "    return predictor, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4c752602-3d44-4aca-9533-b21be1010286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCellPredictor(nn.Module):\n",
    "    \"\"\"Modèle pour prédire les cellules masquées\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 transformer: ExcelGraphTransformer,\n",
    "                 num_candidates: int = 10):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "        self.num_candidates = num_candidates\n",
    "        \n",
    "        # Tête de classification pour choisir parmi les candidats\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(transformer.d_model, transformer.d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(transformer.d_model // 2, num_candidates)\n",
    "        )\n",
    "        \n",
    "    def forward(self, \n",
    "                excel_graph: 'ExcelGraph',\n",
    "                mask_indices: List[int],\n",
    "                candidates: List[List[str]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            excel_graph: Graphe Excel\n",
    "            mask_indices: Indices des cellules masquées\n",
    "            candidates: Liste de candidats pour chaque cellule masquée\n",
    "            \n",
    "        Returns:\n",
    "            Dict avec logits et probabilités\n",
    "        \"\"\"\n",
    "        # Forward pass du transformer\n",
    "        transformer_output = self.transformer(excel_graph, mask_indices)\n",
    "        embeddings = transformer_output['embeddings']  # [1, num_nodes, d_model]\n",
    "        \n",
    "        # Extraire les embeddings des cellules masquées\n",
    "        masked_embeddings = embeddings[0, mask_indices]  # [num_masked, d_model]\n",
    "        \n",
    "        # Prédiction pour chaque cellule masquée\n",
    "        logits = self.classification_head(masked_embeddings)  # [num_masked, num_candidates]\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'probabilities': probabilities,\n",
    "            'masked_embeddings': masked_embeddings,\n",
    "            'candidates': candidates\n",
    "        }\n",
    "    \n",
    "    def predict_top_candidates(self, \n",
    "                              excel_graph: 'ExcelGraph',\n",
    "                              mask_indices: List[int],\n",
    "                              candidates: List[List[str]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Prédit et retourne les candidats ordonnés par probabilité\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(excel_graph, mask_indices, candidates)\n",
    "            probabilities = output['probabilities']\n",
    "            \n",
    "            predictions = []\n",
    "            for i, (mask_idx, cell_candidates) in enumerate(zip(mask_indices, candidates)):\n",
    "                probs = probabilities[i].cpu().numpy()\n",
    "                \n",
    "                # Ordonner par probabilité décroissante\n",
    "                sorted_indices = np.argsort(probs)[::-1]\n",
    "                \n",
    "                prediction = {\n",
    "                    'cell_index': mask_idx,\n",
    "                    'candidates_ranked': [\n",
    "                        {\n",
    "                            'value': cell_candidates[idx] if idx < len(cell_candidates) else f\"candidate_{idx}\",\n",
    "                            'probability': float(probs[idx]),\n",
    "                            'rank': rank + 1\n",
    "                        }\n",
    "                        for rank, idx in enumerate(sorted_indices)\n",
    "                    ]\n",
    "                }\n",
    "                predictions.append(prediction)\n",
    "            \n",
    "            return predictions\n",
    "\n",
    "# Utilitaires pour l'entraînement\n",
    "class ExcelMaskingStrategy:\n",
    "    \"\"\"Stratégies pour masquer les cellules\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_masking(cells: List['FullCellInfo'], \n",
    "                      mask_ratio: float = 0.15) -> List[int]:\n",
    "        \"\"\"Masquage aléatoire\"\"\"\n",
    "        num_cells = len(cells)\n",
    "        num_mask = int(num_cells * mask_ratio)\n",
    "        \n",
    "        # Privilégier les cellules avec contenu\n",
    "        content_indices = [i for i, cell in enumerate(cells) \n",
    "                          if cell.raw_value or cell.formula]\n",
    "        \n",
    "        if len(content_indices) >= num_mask:\n",
    "            return random.sample(content_indices, num_mask)\n",
    "        else:\n",
    "            # Compléter avec des cellules aléatoires\n",
    "            remaining = num_mask - len(content_indices)\n",
    "            other_indices = [i for i in range(num_cells) if i not in content_indices]\n",
    "            additional = random.sample(other_indices, min(remaining, len(other_indices)))\n",
    "            return content_indices + additional\n",
    "    \n",
    "    @staticmethod\n",
    "    def strategic_masking(cells: List['FullCellInfo']) -> List[int]:\n",
    "        \"\"\"Masquage stratégique (formules, valeurs importantes)\"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        for i, cell in enumerate(cells):\n",
    "            priority = 0\n",
    "            \n",
    "            # Formules = haute priorité\n",
    "            if cell.formula:\n",
    "                priority += 100\n",
    "            \n",
    "            # Valeurs numériques = moyenne priorité\n",
    "            elif cell.cell_type == 2:\n",
    "                priority += 50\n",
    "            \n",
    "            # Texte = basse priorité\n",
    "            elif cell.cell_type == 1:\n",
    "                priority += 25\n",
    "            \n",
    "            if priority > 0:\n",
    "                candidates.append((i, priority))\n",
    "        \n",
    "        # Prendre les plus prioritaires\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        num_mask = min(3, len(candidates))  # Masquer au plus 3 cellules\n",
    "        \n",
    "        return [idx for idx, _ in candidates[:num_mask]]\n",
    "\n",
    "def generate_candidates(cell: 'FullCellInfo', num_candidates: int = 10) -> List[str]:\n",
    "    \"\"\"Génère des candidats pour une cellule masquée\"\"\"\n",
    "    candidates = []\n",
    "    \n",
    "    if cell.cell_type == 2:  # Nombre\n",
    "        # Candidats numériques\n",
    "        try:\n",
    "            original = float(cell.raw_value) if cell.raw_value else 0\n",
    "            candidates = [\n",
    "                str(original),  # Valeur originale\n",
    "                str(original * 2),\n",
    "                str(original + 1),\n",
    "                str(original - 1),\n",
    "                str(int(original * 1.1)),\n",
    "                \"0\", \"1\", \"100\", \"42\", \"999\"\n",
    "            ]\n",
    "        except:\n",
    "            candidates = [\"0\", \"1\", \"10\", \"100\", \"42\", \"999\", \"1.5\", \"2.0\", \"3.14\", \"50\"]\n",
    "    \n",
    "    elif cell.cell_type == 1:  # Texte\n",
    "        original = str(cell.raw_value) if cell.raw_value else \"\"\n",
    "        candidates = [\n",
    "            original,  # Valeur originale\n",
    "            original.upper(),\n",
    "            original.lower(),\n",
    "            original + \"_copy\",\n",
    "            \"Text\", \"Data\", \"Value\", \"Item\", \"Total\", \"Summary\"\n",
    "        ]\n",
    "    \n",
    "    elif cell.cell_type == 3:  # Formule\n",
    "        candidates = [\n",
    "            cell.formula,  # Formule originale\n",
    "            \"=SUM(A1:A10)\",\n",
    "            \"=AVERAGE(B1:B5)\",\n",
    "            \"=COUNT(C1:C20)\",\n",
    "            \"=MAX(D1:D10)\",\n",
    "            \"=MIN(E1:E10)\",\n",
    "            \"=IF(F1>0,\\\"Yes\\\",\\\"No\\\")\",\n",
    "            \"=VLOOKUP(G1,A:B,2,FALSE)\",\n",
    "            \"=TODAY()\",\n",
    "            \"=CONCATENATE(H1,I1)\"\n",
    "        ]\n",
    "    \n",
    "    else:  # Vide\n",
    "        candidates = [\n",
    "            \"\", \"0\", \"1\", \"Text\", \"Data\", \"=SUM(A1:A5)\", \n",
    "            \"Total\", \"N/A\", \"TBD\", \"...\"\n",
    "        ]\n",
    "    \n",
    "    # S'assurer qu'on a exactement num_candidates\n",
    "    while len(candidates) < num_candidates:\n",
    "        candidates.append(f\"option_{len(candidates)}\")\n",
    "    \n",
    "    return candidates[:num_candidates]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c593a8-b89b-4a7f-afda-1a3f92109d8d",
   "metadata": {},
   "source": [
    "Pipe pour application sur le dossier data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8a14793a-c0ff-4fa8-a015-2296552c0a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 INSTRUCTIONS D'UTILISATION DANS LE NOTEBOOK:\n",
      "\n",
      "1. Placez vos fichiers *.json dans le dossier 'data/'\n",
      "\n",
      "2. Exécutez cette cellule pour charger les fonctions\n",
      "\n",
      "3. Lancez l'analyse avec:\n",
      "   result = analyze_my_data(\"data\")\n",
      "\n",
      "4. Pour limiter à quelques fichiers:\n",
      "   result = analyze_my_data(\"data\", max_files=5)\n",
      "\n",
      "5. Les résultats seront affichés et un rapport HTML sera créé.\n",
      "\n",
      "📊 Cette version analyse vos données et évalue la faisabilité\n",
      "   d'un entraînement complet sans lancer le transformer lourd.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Version compatible avec Jupyter Notebook\n",
    "# À exécuter directement dans une cellule de notebook\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "import time\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration pour éviter les conflits dans Jupyter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_pipeline_notebook(data_folder=\"data\", max_files=None, quick_mode=True):\n",
    "    \"\"\"\n",
    "    Version simplifiée de la pipeline pour notebook Jupyter\n",
    "    \n",
    "    Args:\n",
    "        data_folder: Dossier contenant les fichiers JSON\n",
    "        max_files: Nombre maximum de fichiers à traiter (None = tous)\n",
    "        quick_mode: Mode rapide avec moins d'époques\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 PIPELINE EXCEL TRANSFORMER - VERSION NOTEBOOK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Vérification de l'environnement\n",
    "    print(\"\\n1️⃣ VÉRIFICATION DE L'ENVIRONNEMENT\")\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        print(f\"✅ PyTorch: {torch.__version__}\")\n",
    "        print(f\"✅ NumPy: {np.__version__}\")\n",
    "        print(f\"✅ Pandas: {pd.__version__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"❌ Module manquant: {e}\")\n",
    "        return {\"error\": f\"Missing module: {e}\"}\n",
    "    \n",
    "    # 2. Recherche des fichiers JSON\n",
    "    print(f\"\\n2️⃣ RECHERCHE DES FICHIERS JSON DANS '{data_folder}'\")\n",
    "    \n",
    "    if not os.path.exists(data_folder):\n",
    "        print(f\"❌ Le dossier '{data_folder}' n'existe pas!\")\n",
    "        print(\"💡 Créez le dossier et placez-y vos fichiers *.json\")\n",
    "        return {\"error\": f\"Folder {data_folder} not found\"}\n",
    "    \n",
    "    # Patterns de recherche\n",
    "    json_files = []\n",
    "    patterns = [\n",
    "        os.path.join(data_folder, \"*.json\"),\n",
    "        os.path.join(data_folder, \"**\", \"*.json\")\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        found_files = glob.glob(pattern, recursive=True)\n",
    "        json_files.extend(found_files)\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"❌ Aucun fichier *.json trouvé dans '{data_folder}'\")\n",
    "        return {\"error\": \"No JSON files found\"}\n",
    "    \n",
    "    print(f\"📁 Trouvé {len(json_files)} fichiers JSON\")\n",
    "    \n",
    "    # Limiter si nécessaire\n",
    "    if max_files and len(json_files) > max_files:\n",
    "        json_files = json_files[:max_files]\n",
    "        print(f\"🔄 Limitation à {max_files} fichiers\")\n",
    "    \n",
    "    # 3. Analyse des fichiers\n",
    "    print(f\"\\n3️⃣ ANALYSE DES FICHIERS\")\n",
    "    \n",
    "    valid_files = []\n",
    "    analysis_summary = {\n",
    "        'total_files': len(json_files),\n",
    "        'valid_files': 0,\n",
    "        'total_cells': 0,\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    for i, file_path in enumerate(json_files):\n",
    "        filename = os.path.basename(file_path)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Validation structure Excel\n",
    "            cell_count = 0\n",
    "            is_valid = False\n",
    "            \n",
    "            if 'sheets' in data:\n",
    "                for sheet_data in data['sheets'].values():\n",
    "                    if 'cellData' in sheet_data:\n",
    "                        for row_data in sheet_data['cellData'].values():\n",
    "                            cell_count += len(row_data)\n",
    "                        is_valid = True\n",
    "            \n",
    "            if is_valid and cell_count > 0:\n",
    "                valid_files.append(data)\n",
    "                analysis_summary['valid_files'] += 1\n",
    "                analysis_summary['total_cells'] += cell_count\n",
    "                print(f\"  ✅ {filename}: {cell_count} cellules\")\n",
    "            else:\n",
    "                print(f\"  ⚠️  {filename}: Structure non-Excel ou vide\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"{filename}: {str(e)[:50]}...\"\n",
    "            analysis_summary['errors'].append(error_msg)\n",
    "            print(f\"  ❌ {error_msg}\")\n",
    "    \n",
    "    print(f\"\\n📊 RÉSUMÉ DE L'ANALYSE:\")\n",
    "    print(f\"   Fichiers analysés: {analysis_summary['total_files']}\")\n",
    "    print(f\"   Fichiers valides: {analysis_summary['valid_files']}\")\n",
    "    print(f\"   Total cellules: {analysis_summary['total_cells']}\")\n",
    "    print(f\"   Erreurs: {len(analysis_summary['errors'])}\")\n",
    "    \n",
    "    if analysis_summary['valid_files'] == 0:\n",
    "        print(\"❌ Aucun fichier Excel valide trouvé!\")\n",
    "        return {\"error\": \"No valid Excel files\", \"analysis\": analysis_summary}\n",
    "    \n",
    "    # 4. Test de base sur les données (sans transformer complet)\n",
    "    print(f\"\\n4️⃣ TEST DE BASE SUR LES DONNÉES\")\n",
    "    \n",
    "    # Prendre les premiers fichiers pour test\n",
    "    test_files = valid_files[:min(3, len(valid_files))]\n",
    "    \n",
    "    cells_extracted = 0\n",
    "    sheets_found = 0\n",
    "    formulas_found = 0\n",
    "    \n",
    "    for file_data in test_files:\n",
    "        for sheet_data in file_data.get('sheets', {}).values():\n",
    "            sheets_found += 1\n",
    "            cell_data = sheet_data.get('cellData', {})\n",
    "            \n",
    "            for row_data in cell_data.values():\n",
    "                for cell_info in row_data.values():\n",
    "                    cells_extracted += 1\n",
    "                    if cell_info.get('f'):  # Formule\n",
    "                        formulas_found += 1\n",
    "    \n",
    "    print(f\"✅ Test réussi:\")\n",
    "    print(f\"   Cellules extraites: {cells_extracted}\")\n",
    "    print(f\"   Feuilles trouvées: {sheets_found}\")\n",
    "    print(f\"   Formules trouvées: {formulas_found}\")\n",
    "    \n",
    "    # 5. Simulation d'entraînement simple\n",
    "    print(f\"\\n5️⃣ SIMULATION D'ENTRAÎNEMENT\")\n",
    "    \n",
    "    # Statistiques des types de cellules\n",
    "    cell_types = {'empty': 0, 'text': 0, 'number': 0, 'formula': 0}\n",
    "    \n",
    "    for file_data in test_files:\n",
    "        for sheet_data in file_data.get('sheets', {}).values():\n",
    "            cell_data = sheet_data.get('cellData', {})\n",
    "            for row_data in cell_data.values():\n",
    "                for cell_info in row_data.values():\n",
    "                    cell_type = cell_info.get('t', 0)\n",
    "                    if cell_type == 0:\n",
    "                        cell_types['empty'] += 1\n",
    "                    elif cell_type == 1:\n",
    "                        cell_types['text'] += 1\n",
    "                    elif cell_type == 2:\n",
    "                        cell_types['number'] += 1\n",
    "                    elif cell_type == 3:\n",
    "                        cell_types['formula'] += 1\n",
    "    \n",
    "    print(\"📊 Types de cellules détectés:\")\n",
    "    total_cells = sum(cell_types.values())\n",
    "    for cell_type, count in cell_types.items():\n",
    "        percentage = (count / total_cells * 100) if total_cells > 0 else 0\n",
    "        print(f\"   {cell_type}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 6. Simulation de prédictions\n",
    "    print(f\"\\n6️⃣ SIMULATION DE PRÉDICTIONS\")\n",
    "    \n",
    "    # Générer des candidats pour quelques cellules d'exemple\n",
    "    sample_predictions = []\n",
    "    \n",
    "    # Prendre quelques cellules d'exemple\n",
    "    for file_data in test_files[:2]:\n",
    "        for sheet_data in file_data.get('sheets', {}).values():\n",
    "            cell_data = sheet_data.get('cellData', {})\n",
    "            count = 0\n",
    "            for row_key, row_data in cell_data.items():\n",
    "                for col_key, cell_info in row_data.items():\n",
    "                    if count >= 3:  # Limiter à 3 exemples par fichier\n",
    "                        break\n",
    "                    \n",
    "                    value = cell_info.get('v', '')\n",
    "                    cell_type = cell_info.get('t', 0)\n",
    "                    formula = cell_info.get('f', '')\n",
    "                    \n",
    "                    # Générer des candidats selon le type\n",
    "                    candidates = []\n",
    "                    if cell_type == 1:  # Texte\n",
    "                        candidates = [str(value), f\"{value}_copy\", \"Text\", \"Data\", \"Item\"]\n",
    "                    elif cell_type == 2:  # Nombre\n",
    "                        try:\n",
    "                            num_val = float(value) if value else 0\n",
    "                            candidates = [str(value), str(num_val + 1), str(num_val * 2), \"0\", \"100\"]\n",
    "                        except:\n",
    "                            candidates = [\"0\", \"1\", \"10\", \"100\", str(value)]\n",
    "                    elif cell_type == 3:  # Formule\n",
    "                        candidates = [formula, \"=SUM(A1:A10)\", \"=AVERAGE(B1:B5)\", \"=COUNT(C1:C10)\", \"=MAX(D1:D5)\"]\n",
    "                    else:\n",
    "                        candidates = [\"\", \"0\", \"Text\", \"Data\", \"N/A\"]\n",
    "                    \n",
    "                    # Simuler une prédiction (le vrai modèle assignerait des probabilités)\n",
    "                    predicted_probs = np.random.dirichlet([2, 1, 1, 1, 1])  # Favoriser le premier candidat\n",
    "                    \n",
    "                    sample_predictions.append({\n",
    "                        'position': f\"({row_key},{col_key})\",\n",
    "                        'true_value': str(value) if value else str(formula),\n",
    "                        'cell_type': ['Empty', 'Text', 'Number', 'Formula'][cell_type],\n",
    "                        'candidates': candidates[:5],\n",
    "                        'probabilities': predicted_probs[:5].tolist()\n",
    "                    })\n",
    "                    \n",
    "                    count += 1\n",
    "                if count >= 3:\n",
    "                    break\n",
    "    \n",
    "    print(\"🔮 Exemples de prédictions simulées:\")\n",
    "    for i, pred in enumerate(sample_predictions[:5]):\n",
    "        print(f\"\\n   Exemple {i+1} - Position {pred['position']} ({pred['cell_type']}):\")\n",
    "        print(f\"      Vraie valeur: '{pred['true_value']}'\")\n",
    "        print(\"      Top 3 prédictions:\")\n",
    "        \n",
    "        # Trier par probabilité\n",
    "        sorted_preds = sorted(zip(pred['candidates'], pred['probabilities']), \n",
    "                            key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for rank, (candidate, prob) in enumerate(sorted_preds[:3]):\n",
    "            marker = \"🎯\" if candidate == pred['true_value'] else f\"{rank+1}.\"\n",
    "            print(f\"         {marker} {candidate[:20]:20s} ({prob:.1%})\")\n",
    "    \n",
    "    # 7. Rapport de faisabilité\n",
    "    print(f\"\\n7️⃣ RAPPORT DE FAISABILITÉ\")\n",
    "    \n",
    "    feasibility_score = 0\n",
    "    recommendations = []\n",
    "    \n",
    "    # Évaluer la faisabilité\n",
    "    if analysis_summary['valid_files'] >= 5:\n",
    "        feasibility_score += 30\n",
    "        recommendations.append(\"✅ Nombre suffisant de fichiers pour l'entraînement\")\n",
    "    else:\n",
    "        recommendations.append(\"⚠️ Peu de fichiers - entraînement limité possible\")\n",
    "    \n",
    "    if analysis_summary['total_cells'] >= 1000:\n",
    "        feasibility_score += 40\n",
    "        recommendations.append(\"✅ Nombre suffisant de cellules pour l'entraînement\")\n",
    "    elif analysis_summary['total_cells'] >= 100:\n",
    "        feasibility_score += 20\n",
    "        recommendations.append(\"⚠️ Nombre modéré de cellules - entraînement possible mais limité\")\n",
    "    else:\n",
    "        recommendations.append(\"❌ Trop peu de cellules pour un entraînement efficace\")\n",
    "    \n",
    "    if formulas_found >= 10:\n",
    "        feasibility_score += 20\n",
    "        recommendations.append(\"✅ Formules détectées - apprentissage des relations possible\")\n",
    "    elif formulas_found > 0:\n",
    "        feasibility_score += 10\n",
    "        recommendations.append(\"⚠️ Quelques formules - apprentissage partiel possible\")\n",
    "    else:\n",
    "        recommendations.append(\"⚠️ Aucune formule - apprentissage limité aux valeurs\")\n",
    "    \n",
    "    if len(analysis_summary['errors']) == 0:\n",
    "        feasibility_score += 10\n",
    "        recommendations.append(\"✅ Aucune erreur de format\")\n",
    "    else:\n",
    "        recommendations.append(f\"⚠️ {len(analysis_summary['errors'])} fichiers avec erreurs\")\n",
    "    \n",
    "    print(f\"📊 Score de faisabilité: {feasibility_score}/100\")\n",
    "    \n",
    "    if feasibility_score >= 70:\n",
    "        print(\"🟢 FAISABILITÉ ÉLEVÉE - Entraînement recommandé\")\n",
    "    elif feasibility_score >= 40:\n",
    "        print(\"🟡 FAISABILITÉ MODÉRÉE - Entraînement possible avec limitations\")\n",
    "    else:\n",
    "        print(\"🔴 FAISABILITÉ FAIBLE - Améliorer les données avant entraînement\")\n",
    "    \n",
    "    print(\"\\n💡 RECOMMANDATIONS:\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"   {rec}\")\n",
    "    \n",
    "    # 8. Prochaines étapes\n",
    "    print(f\"\\n8️⃣ PROCHAINES ÉTAPES\")\n",
    "    \n",
    "    if feasibility_score >= 40:\n",
    "        print(\"Pour lancer l'entraînement complet:\")\n",
    "        print(\"1. 🚀 Utilisez la pipeline complète avec vos données\")\n",
    "        print(\"2. 📊 Ajustez les hyperparamètres selon la taille de vos données\")\n",
    "        print(\"3. 🔄 Itérez sur différentes stratégies de masquage\")\n",
    "        print(\"4. 📈 Évaluez les performances sur un ensemble de test\")\n",
    "    else:\n",
    "        print(\"Pour améliorer vos données:\")\n",
    "        print(\"1. 📁 Ajoutez plus de fichiers Excel au format JSON\")\n",
    "        print(\"2. 🔧 Vérifiez la structure de vos fichiers JSON\")\n",
    "        print(\"3. 📊 Assurez-vous d'avoir des formules et des valeurs variées\")\n",
    "        print(\"4. 🧹 Nettoyez les fichiers corrompus\")\n",
    "    \n",
    "    # Résumé final\n",
    "    result = {\n",
    "        'success': True,\n",
    "        'analysis': analysis_summary,\n",
    "        'feasibility_score': feasibility_score,\n",
    "        'recommendations': recommendations,\n",
    "        'sample_predictions': sample_predictions,\n",
    "        'cell_types': cell_types,\n",
    "        'next_steps': 'full_training' if feasibility_score >= 40 else 'improve_data'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n🎉 ANALYSE TERMINÉE!\")\n",
    "    print(f\"📋 Résultat stocké dans la variable 'result'\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_data_report_notebook(result, output_file=\"data_report_notebook.html\"):\n",
    "    \"\"\"Crée un rapport HTML depuis les résultats d'analyse\"\"\"\n",
    "    \n",
    "    if not result.get('success'):\n",
    "        return None\n",
    "    \n",
    "    analysis = result['analysis']\n",
    "    feasibility = result['feasibility_score']\n",
    "    recommendations = result['recommendations']\n",
    "    \n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Rapport d'Analyse Excel - Notebook</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}\n",
    "            .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
    "                       color: white; padding: 30px; border-radius: 10px; text-align: center; }}\n",
    "            .section {{ margin: 20px 0; padding: 20px; border: 1px solid #ddd; \n",
    "                       border-radius: 8px; background: #f9f9f9; }}\n",
    "            .metric {{ display: inline-block; margin: 10px; padding: 15px; \n",
    "                      background: white; border-radius: 8px; border-left: 4px solid #3498db; }}\n",
    "            .score {{ font-size: 2em; font-weight: bold; color: #2c3e50; }}\n",
    "            .good {{ border-left-color: #27ae60; }}\n",
    "            .warning {{ border-left-color: #f39c12; }}\n",
    "            .error {{ border-left-color: #e74c3c; }}\n",
    "            ul {{ list-style-type: none; padding: 0; }}\n",
    "            li {{ margin: 10px 0; padding: 10px; background: white; border-radius: 5px; }}\n",
    "            .emoji {{ font-size: 1.2em; margin-right: 10px; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"header\">\n",
    "            <h1>📊 Analyse de Faisabilité Excel Transformer</h1>\n",
    "            <p>Rapport généré le {datetime.now().strftime('%d/%m/%Y à %H:%M')}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>📈 Résumé Exécutif</h2>\n",
    "            <div class=\"metric good\">\n",
    "                <strong>Fichiers analysés</strong><br>\n",
    "                <span class=\"score\">{analysis['total_files']}</span>\n",
    "            </div>\n",
    "            <div class=\"metric good\">\n",
    "                <strong>Fichiers valides</strong><br>\n",
    "                <span class=\"score\">{analysis['valid_files']}</span>\n",
    "            </div>\n",
    "            <div class=\"metric good\">\n",
    "                <strong>Total cellules</strong><br>\n",
    "                <span class=\"score\">{analysis['total_cells']:,}</span>\n",
    "            </div>\n",
    "            <div class=\"metric {'good' if feasibility >= 70 else 'warning' if feasibility >= 40 else 'error'}\">\n",
    "                <strong>Score de faisabilité</strong><br>\n",
    "                <span class=\"score\">{feasibility}/100</span>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>🎯 Évaluation de Faisabilité</h2>\n",
    "    \"\"\"\n",
    "    \n",
    "    if feasibility >= 70:\n",
    "        html_content += '<div style=\"background: #d4edda; padding: 15px; border-radius: 5px; border-left: 4px solid #28a745;\">'\n",
    "        html_content += '<h3 style=\"color: #155724; margin: 0;\">🟢 FAISABILITÉ ÉLEVÉE</h3>'\n",
    "        html_content += '<p style=\"margin: 10px 0 0 0;\">Vos données sont excellentes pour l\\'entraînement du transformer Excel!</p>'\n",
    "        html_content += '</div>'\n",
    "    elif feasibility >= 40:\n",
    "        html_content += '<div style=\"background: #fff3cd; padding: 15px; border-radius: 5px; border-left: 4px solid #ffc107;\">'\n",
    "        html_content += '<h3 style=\"color: #856404; margin: 0;\">🟡 FAISABILITÉ MODÉRÉE</h3>'\n",
    "        html_content += '<p style=\"margin: 10px 0 0 0;\">L\\'entraînement est possible mais avec des limitations.</p>'\n",
    "        html_content += '</div>'\n",
    "    else:\n",
    "        html_content += '<div style=\"background: #f8d7da; padding: 15px; border-radius: 5px; border-left: 4px solid #dc3545;\">'\n",
    "        html_content += '<h3 style=\"color: #721c24; margin: 0;\">🔴 FAISABILITÉ FAIBLE</h3>'\n",
    "        html_content += '<p style=\"margin: 10px 0 0 0;\">Améliorez vos données avant de lancer l\\'entraînement.</p>'\n",
    "        html_content += '</div>'\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>💡 Recommandations</h2>\n",
    "            <ul>\n",
    "    \"\"\"\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        html_content += f'<li><span class=\"emoji\">{rec[:2]}</span>{rec[2:]}</li>'\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "            </ul>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>🚀 Prochaines Étapes</h2>\n",
    "    \"\"\"\n",
    "    \n",
    "    if result['next_steps'] == 'full_training':\n",
    "        html_content += \"\"\"\n",
    "            <ol>\n",
    "                <li>✅ Lancez la pipeline complète d'entraînement</li>\n",
    "                <li>📊 Surveillez les métriques d'accuracy pendant l'entraînement</li>\n",
    "                <li>🔄 Expérimentez avec différentes stratégies de masquage</li>\n",
    "                <li>📈 Évaluez les performances sur un ensemble de validation</li>\n",
    "            </ol>\n",
    "        \"\"\"\n",
    "    else:\n",
    "        html_content += \"\"\"\n",
    "            <ol>\n",
    "                <li>📁 Collectez plus de fichiers Excel au format JSON</li>\n",
    "                <li>🔧 Vérifiez et corrigez la structure de vos données</li>\n",
    "                <li>📊 Assurez-vous d'avoir une variété de types de cellules</li>\n",
    "                <li>🧹 Nettoyez les fichiers avec des erreurs</li>\n",
    "            </ol>\n",
    "        \"\"\"\n",
    "    \n",
    "    html_content += f\"\"\"\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>📋 Détails Techniques</h2>\n",
    "            <p><strong>Fichiers avec erreurs:</strong> {len(analysis['errors'])}</p>\n",
    "            <p><strong>Taux de réussite:</strong> {(analysis['valid_files']/analysis['total_files']*100):.1f}%</p>\n",
    "            <p><strong>Cellules par fichier (moyenne):</strong> {(analysis['total_cells']/analysis['valid_files']):.0f}</p>\n",
    "        </div>\n",
    "        \n",
    "        <footer style=\"text-align: center; margin-top: 40px; color: #666;\">\n",
    "            <p>Rapport généré par Excel Graph Transformer Pipeline</p>\n",
    "        </footer>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"📄 Rapport HTML sauvegardé: {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "# Fonction principale pour notebook\n",
    "def analyze_my_data(data_folder=\"data\", max_files=None, create_report=True):\n",
    "    \"\"\"\n",
    "    Fonction principale à appeler dans le notebook\n",
    "    \n",
    "    Usage:\n",
    "        result = analyze_my_data(\"data\", max_files=5)\n",
    "    \"\"\"\n",
    "    result = run_pipeline_notebook(data_folder, max_files, quick_mode=True)\n",
    "    \n",
    "    if result.get('success') and create_report:\n",
    "        create_data_report_notebook(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Instructions d'utilisation\n",
    "print(\"\"\"\n",
    "🎯 INSTRUCTIONS D'UTILISATION DANS LE NOTEBOOK:\n",
    "\n",
    "1. Placez vos fichiers *.json dans le dossier 'data/'\n",
    "\n",
    "2. Exécutez cette cellule pour charger les fonctions\n",
    "\n",
    "3. Lancez l'analyse avec:\n",
    "   result = analyze_my_data(\"data\")\n",
    "\n",
    "4. Pour limiter à quelques fichiers:\n",
    "   result = analyze_my_data(\"data\", max_files=5)\n",
    "\n",
    "5. Les résultats seront affichés et un rapport HTML sera créé.\n",
    "\n",
    "📊 Cette version analyse vos données et évalue la faisabilité\n",
    "   d'un entraînement complet sans lancer le transformer lourd.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fd4b94d8-163c-4709-a5b3-e3109507a17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 PIPELINE EXCEL TRANSFORMER - VERSION NOTEBOOK\n",
      "============================================================\n",
      "\n",
      "1️⃣ VÉRIFICATION DE L'ENVIRONNEMENT\n",
      "✅ PyTorch: 2.7.1+cpu\n",
      "✅ NumPy: 2.3.1\n",
      "✅ Pandas: 2.3.0\n",
      "\n",
      "2️⃣ RECHERCHE DES FICHIERS JSON DANS 'embedding/data'\n",
      "📁 Trouvé 22 fichiers JSON\n",
      "\n",
      "3️⃣ ANALYSE DES FICHIERS\n",
      "  ✅ 001c766dd64f5fdad3768694b0d382b6d988d94ba248c21fb0e7930b27dacf28.json: 3372 cellules\n",
      "  ✅ 0017285f44fbc2ca3e36d830e95a0cb049eb474cf1e17ff400f8074b6c2cea1f.json: 22176 cellules\n",
      "  ✅ 002640c96a51ecb5b9d6ce18816fd01edc27c079c7667156dfa67fc706a703d7.json: 5195 cellules\n",
      "  ✅ 0024d5eb932a3ac70e343098d6a0379394997851df9d83e0850669e23320075e.json: 601 cellules\n",
      "  ✅ 00350d054fe0d85c109eb3607d8aa1dfbef4be0ecbe8e670b7cc8f2f8b236972.json: 657 cellules\n",
      "  ✅ 00013ad39833fc129f5b79553f6d6389dad9b192130ebd754f64fc47c01cef82.json: 33 cellules\n",
      "  ✅ 0014b1c74f12b7f42dc7c267c6b56e3c90dac181ab147e71441d5f6b78c99ac8.json: 392 cellules\n",
      "  ✅ 000b7dd83566a50f96006b5948980639f3382f65b3a34d6bf757ddd3d01bf003.json: 72528 cellules\n",
      "  ✅ 0022c921e3505c0980a6f3b19c9334cc889d4715bf58783beccf68c5feb7b161.json: 2032 cellules\n",
      "  ✅ 0003d90ad249104a7ba0fb6bab08e8b9e70746e0cd2c3b30a006935b55f2a07b.json: 1556 cellules\n",
      "  ✅ 003593d1064dcd41f60b311efc7d7d0fca9c433d5f244481a3e4db279a5083ef.json: 43969 cellules\n",
      "  ✅ 001c766dd64f5fdad3768694b0d382b6d988d94ba248c21fb0e7930b27dacf28.json: 3372 cellules\n",
      "  ✅ 0017285f44fbc2ca3e36d830e95a0cb049eb474cf1e17ff400f8074b6c2cea1f.json: 22176 cellules\n",
      "  ✅ 002640c96a51ecb5b9d6ce18816fd01edc27c079c7667156dfa67fc706a703d7.json: 5195 cellules\n",
      "  ✅ 0024d5eb932a3ac70e343098d6a0379394997851df9d83e0850669e23320075e.json: 601 cellules\n",
      "  ✅ 00350d054fe0d85c109eb3607d8aa1dfbef4be0ecbe8e670b7cc8f2f8b236972.json: 657 cellules\n",
      "  ✅ 00013ad39833fc129f5b79553f6d6389dad9b192130ebd754f64fc47c01cef82.json: 33 cellules\n",
      "  ✅ 0014b1c74f12b7f42dc7c267c6b56e3c90dac181ab147e71441d5f6b78c99ac8.json: 392 cellules\n",
      "  ✅ 000b7dd83566a50f96006b5948980639f3382f65b3a34d6bf757ddd3d01bf003.json: 72528 cellules\n",
      "  ✅ 0022c921e3505c0980a6f3b19c9334cc889d4715bf58783beccf68c5feb7b161.json: 2032 cellules\n",
      "  ✅ 0003d90ad249104a7ba0fb6bab08e8b9e70746e0cd2c3b30a006935b55f2a07b.json: 1556 cellules\n",
      "  ✅ 003593d1064dcd41f60b311efc7d7d0fca9c433d5f244481a3e4db279a5083ef.json: 43969 cellules\n",
      "\n",
      "📊 RÉSUMÉ DE L'ANALYSE:\n",
      "   Fichiers analysés: 22\n",
      "   Fichiers valides: 22\n",
      "   Total cellules: 305022\n",
      "   Erreurs: 0\n",
      "\n",
      "4️⃣ TEST DE BASE SUR LES DONNÉES\n",
      "✅ Test réussi:\n",
      "   Cellules extraites: 30743\n",
      "   Feuilles trouvées: 24\n",
      "   Formules trouvées: 0\n",
      "\n",
      "5️⃣ SIMULATION D'ENTRAÎNEMENT\n",
      "📊 Types de cellules détectés:\n",
      "   empty: 23748 (77.2%)\n",
      "   text: 3606 (11.7%)\n",
      "   number: 3389 (11.0%)\n",
      "   formula: 0 (0.0%)\n",
      "\n",
      "6️⃣ SIMULATION DE PRÉDICTIONS\n",
      "🔮 Exemples de prédictions simulées:\n",
      "\n",
      "   Exemple 1 - Position (1,1) (Text):\n",
      "      Vraie valeur: 'Перечень предприятий, предоставляющих форму 2-услуги (квартальная) за 4 квартал 2024 года.                                                                                                                   Срок  предоставления до 27 января  2025 года. '\n",
      "      Top 3 prédictions:\n",
      "         🎯 Перечень предприятий (60.8%)\n",
      "         2. Перечень предприятий (25.5%)\n",
      "         3. Data                 (10.8%)\n",
      "\n",
      "   Exemple 2 - Position (3,1) (Text):\n",
      "      Vraie valeur: 'КАТО'\n",
      "      Top 3 prédictions:\n",
      "         🎯 КАТО                 (52.2%)\n",
      "         2. Text                 (31.7%)\n",
      "         3. Data                 (9.7%)\n",
      "\n",
      "   Exemple 3 - Position (3,2) (Text):\n",
      "      Vraie valeur: 'БИН/ИИН'\n",
      "      Top 3 prédictions:\n",
      "         🎯 БИН/ИИН              (46.7%)\n",
      "         2. Data                 (31.3%)\n",
      "         3. БИН/ИИН_copy         (12.1%)\n",
      "\n",
      "   Exemple 4 - Position (0,0) (Text):\n",
      "      Vraie valeur: 'Item No. (BOLD is a module)'\n",
      "      Top 3 prédictions:\n",
      "         🎯 Item No. (BOLD is a  (33.2%)\n",
      "         2. Text                 (26.9%)\n",
      "         3. Data                 (20.2%)\n",
      "\n",
      "   Exemple 5 - Position (0,1) (Empty):\n",
      "      Vraie valeur: ''\n",
      "      Top 3 prédictions:\n",
      "         1. Data                 (26.6%)\n",
      "         2. N/A                  (25.2%)\n",
      "         3. 0                    (21.8%)\n",
      "\n",
      "7️⃣ RAPPORT DE FAISABILITÉ\n",
      "📊 Score de faisabilité: 80/100\n",
      "🟢 FAISABILITÉ ÉLEVÉE - Entraînement recommandé\n",
      "\n",
      "💡 RECOMMANDATIONS:\n",
      "   ✅ Nombre suffisant de fichiers pour l'entraînement\n",
      "   ✅ Nombre suffisant de cellules pour l'entraînement\n",
      "   ⚠️ Aucune formule - apprentissage limité aux valeurs\n",
      "   ✅ Aucune erreur de format\n",
      "\n",
      "8️⃣ PROCHAINES ÉTAPES\n",
      "Pour lancer l'entraînement complet:\n",
      "1. 🚀 Utilisez la pipeline complète avec vos données\n",
      "2. 📊 Ajustez les hyperparamètres selon la taille de vos données\n",
      "3. 🔄 Itérez sur différentes stratégies de masquage\n",
      "4. 📈 Évaluez les performances sur un ensemble de test\n",
      "\n",
      "🎉 ANALYSE TERMINÉE!\n",
      "📋 Résultat stocké dans la variable 'result'\n",
      "📄 Rapport HTML sauvegardé: data_report_notebook.html\n"
     ]
    }
   ],
   "source": [
    "# Analyser tous vos fichiers JSON\n",
    "result = analyze_my_data(\"embedding/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1141516f-0963-406a-b360-352e753ae389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 ENTRAÎNEMENT OPTIMISÉ SUR VOS DONNÉES\n",
      "============================================================\n",
      "\n",
      "1️⃣ CONFIGURATION DU MODÈLE\n",
      "   Embedding dimension: 256\n",
      "   Position max: 2000\n",
      "   Vocabulaire couleurs: 300\n",
      "\n",
      "2️⃣ CRÉATION DU TRANSFORMER BUILDER\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'max_value_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 238\u001b[39m\n\u001b[32m    235\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m💡 Lancez d\u001b[39m\u001b[33m'\u001b[39m\u001b[33mabord l\u001b[39m\u001b[33m'\u001b[39m\u001b[33mentraînement avec run_optimized_training()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# Lancer l'entraînement\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m model, trainer = \u001b[43mrun_optimized_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mrun_optimized_training\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# 2. Création du transformer builder\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m2️⃣ CRÉATION DU TRANSFORMER BUILDER\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m transformer_builder = \u001b[43mJSONToGraphTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_cells_per_sheet\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_empty_cells\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     52\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# 3. Modèle avec encodage positionnel corrigé\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m3️⃣ INITIALISATION DU MODÈLE\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mJSONToGraphTransformer.__init__\u001b[39m\u001b[34m(self, embedding_config, max_cells_per_sheet, include_empty_cells, graph_config)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mself\u001b[39m.graph_config = graph_config\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Initialiser les composants\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28mself\u001b[39m.cell_embedder = \u001b[43mExcelCellEmbedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mself\u001b[39m.graph_embedder = ExcelGraphEmbedder(\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mself\u001b[39m.cell_embedder, \n\u001b[32m     44\u001b[39m     edge_embedding_dim=\u001b[32m64\u001b[39m\n\u001b[32m     45\u001b[39m )\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Configurer le graph builder\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mExcelCellEmbedder.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mself\u001b[39m.type_embedding = nn.Embedding(\u001b[32m4\u001b[39m, config.type_embedding_dim)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Contenu (valeur/formule)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28mself\u001b[39m.value_encoder = \u001b[43mValueEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Formatage booléen\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mself\u001b[39m.bool_embedding = nn.Embedding(\u001b[32m2\u001b[39m, \u001b[32m8\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 216\u001b[39m, in \u001b[36mValueEncoder.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.config = config\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28mself\u001b[39m.max_tokens = \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_value_tokens\u001b[49m  \u001b[38;5;66;03m# FIXÉ\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.token_dim = config.value_token_dim    \u001b[38;5;66;03m# FIXÉ\u001b[39;00m\n\u001b[32m    218\u001b[39m \u001b[38;5;28mself\u001b[39m.output_dim = \u001b[38;5;28mself\u001b[39m.max_tokens * \u001b[38;5;28mself\u001b[39m.token_dim  \u001b[38;5;66;03m# 8 * 32 = 256D\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Config' object has no attribute 'max_value_tokens'"
     ]
    }
   ],
   "source": [
    "# Lancer la pipeline complète sur vos données\n",
    "# Cette fonction utilisera vos 22 fichiers pour un vrai entraînement\n",
    "\n",
    "# Configuration optimisée pour vos données\n",
    "class YourDataConfig:\n",
    "    \"\"\"Configuration spécialement adaptée à vos données\"\"\"\n",
    "    embedding_dim = 256\n",
    "    position_embedding_dim = 48\n",
    "    type_embedding_dim = 16\n",
    "    max_position = 2000  # Pour vos gros fichiers\n",
    "    color_vocab_size = 300\n",
    "    align_vocab_size = 5\n",
    "    border_vocab_size = 10\n",
    "    font_vocab_size = 30\n",
    "    max_font_size = 72\n",
    "\n",
    "def run_optimized_training():\n",
    "    \"\"\"Lance l'entraînement optimisé sur vos données avec corrections\"\"\"\n",
    "    \n",
    "    print(\"🚀 ENTRAÎNEMENT OPTIMISÉ SUR VOS DONNÉES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Configuration\n",
    "    print(\"\\n1️⃣ CONFIGURATION DU MODÈLE\")\n",
    "    \n",
    "    # Configuration corrigée (en tant qu'objet, pas classe)\n",
    "    class Config:\n",
    "        def __init__(self):\n",
    "            self.embedding_dim = 256  # Divisible par 4\n",
    "            self.position_embedding_dim = 32\n",
    "            self.type_embedding_dim = 16\n",
    "            self.max_position = 2000\n",
    "            self.color_vocab_size = 300\n",
    "            self.align_vocab_size = 5\n",
    "            self.border_vocab_size = 10\n",
    "            self.font_vocab_size = 30\n",
    "            self.max_font_size = 72\n",
    "    \n",
    "    config = Config()\n",
    "    \n",
    "    print(f\"   Embedding dimension: {config.embedding_dim}\")\n",
    "    print(f\"   Position max: {config.max_position}\")\n",
    "    print(f\"   Vocabulaire couleurs: {config.color_vocab_size}\")\n",
    "    \n",
    "    # 2. Création du transformer builder\n",
    "    print(\"\\n2️⃣ CRÉATION DU TRANSFORMER BUILDER\")\n",
    "    \n",
    "    transformer_builder = JSONToGraphTransformer(\n",
    "        embedding_config=config,\n",
    "        max_cells_per_sheet=300,\n",
    "        include_empty_cells=False\n",
    "    )\n",
    "    \n",
    "    # 3. Modèle avec encodage positionnel corrigé\n",
    "    print(\"\\n3️⃣ INITIALISATION DU MODÈLE\")\n",
    "    \n",
    "    # Utiliser le modèle existant mais corriger le problème d'encodage positionnel\n",
    "    excel_transformer = ExcelGraphTransformer(\n",
    "        config, \n",
    "        num_layers=4,  # Réduit pour commencer\n",
    "        n_heads=8, \n",
    "        d_ff=768\n",
    "    )\n",
    "    \n",
    "    # CORRECTION : Remplacer l'encodage positionnel problématique\n",
    "    excel_transformer.pos_encoding_2d = PositionalEncoding2D(config.embedding_dim)\n",
    "    \n",
    "    predictor = MaskedCellPredictor(excel_transformer, num_candidates=10)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in predictor.parameters())\n",
    "    print(f\"✅ Modèle créé: {total_params:,} paramètres\")\n",
    "    \n",
    "    # 4. Préparation des données\n",
    "    print(\"\\n4️⃣ PRÉPARATION DES DONNÉES\")\n",
    "    \n",
    "    # Charger vos fichiers (adapter le chemin si nécessaire)\n",
    "    try:\n",
    "        json_files, file_paths = load_json_files(\"embedding/data\")  # Votre dossier\n",
    "    except:\n",
    "        try:\n",
    "            json_files, file_paths = load_json_files(\"data\")  # Dossier alternatif\n",
    "        except:\n",
    "            print(\"❌ Impossible de trouver le dossier de données\")\n",
    "            return None, None\n",
    "    \n",
    "    print(f\"   Fichiers chargés: {len(json_files)}\")\n",
    "    \n",
    "    # Limiter pour le test initial\n",
    "    if len(json_files) > 5:  # Encore plus conservateur\n",
    "        json_files = json_files[:5]\n",
    "        print(f\"   Limitation à {len(json_files)} fichiers pour le test\")\n",
    "    \n",
    "    # Créer le dataset\n",
    "    try:\n",
    "        dataset = ExcelMaskedDataset(\n",
    "            json_files,\n",
    "            transformer_builder,\n",
    "            mask_ratio=0.1,  # Masquer seulement 10%\n",
    "            num_candidates=10\n",
    "        )\n",
    "        print(f\"✅ Dataset créé: {len(dataset)} échantillons\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur dataset: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"❌ Dataset vide après filtrage!\")\n",
    "        return None, None\n",
    "    \n",
    "    # 5. Entraînement\n",
    "    print(\"\\n5️⃣ LANCEMENT DE L'ENTRAÎNEMENT\")\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    trainer = MaskedPredictionTrainer(predictor)\n",
    "    \n",
    "    # Entraînement sur 3 époques d'abord pour tester\n",
    "    num_epochs = 3\n",
    "    print(f\"   Entraînement sur {num_epochs} époques de test...\")\n",
    "    \n",
    "    training_results = []\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\n📚 Époque {epoch + 1}/{num_epochs}:\")\n",
    "            \n",
    "            epoch_metrics = trainer.train_epoch(dataloader, epoch)\n",
    "            training_results.append(epoch_metrics)\n",
    "            \n",
    "            # Affichage des métriques\n",
    "            print(f\"   Loss: {epoch_metrics['loss']:.4f}\")\n",
    "            print(f\"   Accuracy: {epoch_metrics['accuracy']:.1%}\")\n",
    "            print(f\"   Top-3: {epoch_metrics['top3_accuracy']:.1%}\")\n",
    "            print(f\"   Top-5: {epoch_metrics['top5_accuracy']:.1%}\")\n",
    "            print(f\"   Prédictions: {epoch_metrics['total_predictions']}\")\n",
    "            \n",
    "            if epoch_metrics['accuracy'] > best_accuracy:\n",
    "                best_accuracy = epoch_metrics['accuracy']\n",
    "                print(f\"   ⭐ Nouveau record: {best_accuracy:.1%}\")\n",
    "                \n",
    "                # Sauvegarder le meilleur modèle\n",
    "                torch.save(predictor.state_dict(), \"best_model_your_data.pt\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n⏱️ Entraînement terminé en {training_time:.1f}s\")\n",
    "        print(f\"🏆 Meilleure accuracy: {best_accuracy:.1%}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur pendant l'entraînement: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "    \n",
    "    # 6. Test rapide\n",
    "    print(\"\\n6️⃣ TEST RAPIDE DU MODÈLE ENTRAÎNÉ\")\n",
    "    \n",
    "    if len(dataset) > 0:\n",
    "        try:\n",
    "            # Prendre un échantillon\n",
    "            sample = dataset[0]\n",
    "            if sample is not None:\n",
    "                graph = sample['graph']\n",
    "                mask_indices = sample['mask_indices']\n",
    "                candidates = sample['candidates']\n",
    "                \n",
    "                # Prédiction\n",
    "                predictions = predictor.predict_top_candidates(graph, mask_indices, candidates)\n",
    "                \n",
    "                print(\"🔮 Exemple de prédiction après entraînement:\")\n",
    "                for i, pred in enumerate(predictions[:2]):  # 2 premiers\n",
    "                    print(f\"\\n   Cellule {pred['cell_index']}:\")\n",
    "                    for rank, cand in enumerate(pred['candidates_ranked'][:3]):\n",
    "                        print(f\"     {rank+1}. {cand['value'][:30]:30s} ({cand['probability']:.1%})\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur test: {e}\")\n",
    "    \n",
    "    # 7. Résumé final\n",
    "    print(f\"\\n7️⃣ RÉSUMÉ FINAL\")\n",
    "    print(f\"   Fichiers traités: {len(json_files)}\")\n",
    "    print(f\"   Échantillons d'entraînement: {len(dataset)}\")\n",
    "    print(f\"   Époques complétées: {len(training_results)}\")\n",
    "    print(f\"   Meilleure accuracy: {best_accuracy:.1%}\")\n",
    "    print(f\"   Modèle sauvegardé: best_model_your_data.pt\")\n",
    "    \n",
    "    return predictor, training_results\n",
    "\n",
    "def quick_test_trained_model():\n",
    "    \"\"\"Test rapide du modèle entraîné\"\"\"\n",
    "    \n",
    "    print(\"\\n🧪 TEST DU MODÈLE ENTRAÎNÉ\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Charger le modèle sauvegardé\n",
    "        config = YourDataConfig()\n",
    "        \n",
    "        # Recréer l'architecture\n",
    "        excel_transformer = ExcelGraphTransformer(config, num_layers=4, n_heads=8)\n",
    "        predictor = MaskedCellPredictor(excel_transformer, num_candidates=10)\n",
    "        \n",
    "        # Charger les poids\n",
    "        predictor.load_state_dict(torch.load(\"best_model_your_data.pt\"))\n",
    "        predictor.eval()\n",
    "        \n",
    "        print(\"✅ Modèle chargé avec succès\")\n",
    "        \n",
    "        # Test sur un fichier de vos données\n",
    "        json_files, file_paths = load_json_files(\"data\")\n",
    "        if json_files:\n",
    "            test_file = json_files[0]\n",
    "            filename = os.path.basename(file_paths[0])\n",
    "            \n",
    "            print(f\"🔍 Test sur: {filename}\")\n",
    "            \n",
    "            # Créer l'évaluateur\n",
    "            transformer_builder = JSONToGraphTransformer(embedding_config=config)\n",
    "            evaluator = ExcelMaskedEvaluator(predictor, transformer_builder)\n",
    "            \n",
    "            # Test interactif\n",
    "            evaluator.interactive_test(test_file)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur: {e}\")\n",
    "        print(\"💡 Lancez d'abord l'entraînement avec run_optimized_training()\")\n",
    "\n",
    "# Lancer l'entraînement\n",
    "model, trainer = run_optimized_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c582ab24-cef2-463b-8240-2ebe948b3101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 LANCEMENT DU TEST COMPLET\n",
      "\n",
      "🔍 TEST AVEC VOS DONNÉES RÉELLES\n",
      "==================================================\n",
      "🚀 ENTRAÎNEMENT CORRIGÉ - Version Complète\n",
      "============================================================\n",
      "\n",
      "1️⃣ CONFIGURATION\n",
      "✅ Configuration créée: embedding_dim=256\n",
      "\n",
      "2️⃣ CONSTRUCTION ET TEST DU MODÈLE\n",
      "   Dimensions avant projection: 176\n",
      "✅ Modèle créé:\n",
      "   Paramètres totaux: 1,969,482\n",
      "\n",
      "3️⃣ TEST AVEC DES DONNÉES FACTICES\n",
      "✅ Embeddings générés: torch.Size([4, 256])\n",
      "✅ Transformer output: torch.Size([1, 4, 256])\n",
      "✅ Prédiction logits: torch.Size([2, 10])\n",
      "✅ Prédictions formatées: 2 cellules\n",
      "\n",
      "   📍 Cellule 0 (vraie valeur: 'Hello'):\n",
      "     1. Hey        (16.4%)\n",
      "     2. Bonjour    (13.9%)\n",
      "     3. Text       (11.0%)\n",
      "\n",
      "   📍 Cellule 2 (vraie valeur: '42'):\n",
      "     1. 33         (13.6%)\n",
      "     2. 1          (13.5%)\n",
      "     3. 999        (11.2%)\n",
      "\n",
      "🎉 SUCCÈS! Le modèle fonctionne correctement.\n",
      "📋 Toutes les dimensions sont compatibles.\n",
      "📁 Trouvé 11 fichiers dans embedding/data\n",
      "📄 Test avec: 001c766dd64f5fdad3768694b0d382b6d988d94ba248c21fb0e7930b27dacf28.json\n",
      "📊 Cellules extraites: 3372\n",
      "✅ Embedding de vos données: torch.Size([5, 256])\n",
      "✅ Prédiction sur vos données réussie!\n",
      "   Cellule testée: 'Перечень предприятий, предоставляющих форму 2-услуги (квартальная) за 4 квартал 2024 года.                                                                                                                   Срок  предоставления до 27 января  2025 года. ' (type 1)\n",
      "   Top 3 prédictions:\n",
      "     - candidate_6 (13.3%)\n",
      "     - candidate_9 (11.5%)\n",
      "     - data (11.1%)\n",
      "\n",
      "🎉 SUCCÈS! Votre modèle est compatible avec vos données JSON!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def run_corrected_training():\n",
    "    \"\"\"Version corrigée de l'entraînement sans erreurs de dimensions ou d'imports\"\"\"\n",
    "    \n",
    "    print(\"🚀 ENTRAÎNEMENT CORRIGÉ - Version Complète\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Configuration simple et claire\n",
    "    print(\"\\n1️⃣ CONFIGURATION\")\n",
    "    \n",
    "    class SimpleConfig:\n",
    "        def __init__(self):\n",
    "            # Dimensions principales\n",
    "            self.embedding_dim = 256\n",
    "            self.position_embedding_dim = 32\n",
    "            self.type_embedding_dim = 16\n",
    "            \n",
    "            # Vocabulaires\n",
    "            self.max_position = 1000\n",
    "            self.max_font_size = 72\n",
    "            self.color_vocab_size = 100\n",
    "            self.value_vocab_size = 10000\n",
    "    \n",
    "    config = SimpleConfig()\n",
    "    print(f\"✅ Configuration créée: embedding_dim={config.embedding_dim}\")\n",
    "    \n",
    "    # 2. Cellule de test simple\n",
    "    class TestCell:\n",
    "        def __init__(self, row=0, col=0, value=\"test\", cell_type=1):\n",
    "            self.row = row\n",
    "            self.col = col\n",
    "            self.cell_type = cell_type\n",
    "            self.raw_value = value\n",
    "            self.formula = \"\"\n",
    "            self.style_id = \"\"\n",
    "            # Attributs de style avec valeurs par défaut\n",
    "            self.bold = False\n",
    "            self.italic = False\n",
    "            self.underline = False\n",
    "            self.strike = False\n",
    "            self.font_size = 11.0\n",
    "            self.font_family = \"Calibri\"\n",
    "            self.text_color = \"#000000\"\n",
    "            self.background_color = \"#FFFFFF\"\n",
    "            self.horizontal_align = 0\n",
    "            self.vertical_align = 0\n",
    "            self.text_wrap = False\n",
    "            self.text_rotation = 0\n",
    "            self.border_top = 0\n",
    "            self.border_bottom = 0\n",
    "            self.border_left = 0\n",
    "            self.border_right = 0\n",
    "            self.is_merged = False\n",
    "            self.merge_range = (0, 0, 0, 0)\n",
    "            self.sheet_name = \"Sheet1\"\n",
    "    \n",
    "    # 3. Embedder de cellules simplifié\n",
    "    class SimpleCellEmbedder(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            \n",
    "            # Embeddings de base\n",
    "            self.row_embedding = nn.Embedding(config.max_position, config.position_embedding_dim)\n",
    "            self.col_embedding = nn.Embedding(config.max_position, config.position_embedding_dim)\n",
    "            self.type_embedding = nn.Embedding(4, config.type_embedding_dim)  # 0=empty, 1=text, 2=number, 3=formula\n",
    "            \n",
    "            # Valeur simplifiée (hash du contenu)\n",
    "            self.value_embedding = nn.Embedding(config.value_vocab_size, 64)\n",
    "            \n",
    "            # Style simplifié\n",
    "            self.style_embedding = nn.Embedding(100, 32)  # 100 styles possibles\n",
    "            \n",
    "            # Calcul des dimensions EXACT\n",
    "            total_dim = (\n",
    "                config.position_embedding_dim +  # row = 32\n",
    "                config.position_embedding_dim +  # col = 32  \n",
    "                config.type_embedding_dim +      # type = 16\n",
    "                64 +                             # value = 64\n",
    "                32                               # style = 32\n",
    "            )  # Total = 176\n",
    "            \n",
    "            print(f\"   Dimensions avant projection: {total_dim}\")\n",
    "            \n",
    "            # Projection vers la dimension finale\n",
    "            self.projection = nn.Linear(total_dim, config.embedding_dim)\n",
    "            self.layer_norm = nn.LayerNorm(config.embedding_dim)\n",
    "        \n",
    "        def forward(self, cells):\n",
    "            if not isinstance(cells, list):\n",
    "                cells = [cells]\n",
    "            \n",
    "            embeddings = []\n",
    "            \n",
    "            for cell in cells:\n",
    "                # Position (clampée pour éviter les erreurs)\n",
    "                row_idx = min(max(cell.row, 0), self.config.max_position - 1)\n",
    "                col_idx = min(max(cell.col, 0), self.config.max_position - 1)\n",
    "                \n",
    "                row_emb = self.row_embedding(torch.tensor(row_idx))\n",
    "                col_emb = self.col_embedding(torch.tensor(col_idx))\n",
    "                \n",
    "                # Type de cellule\n",
    "                type_emb = self.type_embedding(torch.tensor(cell.cell_type))\n",
    "                \n",
    "                # Valeur (hash du contenu)\n",
    "                content = str(cell.raw_value) + str(cell.formula)\n",
    "                value_hash = abs(hash(content)) % self.config.value_vocab_size\n",
    "                value_emb = self.value_embedding(torch.tensor(value_hash))\n",
    "                \n",
    "                # Style (hash du style_id)\n",
    "                style_hash = abs(hash(cell.style_id)) % 100 if cell.style_id else 0\n",
    "                style_emb = self.style_embedding(torch.tensor(style_hash))\n",
    "                \n",
    "                # Concaténer tous les embeddings\n",
    "                cell_embedding = torch.cat([\n",
    "                    row_emb,    # 32 dim\n",
    "                    col_emb,    # 32 dim\n",
    "                    type_emb,   # 16 dim\n",
    "                    value_emb,  # 64 dim\n",
    "                    style_emb   # 32 dim\n",
    "                ], dim=0)       # Total: 176 dim\n",
    "                \n",
    "                embeddings.append(cell_embedding)\n",
    "            \n",
    "            # Stack et projeter\n",
    "            batch_embeddings = torch.stack(embeddings)  # [batch_size, 176]\n",
    "            projected = self.projection(batch_embeddings)  # [batch_size, 256]\n",
    "            normalized = self.layer_norm(projected)\n",
    "            \n",
    "            return normalized.squeeze(0) if len(cells) == 1 else normalized\n",
    "    \n",
    "    # 4. Transformer simple\n",
    "    class SimpleTransformer(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.d_model = config.embedding_dim\n",
    "            self.cell_embedder = SimpleCellEmbedder(config)\n",
    "            \n",
    "            # Couche transformer simple\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=self.d_model,\n",
    "                nhead=8,\n",
    "                dim_feedforward=512,\n",
    "                dropout=0.1,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "            \n",
    "            # Encodage positionnel simple (paramètre apprenable)\n",
    "            self.pos_encoding = nn.Parameter(torch.randn(500, self.d_model) * 0.1)\n",
    "        \n",
    "        def forward(self, cells):\n",
    "            # Embedder les cellules\n",
    "            embeddings = self.cell_embedder(cells)  # [num_cells, d_model]\n",
    "            \n",
    "            # Ajouter dimension batch si nécessaire\n",
    "            if len(embeddings.shape) == 1:\n",
    "                embeddings = embeddings.unsqueeze(0)  # [1, d_model]\n",
    "            if len(embeddings.shape) == 2:\n",
    "                embeddings = embeddings.unsqueeze(0)  # [1, num_cells, d_model]\n",
    "            \n",
    "            batch_size, seq_len, d_model = embeddings.shape\n",
    "            \n",
    "            # Ajouter encodage positionnel\n",
    "            pos_emb = self.pos_encoding[:seq_len].unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            embeddings = embeddings + pos_emb\n",
    "            \n",
    "            # Passer dans le transformer\n",
    "            output = self.transformer(embeddings)\n",
    "            \n",
    "            return output\n",
    "    \n",
    "    # 5. Modèle de prédiction\n",
    "    class SimplePredictor(nn.Module):\n",
    "        def __init__(self, transformer, num_candidates=10):\n",
    "            super().__init__()\n",
    "            self.transformer = transformer\n",
    "            self.num_candidates = num_candidates\n",
    "            \n",
    "            # Tête de classification\n",
    "            self.classification_head = nn.Sequential(\n",
    "                nn.Linear(transformer.d_model, transformer.d_model // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(transformer.d_model // 2, num_candidates)\n",
    "            )\n",
    "        \n",
    "        def forward(self, cells, mask_indices, candidates):\n",
    "            # Forward pass du transformer\n",
    "            transformer_output = self.transformer(cells)  # [1, num_cells, d_model]\n",
    "            \n",
    "            if len(mask_indices) == 0:\n",
    "                return {'logits': torch.empty(0, self.num_candidates)}\n",
    "            \n",
    "            # Extraire les embeddings des cellules masquées\n",
    "            masked_embeddings = transformer_output[0, mask_indices]  # [num_masked, d_model]\n",
    "            \n",
    "            # Classification\n",
    "            logits = self.classification_head(masked_embeddings)  # [num_masked, num_candidates]\n",
    "            \n",
    "            return {\n",
    "                'logits': logits,\n",
    "                'embeddings': masked_embeddings\n",
    "            }\n",
    "        \n",
    "        def predict_candidates(self, cells, mask_indices, candidates):\n",
    "            \"\"\"Version simplifiée de prédiction\"\"\"\n",
    "            with torch.no_grad():\n",
    "                output = self.forward(cells, mask_indices, candidates)\n",
    "                logits = output['logits']\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                \n",
    "                predictions = []\n",
    "                for i, (mask_idx, cell_candidates) in enumerate(zip(mask_indices, candidates)):\n",
    "                    probs = probabilities[i].cpu().numpy()\n",
    "                    \n",
    "                    # Ordonner par probabilité\n",
    "                    sorted_indices = torch.argsort(probabilities[i], descending=True)\n",
    "                    \n",
    "                    ranked_candidates = []\n",
    "                    for rank, idx in enumerate(sorted_indices[:5]):  # Top 5\n",
    "                        ranked_candidates.append({\n",
    "                            'value': cell_candidates[idx] if idx < len(cell_candidates) else f\"candidate_{idx}\",\n",
    "                            'probability': float(probabilities[i][idx]),\n",
    "                            'rank': rank + 1\n",
    "                        })\n",
    "                    \n",
    "                    predictions.append({\n",
    "                        'cell_index': mask_idx,\n",
    "                        'candidates_ranked': ranked_candidates\n",
    "                    })\n",
    "                \n",
    "                return predictions\n",
    "    \n",
    "    # 6. TEST DU MODÈLE\n",
    "    print(\"\\n2️⃣ CONSTRUCTION ET TEST DU MODÈLE\")\n",
    "    \n",
    "    try:\n",
    "        # Créer les modèles\n",
    "        transformer = SimpleTransformer(config)\n",
    "        predictor = SimplePredictor(transformer, num_candidates=10)\n",
    "        \n",
    "        print(f\"✅ Modèle créé:\")\n",
    "        total_params = sum(p.numel() for p in predictor.parameters())\n",
    "        print(f\"   Paramètres totaux: {total_params:,}\")\n",
    "        \n",
    "        # 7. TEST AVEC DES CELLULES FACTICES\n",
    "        print(\"\\n3️⃣ TEST AVEC DES DONNÉES FACTICES\")\n",
    "        \n",
    "        # Créer des cellules de test\n",
    "        test_cells = [\n",
    "            TestCell(row=0, col=0, value=\"Hello\", cell_type=1),\n",
    "            TestCell(row=0, col=1, value=\"World\", cell_type=1),\n",
    "            TestCell(row=1, col=0, value=\"42\", cell_type=2),\n",
    "            TestCell(row=1, col=1, value=\"\", cell_type=0)  # cellule vide\n",
    "        ]\n",
    "        \n",
    "        # Test d'embedding\n",
    "        with torch.no_grad():\n",
    "            embeddings = transformer.cell_embedder(test_cells)\n",
    "            print(f\"✅ Embeddings générés: {embeddings.shape}\")\n",
    "            \n",
    "            # Test du transformer\n",
    "            transformer_output = transformer(test_cells)\n",
    "            print(f\"✅ Transformer output: {transformer_output.shape}\")\n",
    "            \n",
    "            # Test de prédiction\n",
    "            mask_indices = [0, 2]  # Masquer les cellules 0 et 2\n",
    "            candidates = [\n",
    "                [\"Hello\", \"Hi\", \"Hey\", \"Bonjour\", \"Text\", \"Data\", \"Value\", \"Item\", \"Word\", \"Cell\"],\n",
    "                [\"42\", \"0\", \"100\", \"1\", \"999\", \"3.14\", \"50\", \"200\", \"75\", \"33\"]\n",
    "            ]\n",
    "            \n",
    "            prediction_output = predictor(test_cells, mask_indices, candidates)\n",
    "            print(f\"✅ Prédiction logits: {prediction_output['logits'].shape}\")\n",
    "            \n",
    "            # Test de prédiction avec candidats\n",
    "            predictions = predictor.predict_candidates(test_cells, mask_indices, candidates)\n",
    "            print(f\"✅ Prédictions formatées: {len(predictions)} cellules\")\n",
    "            \n",
    "            # Afficher les résultats\n",
    "            for i, pred in enumerate(predictions):\n",
    "                cell_idx = pred['cell_index']\n",
    "                test_cell = test_cells[cell_idx]\n",
    "                true_value = test_cell.raw_value\n",
    "                \n",
    "                print(f\"\\n   📍 Cellule {cell_idx} (vraie valeur: '{true_value}'):\")\n",
    "                for rank, candidate in enumerate(pred['candidates_ranked'][:3]):\n",
    "                    marker = \"🎯\" if candidate['value'] == true_value else f\"{rank+1}.\"\n",
    "                    print(f\"     {marker} {candidate['value']:10s} ({candidate['probability']:.1%})\")\n",
    "        \n",
    "        print(f\"\\n🎉 SUCCÈS! Le modèle fonctionne correctement.\")\n",
    "        print(f\"📋 Toutes les dimensions sont compatibles.\")\n",
    "        \n",
    "        return predictor, transformer, config\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de la construction: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "# 8. FONCTION POUR TESTER AVEC VOS VRAIES DONNÉES\n",
    "def test_with_real_data():\n",
    "    \"\"\"Test avec vos fichiers JSON réels\"\"\"\n",
    "    print(\"\\n🔍 TEST AVEC VOS DONNÉES RÉELLES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    predictor, transformer, config = run_corrected_training()\n",
    "    \n",
    "    if predictor is None:\n",
    "        print(\"❌ Échec de la création du modèle\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Essayer de charger vos fichiers\n",
    "        data_folders = [\"embedding/data\", \"data\", \"./data\"]\n",
    "        json_files = []\n",
    "        \n",
    "        for folder in data_folders:\n",
    "            if os.path.exists(folder):\n",
    "                pattern = os.path.join(folder, \"*.json\")\n",
    "                found_files = glob.glob(pattern)\n",
    "                if found_files:\n",
    "                    print(f\"📁 Trouvé {len(found_files)} fichiers dans {folder}\")\n",
    "                    \n",
    "                    # Charger le premier fichier\n",
    "                    with open(found_files[0], 'r', encoding='utf-8') as f:\n",
    "                        sample_data = json.load(f)\n",
    "                    \n",
    "                    print(f\"📄 Test avec: {os.path.basename(found_files[0])}\")\n",
    "                    \n",
    "                    # Parser avec votre classe existante\n",
    "                    cells = ExcelParser.parse_excel_json(sample_data)\n",
    "                    print(f\"📊 Cellules extraites: {len(cells)}\")\n",
    "                    \n",
    "                    if cells and len(cells) > 0:\n",
    "                        # Prendre quelques cellules pour test\n",
    "                        test_cells = cells[:5]\n",
    "                        \n",
    "                        # Test d'embedding\n",
    "                        with torch.no_grad():\n",
    "                            embeddings = transformer.cell_embedder(test_cells)\n",
    "                            print(f\"✅ Embedding de vos données: {embeddings.shape}\")\n",
    "                            \n",
    "                            # Test de prédiction simple\n",
    "                            mask_indices = [0] if len(test_cells) > 0 else []\n",
    "                            candidates = [[\"test\", \"data\", \"value\", \"item\", \"text\"]]\n",
    "                            \n",
    "                            if mask_indices:\n",
    "                                predictions = predictor.predict_candidates(test_cells, mask_indices, candidates)\n",
    "                                print(f\"✅ Prédiction sur vos données réussie!\")\n",
    "                                \n",
    "                                # Afficher le résultat\n",
    "                                pred = predictions[0]\n",
    "                                real_cell = test_cells[pred['cell_index']]\n",
    "                                print(f\"   Cellule testée: '{real_cell.raw_value}' (type {real_cell.cell_type})\")\n",
    "                                print(f\"   Top 3 prédictions:\")\n",
    "                                for cand in pred['candidates_ranked'][:3]:\n",
    "                                    print(f\"     - {cand['value']} ({cand['probability']:.1%})\")\n",
    "                    \n",
    "                    print(f\"\\n🎉 SUCCÈS! Votre modèle est compatible avec vos données JSON!\")\n",
    "                    return predictor\n",
    "                    \n",
    "        print(\"❌ Aucun fichier JSON trouvé dans les dossiers standards\")\n",
    "        print(\"💡 Placez vos fichiers *.json dans un dossier 'data/' ou 'embedding/data/'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur avec vos données: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return predictor\n",
    "\n",
    "# Lancer le test complet\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 LANCEMENT DU TEST COMPLET\")\n",
    "    model = test_with_real_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a7a91513-234c-47a6-820a-ffe34c07366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Version corrigée de l'entraînement\n",
      "🚀 ENTRAÎNEMENT CORRIGÉ - Version Debug\n",
      "============================================================\n",
      "\n",
      "1️⃣ CHARGEMENT DES DONNÉES\n",
      "📁 Dossier trouvé: embedding/data (11 fichiers)\n",
      "✅ 11 fichiers chargés\n",
      "\n",
      "2️⃣ CRÉATION DU MODÈLE\n",
      "✅ Modèle créé: 6,920,906 paramètres\n",
      "\n",
      "3️⃣ CRÉATION ET DEBUG DU DATASET\n",
      "📚 Création du dataset avec 8 fichiers...\n",
      "✅ Dataset créé: 8 échantillons valides\n",
      "🔍 DEBUG DATASET\n",
      "==============================\n",
      "\n",
      "Échantillon 0:\n",
      "  Type: <class 'dict'>\n",
      "  - cells: <class 'list'>\n",
      "    Length: 20\n",
      "  - mask_indices: <class 'list'>\n",
      "    Value: [16, 12, 0]\n",
      "  - candidates: <class 'list'>\n",
      "  - labels: <class 'list'>\n",
      "    Value: [3, 3, 0]\n",
      "  - file_id: <class 'int'>\n",
      "\n",
      "Échantillon 1:\n",
      "  Type: <class 'dict'>\n",
      "  - cells: <class 'list'>\n",
      "    Length: 20\n",
      "  - mask_indices: <class 'list'>\n",
      "    Value: [11, 5, 14]\n",
      "  - candidates: <class 'list'>\n",
      "  - labels: <class 'list'>\n",
      "    Value: [0, 0, 0]\n",
      "  - file_id: <class 'int'>\n",
      "\n",
      "🧪 TEST D'UN ÉCHANTILLON\n",
      "✅ Échantillon récupéré: 20 cellules, 3 masquées\n",
      "✅ Forward pass réussi: torch.Size([3, 10])\n",
      "\n",
      "4️⃣ LANCEMENT DE L'ENTRAÎNEMENT CORRIGÉ\n",
      "🎯 Entraînement sur 5 époques\n",
      "📊 8 batches par époque\n",
      "\n",
      "📚 Époque 1/5\n",
      "----------------------------------------\n",
      "    Batch   0: Loss 2.5479, Acc 0.0% [3 prédictions]\n",
      "    Batch   5: Loss 2.0219, Acc 100.0% [3 prédictions]\n",
      "    💾 Nouveau record sauvegardé: 45.8%\n",
      "  📈 Train - Loss: 2.1824, Acc: 45.8%, Batches réussis: 8/8, Time: 2.3s\n",
      "\n",
      "📚 Époque 2/5\n",
      "----------------------------------------\n",
      "    Batch   0: Loss 1.8199, Acc 100.0% [3 prédictions]\n",
      "    Batch   5: Loss 1.3810, Acc 100.0% [3 prédictions]\n",
      "    💾 Nouveau record sauvegardé: 100.0%\n",
      "  📈 Train - Loss: 1.5230, Acc: 100.0%, Batches réussis: 8/8, Time: 1.8s\n",
      "\n",
      "📚 Époque 3/5\n",
      "----------------------------------------\n",
      "    Batch   0: Loss 1.1147, Acc 100.0% [3 prédictions]\n",
      "    Batch   5: Loss 0.7315, Acc 100.0% [3 prédictions]\n",
      "  📈 Train - Loss: 0.9977, Acc: 91.7%, Batches réussis: 8/8, Time: 1.2s\n",
      "\n",
      "📚 Époque 4/5\n",
      "----------------------------------------\n",
      "    Batch   0: Loss 1.2966, Acc 66.7% [3 prédictions]\n",
      "    Batch   5: Loss 1.0329, Acc 66.7% [3 prédictions]\n",
      "  📈 Train - Loss: 0.7261, Acc: 87.5%, Batches réussis: 8/8, Time: 1.2s\n",
      "\n",
      "📚 Époque 5/5\n",
      "----------------------------------------\n",
      "    Batch   0: Loss 1.1611, Acc 66.7% [3 prédictions]\n",
      "    Batch   5: Loss 0.0965, Acc 100.0% [3 prédictions]\n",
      "  📈 Train - Loss: 0.3953, Acc: 91.7%, Batches réussis: 8/8, Time: 1.5s\n",
      "\n",
      "5️⃣ RÉSULTATS\n",
      "🏆 Accuracy finale: 91.7%\n",
      "🏆 Meilleure accuracy: 100.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ExcelDataset(Dataset):\n",
    "    \"\"\"Dataset corrigé pour l'entraînement avec vos données Excel JSON\"\"\"\n",
    "    \n",
    "    def __init__(self, json_files, max_cells_per_file=50, mask_ratio=0.15, num_candidates=10):\n",
    "        self.max_cells_per_file = max_cells_per_file\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.num_candidates = num_candidates\n",
    "        self.samples = []\n",
    "        \n",
    "        print(f\"📚 Création du dataset avec {len(json_files)} fichiers...\")\n",
    "        \n",
    "        for i, json_data in enumerate(json_files):\n",
    "            try:\n",
    "                # Parser les cellules\n",
    "                cells = ExcelParser.parse_excel_json(json_data)\n",
    "                \n",
    "                # Filtrer et limiter\n",
    "                if len(cells) > self.max_cells_per_file:\n",
    "                    # Prendre les cellules les plus importantes\n",
    "                    cells_with_content = [c for c in cells if c.raw_value or c.formula]\n",
    "                    if len(cells_with_content) >= self.max_cells_per_file:\n",
    "                        cells = cells_with_content[:self.max_cells_per_file]\n",
    "                    else:\n",
    "                        # Compléter avec des cellules quelconques\n",
    "                        remaining = self.max_cells_per_file - len(cells_with_content)\n",
    "                        other_cells = [c for c in cells if c not in cells_with_content]\n",
    "                        cells = cells_with_content + other_cells[:remaining]\n",
    "                \n",
    "                if len(cells) >= 3:  # Minimum 3 cellules\n",
    "                    self.samples.append({\n",
    "                        'cells': cells,\n",
    "                        'file_id': i\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Erreur fichier {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"✅ Dataset créé: {len(self.samples)} échantillons valides\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        cells = sample['cells']\n",
    "        \n",
    "        # Choisir les cellules à masquer\n",
    "        num_to_mask = max(1, int(len(cells) * self.mask_ratio))\n",
    "        \n",
    "        # Privilégier les cellules avec contenu\n",
    "        cells_with_content = [i for i, c in enumerate(cells) if c.raw_value or c.formula]\n",
    "        if len(cells_with_content) >= num_to_mask:\n",
    "            mask_indices = random.sample(cells_with_content, num_to_mask)\n",
    "        else:\n",
    "            # Compléter avec des cellules aléatoires\n",
    "            remaining = num_to_mask - len(cells_with_content)\n",
    "            other_indices = [i for i in range(len(cells)) if i not in cells_with_content]\n",
    "            if other_indices:\n",
    "                additional = random.sample(other_indices, min(remaining, len(other_indices)))\n",
    "                mask_indices = cells_with_content + additional\n",
    "            else:\n",
    "                mask_indices = cells_with_content if cells_with_content else [0]\n",
    "        \n",
    "        # Générer les candidats et labels\n",
    "        candidates = []\n",
    "        labels = []\n",
    "        \n",
    "        for mask_idx in mask_indices:\n",
    "            cell = cells[mask_idx]\n",
    "            \n",
    "            # Générer candidats basés sur le type de cellule\n",
    "            cell_candidates = self._generate_candidates(cell)\n",
    "            \n",
    "            # Trouver la vraie valeur\n",
    "            true_value = str(cell.raw_value) if cell.raw_value else str(cell.formula) if cell.formula else \"\"\n",
    "            \n",
    "            # S'assurer que la vraie valeur est dans les candidats\n",
    "            if true_value and true_value not in cell_candidates:\n",
    "                # Remplacer un candidat aléatoire\n",
    "                replace_idx = random.randint(0, len(cell_candidates) - 1)\n",
    "                cell_candidates[replace_idx] = true_value\n",
    "            \n",
    "            # Label = position de la vraie valeur\n",
    "            try:\n",
    "                label = cell_candidates.index(true_value) if true_value else 0\n",
    "            except ValueError:\n",
    "                label = 0\n",
    "            \n",
    "            candidates.append(cell_candidates)\n",
    "            labels.append(label)\n",
    "        \n",
    "        return {\n",
    "            'cells': cells,\n",
    "            'mask_indices': mask_indices,\n",
    "            'candidates': candidates,\n",
    "            'labels': labels,\n",
    "            'file_id': sample['file_id']\n",
    "        }\n",
    "    \n",
    "    def _generate_candidates(self, cell):\n",
    "        \"\"\"Génère des candidats réalistes selon le type de cellule\"\"\"\n",
    "        if cell.cell_type == 1:  # Texte\n",
    "            original = str(cell.raw_value) if cell.raw_value else \"\"\n",
    "            candidates = [\n",
    "                original,\n",
    "                original.upper() if original else \"TEXT\",\n",
    "                original.lower() if original else \"text\", \n",
    "                original + \"_copy\" if original else \"Data\",\n",
    "                \"Данные\", \"Информация\", \"Значение\", \"Элемент\", \"Запись\", \"Пункт\"\n",
    "            ]\n",
    "        elif cell.cell_type == 2:  # Nombre\n",
    "            try:\n",
    "                original = float(cell.raw_value) if cell.raw_value else 0\n",
    "                candidates = [\n",
    "                    str(original),\n",
    "                    str(original + 1),\n",
    "                    str(original * 2),\n",
    "                    str(int(original)),\n",
    "                    \"0\", \"1\", \"100\", \"1000\", \"0.5\", \"2.0\"\n",
    "                ]\n",
    "            except:\n",
    "                candidates = [\"0\", \"1\", \"10\", \"100\", \"1000\", \"0.5\", \"2.0\", \"50\", \"200\", \"999\"]\n",
    "        elif cell.cell_type == 3:  # Formule\n",
    "            candidates = [\n",
    "                cell.formula if cell.formula else \"=SUM(A1:A10)\",\n",
    "                \"=SUM(A1:A10)\",\n",
    "                \"=AVERAGE(B1:B10)\", \n",
    "                \"=COUNT(C1:C10)\",\n",
    "                \"=MAX(D1:D10)\",\n",
    "                \"=MIN(E1:E10)\",\n",
    "                \"=IF(A1>0,\\\"Да\\\",\\\"Нет\\\")\",\n",
    "                \"=CONCATENATE(A1,B1)\",\n",
    "                \"=VLOOKUP(A1,B:C,2,0)\",\n",
    "                \"=TODAY()\"\n",
    "            ]\n",
    "        else:  # Vide\n",
    "            candidates = [\"\", \"0\", \"1\", \"Данные\", \"Значение\", \"N/A\", \"TBD\", \"...\", \"-\", \"Пусто\"]\n",
    "        \n",
    "        # S'assurer qu'on a exactement num_candidates\n",
    "        while len(candidates) < self.num_candidates:\n",
    "            candidates.append(f\"option_{len(candidates)}\")\n",
    "        \n",
    "        return candidates[:self.num_candidates]\n",
    "\n",
    "class ExcelTrainer:\n",
    "    \"\"\"Entraîneur corrigé pour le modèle Excel\"\"\"\n",
    "    \n",
    "    def __init__(self, model, learning_rate=1e-4, weight_decay=1e-5):\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=learning_rate, \n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=50, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        self.training_history = []\n",
    "        self.best_accuracy = 0.0\n",
    "    \n",
    "    def train_epoch(self, dataloader, epoch):\n",
    "        \"\"\"Entraîne le modèle sur une époque - VERSION CORRIGÉE\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_predictions = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            try:\n",
    "                # CORRECTION : Gestion correcte des batches\n",
    "                if isinstance(batch, list):\n",
    "                    batch = batch[0]  # Si c'est une liste, prendre le premier élément\n",
    "                \n",
    "                # Extraire les données\n",
    "                cells = batch['cells']\n",
    "                mask_indices = batch['mask_indices'] \n",
    "                candidates = batch['candidates']\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                # VÉRIFICATION : S'assurer que ce sont des listes\n",
    "                if not isinstance(mask_indices, list):\n",
    "                    print(f\"🔍 Debug - mask_indices type: {type(mask_indices)}, value: {mask_indices}\")\n",
    "                    continue\n",
    "                \n",
    "                if not isinstance(labels, list):\n",
    "                    print(f\"🔍 Debug - labels type: {type(labels)}, value: {labels}\")\n",
    "                    continue\n",
    "                \n",
    "                if len(mask_indices) == 0 or len(labels) == 0:\n",
    "                    print(f\"🔍 Debug - Empty mask_indices or labels\")\n",
    "                    continue\n",
    "                \n",
    "                # Convertir labels en tensor\n",
    "                labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "                \n",
    "                # Forward pass\n",
    "                output = self.model(cells, mask_indices, candidates)\n",
    "                logits = output['logits']\n",
    "                \n",
    "                # Vérifier les dimensions\n",
    "                if logits.size(0) != labels_tensor.size(0):\n",
    "                    print(f\"🔍 Debug - Dimension mismatch: logits {logits.shape}, labels {labels_tensor.shape}\")\n",
    "                    continue\n",
    "                \n",
    "                # Loss\n",
    "                loss = F.cross_entropy(logits, labels_tensor)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Métriques\n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                correct = (predictions == labels_tensor).sum().item()\n",
    "                total_correct += correct\n",
    "                total_predictions += len(labels)\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Log périodique\n",
    "                if batch_idx % 5 == 0:\n",
    "                    acc = correct / len(labels) if len(labels) > 0 else 0\n",
    "                    print(f\"    Batch {batch_idx:3d}: Loss {loss.item():.4f}, Acc {acc:.1%} [{len(labels)} prédictions]\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Erreur batch {batch_idx}: {e}\")\n",
    "                # AJOUT : Debug détaillé\n",
    "                try:\n",
    "                    print(f\"   Debug batch type: {type(batch)}\")\n",
    "                    if isinstance(batch, dict):\n",
    "                        for key, value in batch.items():\n",
    "                            print(f\"   - {key}: {type(value)}\")\n",
    "                            if isinstance(value, list) and len(value) > 0:\n",
    "                                print(f\"     First element: {type(value[0])}\")\n",
    "                except:\n",
    "                    pass\n",
    "                continue\n",
    "        \n",
    "        # Scheduler step\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        # Métriques d'époque\n",
    "        epoch_metrics = {\n",
    "            'loss': total_loss / max(batch_count, 1),\n",
    "            'accuracy': total_correct / max(total_predictions, 1),\n",
    "            'total_predictions': total_predictions,\n",
    "            'successful_batches': batch_count,\n",
    "            'epoch_time': time.time() - start_time,\n",
    "            'learning_rate': self.optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "        \n",
    "        self.training_history.append(epoch_metrics)\n",
    "        \n",
    "        # Sauvegarder le meilleur modèle\n",
    "        if epoch_metrics['accuracy'] > self.best_accuracy:\n",
    "            self.best_accuracy = epoch_metrics['accuracy']\n",
    "            torch.save(self.model.state_dict(), f\"best_excel_model_epoch_{epoch}.pt\")\n",
    "            print(f\"    💾 Nouveau record sauvegardé: {self.best_accuracy:.1%}\")\n",
    "        \n",
    "        return epoch_metrics\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Affiche l'historique d'entraînement\"\"\"\n",
    "        if not self.training_history:\n",
    "            return\n",
    "        \n",
    "        epochs = range(len(self.training_history))\n",
    "        losses = [h['loss'] for h in self.training_history]\n",
    "        accuracies = [h['accuracy'] for h in self.training_history]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        ax1.plot(epochs, losses, 'b-', label='Loss')\n",
    "        ax1.set_title('Loss d\\'entraînement')\n",
    "        ax1.set_xlabel('Époque')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        ax2.plot(epochs, accuracies, 'r-', label='Accuracy')\n",
    "        ax2.set_title('Accuracy d\\'entraînement')\n",
    "        ax2.set_xlabel('Époque')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.grid(True)\n",
    "        ax2.set_ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def collate_fn_fixed(batch):\n",
    "    \"\"\"Fonction de collation corrigée\"\"\"\n",
    "    # Retourner directement l'élément (batch size = 1)\n",
    "    return batch[0] if len(batch) == 1 else batch\n",
    "\n",
    "def debug_dataset(dataset, num_samples=3):\n",
    "    \"\"\"Debug le dataset pour comprendre le format\"\"\"\n",
    "    print(\"🔍 DEBUG DATASET\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        try:\n",
    "            sample = dataset[i]\n",
    "            print(f\"\\nÉchantillon {i}:\")\n",
    "            print(f\"  Type: {type(sample)}\")\n",
    "            \n",
    "            if isinstance(sample, dict):\n",
    "                for key, value in sample.items():\n",
    "                    print(f\"  - {key}: {type(value)}\")\n",
    "                    if key == 'mask_indices':\n",
    "                        print(f\"    Value: {value}\")\n",
    "                    elif key == 'labels':\n",
    "                        print(f\"    Value: {value}\")\n",
    "                    elif key == 'cells':\n",
    "                        print(f\"    Length: {len(value) if hasattr(value, '__len__') else 'N/A'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur échantillon {i}: {e}\")\n",
    "\n",
    "def run_fixed_training():\n",
    "    \"\"\"Version corrigée de l'entraînement complet\"\"\"\n",
    "    \n",
    "    print(\"🚀 ENTRAÎNEMENT CORRIGÉ - Version Debug\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Charger vos données\n",
    "    print(\"\\n1️⃣ CHARGEMENT DES DONNÉES\")\n",
    "    \n",
    "    data_folders = [\"embedding/data\", \"data\", \"./data\"]\n",
    "    json_files = []\n",
    "    \n",
    "    for folder in data_folders:\n",
    "        if os.path.exists(folder):\n",
    "            pattern = os.path.join(folder, \"*.json\")\n",
    "            found_files = glob.glob(pattern)\n",
    "            if found_files:\n",
    "                print(f\"📁 Dossier trouvé: {folder} ({len(found_files)} fichiers)\")\n",
    "                \n",
    "                for file_path in found_files:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            json_data = json.load(f)\n",
    "                        json_files.append(json_data)\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Erreur {os.path.basename(file_path)}: {e}\")\n",
    "                break\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"❌ Aucun fichier JSON trouvé!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"✅ {len(json_files)} fichiers chargés\")\n",
    "    \n",
    "    # 2. Créer le modèle (réutiliser la configuration qui marche)\n",
    "    print(\"\\n2️⃣ CRÉATION DU MODÈLE\")\n",
    "    \n",
    "    class TrainingConfig:\n",
    "        def __init__(self):\n",
    "            self.embedding_dim = 256\n",
    "            self.position_embedding_dim = 32\n",
    "            self.type_embedding_dim = 16\n",
    "            self.max_position = 2000\n",
    "            self.max_font_size = 72\n",
    "            self.color_vocab_size = 300\n",
    "            self.value_vocab_size = 50000\n",
    "    \n",
    "    config = TrainingConfig()\n",
    "    \n",
    "    # Réutiliser les classes qui fonctionnent du test précédent\n",
    "    class SimpleCellEmbedder(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            \n",
    "            self.row_embedding = nn.Embedding(config.max_position, config.position_embedding_dim)\n",
    "            self.col_embedding = nn.Embedding(config.max_position, config.position_embedding_dim)\n",
    "            self.type_embedding = nn.Embedding(4, config.type_embedding_dim)\n",
    "            self.value_embedding = nn.Embedding(config.value_vocab_size, 64)\n",
    "            self.style_embedding = nn.Embedding(1000, 32)\n",
    "            \n",
    "            total_dim = (config.position_embedding_dim * 2 + config.type_embedding_dim + 64 + 32)\n",
    "            self.projection = nn.Linear(total_dim, config.embedding_dim)\n",
    "            self.layer_norm = nn.LayerNorm(config.embedding_dim)\n",
    "        \n",
    "        def forward(self, cells):\n",
    "            if not isinstance(cells, list):\n",
    "                cells = [cells]\n",
    "            \n",
    "            embeddings = []\n",
    "            for cell in cells:\n",
    "                row_idx = min(max(cell.row, 0), self.config.max_position - 1)\n",
    "                col_idx = min(max(cell.col, 0), self.config.max_position - 1)\n",
    "                \n",
    "                row_emb = self.row_embedding(torch.tensor(row_idx))\n",
    "                col_emb = self.col_embedding(torch.tensor(col_idx))\n",
    "                type_emb = self.type_embedding(torch.tensor(cell.cell_type))\n",
    "                \n",
    "                content = str(cell.raw_value) + str(cell.formula)\n",
    "                value_hash = abs(hash(content)) % self.config.value_vocab_size\n",
    "                value_emb = self.value_embedding(torch.tensor(value_hash))\n",
    "                \n",
    "                style_hash = abs(hash(cell.style_id)) % 1000 if cell.style_id else 0\n",
    "                style_emb = self.style_embedding(torch.tensor(style_hash))\n",
    "                \n",
    "                cell_embedding = torch.cat([row_emb, col_emb, type_emb, value_emb, style_emb], dim=0)\n",
    "                embeddings.append(cell_embedding)\n",
    "            \n",
    "            batch_embeddings = torch.stack(embeddings)\n",
    "            projected = self.projection(batch_embeddings)\n",
    "            return self.layer_norm(projected).squeeze(0) if len(cells) == 1 else self.layer_norm(projected)\n",
    "    \n",
    "    class ExcelTransformer(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.d_model = config.embedding_dim\n",
    "            self.cell_embedder = SimpleCellEmbedder(config)\n",
    "            \n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=self.d_model,\n",
    "                nhead=8,\n",
    "                dim_feedforward=1024,\n",
    "                dropout=0.1,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "            self.pos_encoding = nn.Parameter(torch.randn(1000, self.d_model) * 0.1)\n",
    "        \n",
    "        def forward(self, cells):\n",
    "            embeddings = self.cell_embedder(cells)\n",
    "            \n",
    "            if len(embeddings.shape) == 1:\n",
    "                embeddings = embeddings.unsqueeze(0)\n",
    "            if len(embeddings.shape) == 2:\n",
    "                embeddings = embeddings.unsqueeze(0)\n",
    "            \n",
    "            batch_size, seq_len, d_model = embeddings.shape\n",
    "            pos_emb = self.pos_encoding[:seq_len].unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            embeddings = embeddings + pos_emb\n",
    "            \n",
    "            return self.transformer(embeddings)\n",
    "    \n",
    "    class ExcelPredictor(nn.Module):\n",
    "        def __init__(self, transformer, num_candidates=10):\n",
    "            super().__init__()\n",
    "            self.transformer = transformer\n",
    "            self.num_candidates = num_candidates\n",
    "            \n",
    "            self.classification_head = nn.Sequential(\n",
    "                nn.Linear(transformer.d_model, transformer.d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(transformer.d_model, transformer.d_model // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(transformer.d_model // 2, num_candidates)\n",
    "            )\n",
    "        \n",
    "        def forward(self, cells, mask_indices, candidates):\n",
    "            transformer_output = self.transformer(cells)\n",
    "            \n",
    "            if len(mask_indices) == 0:\n",
    "                return {'logits': torch.empty(0, self.num_candidates)}\n",
    "            \n",
    "            masked_embeddings = transformer_output[0, mask_indices]\n",
    "            logits = self.classification_head(masked_embeddings)\n",
    "            \n",
    "            return {'logits': logits, 'embeddings': masked_embeddings}\n",
    "    \n",
    "    # Créer le modèle\n",
    "    transformer = ExcelTransformer(config)\n",
    "    model = ExcelPredictor(transformer, num_candidates=10)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"✅ Modèle créé: {total_params:,} paramètres\")\n",
    "    \n",
    "    # 3. Créer et débugger le dataset\n",
    "    print(\"\\n3️⃣ CRÉATION ET DEBUG DU DATASET\")\n",
    "    \n",
    "    # Diviser train/val\n",
    "    train_files = json_files[:max(1, int(0.8 * len(json_files)))]\n",
    "    val_files = json_files[int(0.8 * len(json_files)):] if len(json_files) > 1 else []\n",
    "    \n",
    "    train_dataset = ExcelDataset(train_files, max_cells_per_file=20, mask_ratio=0.15)\n",
    "    \n",
    "    # DEBUG du dataset\n",
    "    debug_dataset(train_dataset, num_samples=2)\n",
    "    \n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"❌ Dataset vide!\")\n",
    "        return None\n",
    "    \n",
    "    # Test d'un échantillon\n",
    "    print(\"\\n🧪 TEST D'UN ÉCHANTILLON\")\n",
    "    try:\n",
    "        sample = train_dataset[0]\n",
    "        print(f\"✅ Échantillon récupéré: {len(sample['cells'])} cellules, {len(sample['mask_indices'])} masquées\")\n",
    "        \n",
    "        # Test du modèle sur cet échantillon\n",
    "        with torch.no_grad():\n",
    "            output = model(sample['cells'], sample['mask_indices'], sample['candidates'])\n",
    "            print(f\"✅ Forward pass réussi: {output['logits'].shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur échantillon: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 4. Créer le DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=1, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn_fixed\n",
    "    )\n",
    "    \n",
    "    # 5. Entraînement\n",
    "    print(\"\\n4️⃣ LANCEMENT DE L'ENTRAÎNEMENT CORRIGÉ\")\n",
    "    \n",
    "    trainer = ExcelTrainer(model, learning_rate=2e-4)\n",
    "    num_epochs = 5  # Commencer avec moins d'époques\n",
    "    \n",
    "    print(f\"🎯 Entraînement sur {num_epochs} époques\")\n",
    "    print(f\"📊 {len(train_loader)} batches par époque\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n📚 Époque {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Entraînement\n",
    "        train_metrics = trainer.train_epoch(train_loader, epoch)\n",
    "        \n",
    "        print(f\"  📈 Train - Loss: {train_metrics['loss']:.4f}, \"\n",
    "              f\"Acc: {train_metrics['accuracy']:.1%}, \"\n",
    "              f\"Batches réussis: {train_metrics['successful_batches']}/{len(train_loader)}, \"\n",
    "              f\"Time: {train_metrics['epoch_time']:.1f}s\")\n",
    "        \n",
    "        # Arrêter si aucun batch ne fonctionne\n",
    "        if train_metrics['successful_batches'] == 0:\n",
    "            print(\"❌ Aucun batch réussi - Arrêt de l'entraînement\")\n",
    "            break\n",
    "    \n",
    "    # 6. Résultats\n",
    "    print(f\"\\n5️⃣ RÉSULTATS\")\n",
    "    if trainer.training_history:\n",
    "        final_acc = trainer.training_history[-1]['accuracy']\n",
    "        print(f\"🏆 Accuracy finale: {final_acc:.1%}\")\n",
    "        print(f\"🏆 Meilleure accuracy: {trainer.best_accuracy:.1%}\")\n",
    "        \n",
    "        # Graphiques si on a des données\n",
    "        trainer.plot_training_history()\n",
    "    else:\n",
    "        print(\"❌ Aucune donnée d'entraînement\")\n",
    "    \n",
    "    return model, trainer\n",
    "\n",
    "# Lancer l'entraînement corrigé\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🔧 Version corrigée de l'entraînement\")\n",
    "    model, trainer = run_fixed_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "de2ce88b-8fc0-4b14-a3cf-b70bea6da26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 ENTRAÎNEMENT RÉUSSI! PROCHAINES ÉTAPES:\n",
      "==================================================\n",
      "🔍 ÉVALUATION DU MODÈLE ENTRAÎNÉ\n",
      "==================================================\n",
      "\n",
      "1️⃣ TEST SUR NOUVEAUX ÉCHANTILLONS\n",
      "📚 Création du dataset avec 2 fichiers...\n",
      "✅ Dataset créé: 2 échantillons valides\n",
      "\n",
      "📋 Test 1:\n",
      "   Cellules masquées: 3\n",
      "   ✅ Cellule (0,13): 'PB' → 'PB'\n",
      "   ✅ Cellule (0,5): 'Oddíl/stát' → 'Oddíl/stát'\n",
      "   ✅ Cellule (0,2): 'Závodník' → 'Závodník'\n",
      "\n",
      "📋 Test 2:\n",
      "   Cellules masquées: 3\n",
      "   ✅ Cellule (2,0): '（ARWU、THE、QS、U.S.News）' → '（ARWU、THE、QS、U.S.News）'\n",
      "   ✅ Cellule (0,0): '附件4' → '附件4'\n",
      "   ✅ Cellule (5,1): '学校名称' → '学校名称'\n",
      "\n",
      "📊 RÉSULTATS GLOBAUX:\n",
      "   Accuracy globale: 100.0% (6/6)\n",
      "\n",
      "📈 ACCURACY PAR TYPE:\n",
      "   Texte   : 100.0% (6/6)\n",
      "\n",
      "🏆 EXCELLENTE PERFORMANCE (100.0%)!\n",
      "   Votre modèle est prêt pour des tests avancés\n",
      "\n",
      "🎮 Pour tester interactivement:\n",
      "   interactive_prediction_demo(model)\n",
      "\n",
      "🚀 SUGGESTIONS D'AMÉLIORATION\n",
      "========================================\n",
      "1️⃣ DONNÉES\n",
      "   • Ajouter plus de fichiers Excel variés\n",
      "   • Inclure des formules plus complexes\n",
      "   • Augmenter la diversité des types de cellules\n",
      "\n",
      "2️⃣ ARCHITECTURE\n",
      "   • Augmenter à 6-8 couches transformer\n",
      "   • Essayer l'attention cross-sheet\n",
      "   • Ajouter un encodage positionnel 2D sophistiqué\n",
      "\n",
      "3️⃣ ENTRAÎNEMENT\n",
      "   • Passer à 20-50 époques\n",
      "   • Implémenter l'early stopping\n",
      "   • Utiliser data augmentation (rotation, masquage variable)\n",
      "\n",
      "4️⃣ ÉVALUATION\n",
      "   • Créer un dataset de test dédié\n",
      "   • Mesurer performance par type de contenu\n",
      "   • Tester sur des classeurs réels\n",
      "\n",
      "5️⃣ DÉPLOIEMENT\n",
      "   • Créer une API REST\n",
      "   • Interface web pour test interactif\n",
      "   • Plugin Excel/Google Sheets\n",
      "\n",
      "💾 Pour sauvegarder:\n",
      "   save_model_for_production(model, config)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExcelPredictor(\n",
       "  (transformer): ExcelTransformer(\n",
       "    (cell_embedder): SimpleCellEmbedder(\n",
       "      (row_embedding): Embedding(2000, 32)\n",
       "      (col_embedding): Embedding(2000, 32)\n",
       "      (type_embedding): Embedding(4, 16)\n",
       "      (value_embedding): Embedding(50000, 64)\n",
       "      (style_embedding): Embedding(1000, 32)\n",
       "      (projection): Linear(in_features=176, out_features=256, bias=True)\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (transformer): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classification_head): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 🎯 PROCHAINES ÉTAPES POUR VOTRE MODÈLE EXCEL\n",
    "\n",
    "def evaluate_trained_model(model, json_files):\n",
    "    \"\"\"Évaluation complète du modèle entraîné\"\"\"\n",
    "    \n",
    "    print(\"🔍 ÉVALUATION DU MODÈLE ENTRAÎNÉ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Test sur de nouveaux échantillons\n",
    "    print(\"\\n1️⃣ TEST SUR NOUVEAUX ÉCHANTILLONS\")\n",
    "    \n",
    "    test_files = json_files[-2:]  # Prendre les 2 derniers fichiers comme test\n",
    "    test_dataset = ExcelDataset(test_files, max_cells_per_file=15, mask_ratio=0.2)\n",
    "    \n",
    "    if len(test_dataset) == 0:\n",
    "        print(\"❌ Pas de données de test\")\n",
    "        return\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "    predictions_by_type = {0: {'correct': 0, 'total': 0}, \n",
    "                          1: {'correct': 0, 'total': 0}, \n",
    "                          2: {'correct': 0, 'total': 0}, \n",
    "                          3: {'correct': 0, 'total': 0}}\n",
    "    \n",
    "    # Tester sur plusieurs échantillons\n",
    "    for i in range(min(len(test_dataset), 5)):\n",
    "        sample = test_dataset[i]\n",
    "        cells = sample['cells']\n",
    "        mask_indices = sample['mask_indices']\n",
    "        candidates = sample['candidates']\n",
    "        true_labels = sample['labels']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(cells, mask_indices, candidates)\n",
    "            logits = output['logits']\n",
    "            predicted_labels = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            \n",
    "            print(f\"\\n📋 Test {i+1}:\")\n",
    "            print(f\"   Cellules masquées: {len(mask_indices)}\")\n",
    "            \n",
    "            for j, (mask_idx, pred_label, true_label) in enumerate(zip(mask_indices, predicted_labels, true_labels)):\n",
    "                cell = cells[mask_idx]\n",
    "                true_value = str(cell.raw_value) if cell.raw_value else str(cell.formula) if cell.formula else \"\"\n",
    "                predicted_value = candidates[j][pred_label] if pred_label < len(candidates[j]) else \"N/A\"\n",
    "                \n",
    "                is_correct = pred_label == true_label\n",
    "                total_correct += is_correct\n",
    "                total_predictions += 1\n",
    "                \n",
    "                # Stats par type\n",
    "                cell_type = cell.cell_type\n",
    "                predictions_by_type[cell_type]['total'] += 1\n",
    "                if is_correct:\n",
    "                    predictions_by_type[cell_type]['correct'] += 1\n",
    "                \n",
    "                status = \"✅\" if is_correct else \"❌\"\n",
    "                print(f\"   {status} Cellule ({cell.row},{cell.col}): '{true_value}' → '{predicted_value}'\")\n",
    "    \n",
    "    # Résultats globaux\n",
    "    overall_accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
    "    print(f\"\\n📊 RÉSULTATS GLOBAUX:\")\n",
    "    print(f\"   Accuracy globale: {overall_accuracy:.1%} ({total_correct}/{total_predictions})\")\n",
    "    \n",
    "    type_names = {0: \"Vide\", 1: \"Texte\", 2: \"Nombre\", 3: \"Formule\"}\n",
    "    print(f\"\\n📈 ACCURACY PAR TYPE:\")\n",
    "    for cell_type, stats in predictions_by_type.items():\n",
    "        if stats['total'] > 0:\n",
    "            acc = stats['correct'] / stats['total']\n",
    "            print(f\"   {type_names[cell_type]:8s}: {acc:.1%} ({stats['correct']}/{stats['total']})\")\n",
    "    \n",
    "    return overall_accuracy\n",
    "\n",
    "def interactive_prediction_demo(model):\n",
    "    \"\"\"Demo interactif de prédiction\"\"\"\n",
    "    \n",
    "    print(\"\\n🎮 DEMO INTERACTIF\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Charger un fichier pour demo\n",
    "    data_folders = [\"embedding/data\", \"data\", \"./data\"]\n",
    "    for folder in data_folders:\n",
    "        if os.path.exists(folder):\n",
    "            json_files = glob.glob(os.path.join(folder, \"*.json\"))\n",
    "            if json_files:\n",
    "                with open(json_files[0], 'r', encoding='utf-8') as f:\n",
    "                    demo_data = json.load(f)\n",
    "                break\n",
    "    \n",
    "    # Parser et prendre quelques cellules\n",
    "    cells = ExcelParser.parse_excel_json(demo_data)\n",
    "    demo_cells = [c for c in cells if c.raw_value or c.formula][:10]\n",
    "    \n",
    "    if not demo_cells:\n",
    "        print(\"❌ Pas de cellules pour la demo\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📋 Cellules disponibles pour test:\")\n",
    "    for i, cell in enumerate(demo_cells):\n",
    "        content = str(cell.raw_value) if cell.raw_value else str(cell.formula) if cell.formula else \"[vide]\"\n",
    "        type_name = [\"Vide\", \"Texte\", \"Nombre\", \"Formule\"][cell.cell_type]\n",
    "        print(f\"   {i:2d}. ({cell.row:2d},{cell.col:2d}) {type_name:8s} | {content[:40]}\")\n",
    "    \n",
    "    # Test interactif\n",
    "    try:\n",
    "        choice = int(input(f\"\\nChoisissez une cellule à prédire (0-{len(demo_cells)-1}): \"))\n",
    "        if 0 <= choice < len(demo_cells):\n",
    "            test_cell = demo_cells[choice]\n",
    "            \n",
    "            # Créer candidats\n",
    "            dataset = ExcelDataset([demo_data], max_cells_per_file=20)\n",
    "            if len(dataset) > 0:\n",
    "                sample = dataset[0]\n",
    "                \n",
    "                # Forcer le masquage de la cellule choisie\n",
    "                mask_indices = [choice] if choice < len(sample['cells']) else [0]\n",
    "                candidates = [dataset._generate_candidates(test_cell)]\n",
    "                \n",
    "                # Prédiction\n",
    "                with torch.no_grad():\n",
    "                    output = model(sample['cells'][:len(demo_cells)], mask_indices, candidates)\n",
    "                    logits = output['logits']\n",
    "                    probabilities = torch.softmax(logits, dim=-1)\n",
    "                    \n",
    "                    print(f\"\\n🔮 PRÉDICTION POUR LA CELLULE:\")\n",
    "                    print(f\"   Position: ({test_cell.row}, {test_cell.col})\")\n",
    "                    print(f\"   Vraie valeur: '{test_cell.raw_value}'\")\n",
    "                    print(f\"   Type: {['Vide', 'Texte', 'Nombre', 'Formule'][test_cell.cell_type]}\")\n",
    "                    print(f\"\\n   Top 5 prédictions:\")\n",
    "                    \n",
    "                    # Trier par probabilité\n",
    "                    sorted_indices = torch.argsort(probabilities[0], descending=True)\n",
    "                    \n",
    "                    for rank, idx in enumerate(sorted_indices[:5]):\n",
    "                        candidate = candidates[0][idx] if idx < len(candidates[0]) else f\"option_{idx}\"\n",
    "                        prob = probabilities[0][idx].item()\n",
    "                        marker = \"🎯\" if candidate == str(test_cell.raw_value) else f\"{rank+1}.\"\n",
    "                        \n",
    "                        # Barre de progression visuelle\n",
    "                        bar_length = int(prob * 20)\n",
    "                        bar = \"█\" * bar_length + \"░\" * (20 - bar_length)\n",
    "                        \n",
    "                        print(f\"     {marker:3s} {candidate[:25]:25s} │{bar}│ {prob:.1%}\")\n",
    "        \n",
    "    except (ValueError, KeyboardInterrupt):\n",
    "        print(\"Demo terminée\")\n",
    "\n",
    "def advanced_training_suggestions():\n",
    "    \"\"\"Suggestions pour améliorer encore le modèle\"\"\"\n",
    "    \n",
    "    print(\"\\n🚀 SUGGESTIONS D'AMÉLIORATION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    suggestions = [\n",
    "        \"1️⃣ DONNÉES\",\n",
    "        \"   • Ajouter plus de fichiers Excel variés\",\n",
    "        \"   • Inclure des formules plus complexes\", \n",
    "        \"   • Augmenter la diversité des types de cellules\",\n",
    "        \"\",\n",
    "        \"2️⃣ ARCHITECTURE\",\n",
    "        \"   • Augmenter à 6-8 couches transformer\",\n",
    "        \"   • Essayer l'attention cross-sheet\",\n",
    "        \"   • Ajouter un encodage positionnel 2D sophistiqué\",\n",
    "        \"\",\n",
    "        \"3️⃣ ENTRAÎNEMENT\",\n",
    "        \"   • Passer à 20-50 époques\",\n",
    "        \"   • Implémenter l'early stopping\",\n",
    "        \"   • Utiliser data augmentation (rotation, masquage variable)\",\n",
    "        \"\",\n",
    "        \"4️⃣ ÉVALUATION\",\n",
    "        \"   • Créer un dataset de test dédié\",\n",
    "        \"   • Mesurer performance par type de contenu\",\n",
    "        \"   • Tester sur des classeurs réels\",\n",
    "        \"\",\n",
    "        \"5️⃣ DÉPLOIEMENT\",\n",
    "        \"   • Créer une API REST\",\n",
    "        \"   • Interface web pour test interactif\", \n",
    "        \"   • Plugin Excel/Google Sheets\"\n",
    "    ]\n",
    "    \n",
    "    for suggestion in suggestions:\n",
    "        print(suggestion)\n",
    "\n",
    "def save_model_for_production(model, config):\n",
    "    \"\"\"Sauvegarde le modèle pour utilisation en production\"\"\"\n",
    "    \n",
    "    print(\"\\n💾 SAUVEGARDE POUR PRODUCTION\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Sauvegarder le modèle complet\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config.__dict__ if hasattr(config, '__dict__') else vars(config),\n",
    "        'model_architecture': {\n",
    "            'embedding_dim': 256,\n",
    "            'num_layers': 4,\n",
    "            'num_heads': 8,\n",
    "            'vocab_size': 50000\n",
    "        }\n",
    "    }, 'excel_transformer_production.pt')\n",
    "    \n",
    "    print(\"✅ Modèle sauvegardé: excel_transformer_production.pt\")\n",
    "    \n",
    "    # Créer un script de chargement\n",
    "    loader_script = '''\n",
    "# Script pour charger le modèle en production\n",
    "import torch\n",
    "\n",
    "def load_excel_model(model_path='excel_transformer_production.pt'):\n",
    "    \"\"\"Charge le modèle Excel entraîné\"\"\"\n",
    "    \n",
    "    # Charger les données\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    config = checkpoint['config']\n",
    "    \n",
    "    # Recréer l'architecture (copier les classes depuis votre notebook)\n",
    "    # ... (inclure les classes ExcelTransformer, ExcelPredictor, etc.)\n",
    "    \n",
    "    # Charger les poids\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, config\n",
    "\n",
    "# Utilisation:\n",
    "# model, config = load_excel_model()\n",
    "'''\n",
    "    \n",
    "    with open('load_model.py', 'w', encoding='utf-8') as f:\n",
    "        f.write(loader_script)\n",
    "    \n",
    "    print(\"✅ Script de chargement: load_model.py\")\n",
    "\n",
    "# 🎯 FONCTION PRINCIPALE POUR CONTINUER\n",
    "def continue_development(model, trainer):\n",
    "    \"\"\"Suite du développement après entraînement réussi\"\"\"\n",
    "    \n",
    "    print(\"🎉 ENTRAÎNEMENT RÉUSSI! PROCHAINES ÉTAPES:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Évaluation\n",
    "    data_folders = [\"embedding/data\", \"data\", \"./data\"]\n",
    "    json_files = []\n",
    "    for folder in data_folders:\n",
    "        if os.path.exists(folder):\n",
    "            json_files = glob.glob(os.path.join(folder, \"*.json\"))\n",
    "            json_files = [json.load(open(f, 'r', encoding='utf-8')) for f in json_files[:5]]\n",
    "            break\n",
    "    \n",
    "    if json_files:\n",
    "        accuracy = evaluate_trained_model(model, json_files)\n",
    "        \n",
    "        if accuracy > 0.7:\n",
    "            print(f\"\\n🏆 EXCELLENTE PERFORMANCE ({accuracy:.1%})!\")\n",
    "            print(\"   Votre modèle est prêt pour des tests avancés\")\n",
    "        elif accuracy > 0.5:\n",
    "            print(f\"\\n👍 BONNE PERFORMANCE ({accuracy:.1%})\")\n",
    "            print(\"   Continuez l'entraînement pour améliorer\")\n",
    "        else:\n",
    "            print(f\"\\n📈 PERFORMANCE MODÉRÉE ({accuracy:.1%})\")\n",
    "            print(\"   Ajustez les hyperparamètres ou ajoutez plus de données\")\n",
    "    \n",
    "    # 2. Demo interactif\n",
    "    print(f\"\\n🎮 Pour tester interactivement:\")\n",
    "    print(f\"   interactive_prediction_demo(model)\")\n",
    "    \n",
    "    # 3. Suggestions\n",
    "    advanced_training_suggestions()\n",
    "    \n",
    "    # 4. Sauvegarde\n",
    "    print(f\"\\n💾 Pour sauvegarder:\")\n",
    "    print(f\"   save_model_for_production(model, config)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Utilisation après votre entraînement:\n",
    "continue_development(model, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b52a2aa-54d9-45c1-8ac6-3affcb7ef376",
   "metadata": {},
   "source": [
    "Proposition d'autres entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1ee91230-0e1f-42ea-b25a-8b005a07b513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Tâches d'entraînement avancées pour Excel Transformer\n",
      "============================================================\n",
      "\n",
      "📚 Tâches disponibles:\n",
      "1. 🧮 Formula Generation & Completion\n",
      "2. 🔗 Cell Relationship Prediction\n",
      "3. 📊 Sheet Structure Prediction\n",
      "4. 🚀 Multi-Task Training Framework\n",
      "\n",
      "Chaque tâche enrichit la compréhension du modèle!\n"
     ]
    }
   ],
   "source": [
    "# Tâches d'Entraînement Avancées pour Excel Transformer\n",
    "# =====================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import re\n",
    "\n",
    "# ========================================\n",
    "# 1. FORMULA GENERATION & COMPLETION\n",
    "# ========================================\n",
    "\n",
    "class FormulaGenerationTask:\n",
    "    \"\"\"Tâche : Générer des formules basées sur le contexte\"\"\"\n",
    "    \n",
    "    def __init__(self, max_formula_tokens=50):\n",
    "        self.max_formula_tokens = max_formula_tokens\n",
    "        \n",
    "        # Vocabulaire Excel étendu\n",
    "        self.excel_functions = [\n",
    "            'SUM', 'AVERAGE', 'COUNT', 'MAX', 'MIN', 'IF', 'VLOOKUP', 'HLOOKUP',\n",
    "            'INDEX', 'MATCH', 'CONCATENATE', 'LEFT', 'RIGHT', 'MID', 'LEN',\n",
    "            'UPPER', 'LOWER', 'TRIM', 'TODAY', 'NOW', 'DATE', 'YEAR', 'MONTH',\n",
    "            'AND', 'OR', 'NOT', 'IFERROR', 'ISBLANK', 'ROUND', 'ABS', 'SQRT'\n",
    "        ]\n",
    "        \n",
    "        self.operators = ['+', '-', '*', '/', '^', '&', '=', '<', '>', '<=', '>=', '<>']\n",
    "        self.delimiters = ['(', ')', ',', ':', ';', '\"']\n",
    "    \n",
    "    def create_training_samples(self, cells: List['FullCellInfo']) -> List[Dict]:\n",
    "        \"\"\"Crée des échantillons d'entraînement pour la génération de formules\"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        for i, cell in enumerate(cells):\n",
    "            if cell.formula:  # Cellule avec formule\n",
    "                # Contexte : cellules environnantes\n",
    "                context_cells = self._get_context_cells(cells, i, radius=3)\n",
    "                \n",
    "                # 1. Completion de formule (masquer une partie)\n",
    "                formula_parts = self._tokenize_formula(cell.formula)\n",
    "                if len(formula_parts) > 3:\n",
    "                    # Masquer 20-40% de la formule\n",
    "                    mask_ratio = random.uniform(0.2, 0.4)\n",
    "                    num_mask = max(1, int(len(formula_parts) * mask_ratio))\n",
    "                    mask_positions = random.sample(range(len(formula_parts)), num_mask)\n",
    "                    \n",
    "                    masked_formula = formula_parts.copy()\n",
    "                    target_tokens = []\n",
    "                    for pos in sorted(mask_positions, reverse=True):\n",
    "                        target_tokens.append(masked_formula.pop(pos))\n",
    "                    target_tokens.reverse()\n",
    "                    \n",
    "                    samples.append({\n",
    "                        'task': 'formula_completion',\n",
    "                        'context_cells': context_cells,\n",
    "                        'partial_formula': masked_formula,\n",
    "                        'target_tokens': target_tokens,\n",
    "                        'mask_positions': mask_positions,\n",
    "                        'original_formula': cell.formula\n",
    "                    })\n",
    "                \n",
    "                # 2. Génération de formule similaire\n",
    "                samples.append({\n",
    "                    'task': 'formula_generation',\n",
    "                    'context_cells': context_cells,\n",
    "                    'target_formula': cell.formula,\n",
    "                    'cell_position': (cell.row, cell.col),\n",
    "                    'pattern_hint': self._extract_formula_pattern(cell.formula)\n",
    "                })\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _get_context_cells(self, cells: List['FullCellInfo'], center_idx: int, radius: int = 3) -> List[Dict]:\n",
    "        \"\"\"Récupère les cellules dans un rayon donné\"\"\"\n",
    "        center_cell = cells[center_idx]\n",
    "        context = []\n",
    "        \n",
    "        for cell in cells:\n",
    "            if cell == center_cell:\n",
    "                continue\n",
    "            \n",
    "            # Distance Manhattan\n",
    "            distance = abs(cell.row - center_cell.row) + abs(cell.col - center_cell.col)\n",
    "            if distance <= radius:\n",
    "                context.append({\n",
    "                    'position': (cell.row, cell.col),\n",
    "                    'relative_pos': (cell.row - center_cell.row, cell.col - center_cell.col),\n",
    "                    'value': cell.raw_value,\n",
    "                    'type': cell.cell_type,\n",
    "                    'distance': distance\n",
    "                })\n",
    "        \n",
    "        return sorted(context, key=lambda x: x['distance'])\n",
    "    \n",
    "    def _tokenize_formula(self, formula: str) -> List[str]:\n",
    "        \"\"\"Tokenise une formule Excel de manière intelligente\"\"\"\n",
    "        # Retirer le = initial\n",
    "        formula = formula.lstrip('=')\n",
    "        \n",
    "        tokens = []\n",
    "        current_token = \"\"\n",
    "        in_quotes = False\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(formula):\n",
    "            char = formula[i]\n",
    "            \n",
    "            if char == '\"':\n",
    "                in_quotes = not in_quotes\n",
    "                current_token += char\n",
    "            elif in_quotes:\n",
    "                current_token += char\n",
    "            elif char in self.delimiters + self.operators:\n",
    "                if current_token:\n",
    "                    tokens.append(current_token)\n",
    "                    current_token = \"\"\n",
    "                tokens.append(char)\n",
    "            elif char.isspace():\n",
    "                if current_token:\n",
    "                    tokens.append(current_token)\n",
    "                    current_token = \"\"\n",
    "            else:\n",
    "                current_token += char\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        if current_token:\n",
    "            tokens.append(current_token)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def _extract_formula_pattern(self, formula: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extrait le pattern d'une formule pour guider la génération\"\"\"\n",
    "        pattern = {\n",
    "            'main_function': None,\n",
    "            'has_range': False,\n",
    "            'has_conditions': False,\n",
    "            'complexity': 'simple'\n",
    "        }\n",
    "        \n",
    "        # Fonction principale\n",
    "        for func in self.excel_functions:\n",
    "            if func in formula.upper():\n",
    "                pattern['main_function'] = func\n",
    "                break\n",
    "        \n",
    "        # Détection de ranges (A1:B10)\n",
    "        if ':' in formula:\n",
    "            pattern['has_range'] = True\n",
    "        \n",
    "        # Détection de conditions\n",
    "        if any(op in formula for op in ['IF(', 'AND(', 'OR(']):\n",
    "            pattern['has_conditions'] = True\n",
    "            pattern['complexity'] = 'conditional'\n",
    "        \n",
    "        # Complexité\n",
    "        if formula.count('(') > 2:\n",
    "            pattern['complexity'] = 'complex'\n",
    "        \n",
    "        return pattern\n",
    "\n",
    "# ========================================\n",
    "# 2. CELL RELATIONSHIP PREDICTION\n",
    "# ========================================\n",
    "\n",
    "class CellRelationshipTask:\n",
    "    \"\"\"Tâche : Prédire les relations entre cellules\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.relationship_types = [\n",
    "            'FORMULA_DEPENDENCY',    # A1 utilise B1 dans sa formule\n",
    "            'LAYOUT_HEADER',         # A1 est l'en-tête de B1\n",
    "            'LAYOUT_TOTAL',          # A1 est le total de B1:B10\n",
    "            'SEMANTIC_SIMILAR',      # A1 et B1 contiennent des données similaires\n",
    "            'TEMPORAL_SEQUENCE',     # A1, B1, C1 forment une séquence temporelle\n",
    "            'CATEGORICAL_GROUP',     # A1 et B1 appartiennent à la même catégorie\n",
    "            'NO_RELATIONSHIP'        # Aucune relation\n",
    "        ]\n",
    "    \n",
    "    def create_training_samples(self, cells: List['FullCellInfo']) -> List[Dict]:\n",
    "        \"\"\"Crée des échantillons pour prédire les relations\"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        # Relations positives (vraies relations)\n",
    "        positive_relations = self._extract_true_relationships(cells)\n",
    "        \n",
    "        # Relations négatives (échantillonnage négatif)\n",
    "        negative_relations = self._sample_negative_relationships(cells, len(positive_relations))\n",
    "        \n",
    "        # Combiner\n",
    "        all_relations = positive_relations + negative_relations\n",
    "        random.shuffle(all_relations)\n",
    "        \n",
    "        for relation in all_relations:\n",
    "            samples.append({\n",
    "                'task': 'relationship_prediction',\n",
    "                'cell_a': relation['cell_a'],\n",
    "                'cell_b': relation['cell_b'],\n",
    "                'relationship_type': relation['type'],\n",
    "                'is_positive': relation['is_positive'],\n",
    "                'context_cells': relation.get('context', [])\n",
    "            })\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _extract_true_relationships(self, cells: List['FullCellInfo']) -> List[Dict]:\n",
    "        \"\"\"Extrait les vraies relations entre cellules\"\"\"\n",
    "        relations = []\n",
    "        \n",
    "        # Index des cellules par position\n",
    "        position_to_cell = {(cell.row, cell.col): cell for cell in cells}\n",
    "        \n",
    "        for i, cell in enumerate(cells):\n",
    "            # 1. Relations de formule\n",
    "            if cell.formula:\n",
    "                referenced_cells = self._extract_formula_references(cell.formula)\n",
    "                for ref_pos in referenced_cells:\n",
    "                    if ref_pos in position_to_cell:\n",
    "                        relations.append({\n",
    "                            'cell_a': cell,\n",
    "                            'cell_b': position_to_cell[ref_pos],\n",
    "                            'type': 'FORMULA_DEPENDENCY',\n",
    "                            'is_positive': True\n",
    "                        })\n",
    "            \n",
    "            # 2. Relations de layout (en-têtes, totaux)\n",
    "            layout_relations = self._detect_layout_relationships(cell, cells, position_to_cell)\n",
    "            relations.extend(layout_relations)\n",
    "            \n",
    "            # 3. Relations sémantiques\n",
    "            semantic_relations = self._detect_semantic_relationships(cell, cells)\n",
    "            relations.extend(semantic_relations)\n",
    "        \n",
    "        return relations\n",
    "    \n",
    "    def _extract_formula_references(self, formula: str) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Extrait les références de cellules d'une formule\"\"\"\n",
    "        references = []\n",
    "        \n",
    "        # Pattern pour A1, B2, etc.\n",
    "        cell_pattern = r'([A-Z]+)(\\d+)'\n",
    "        matches = re.findall(cell_pattern, formula.upper())\n",
    "        \n",
    "        for col_str, row_str in matches:\n",
    "            try:\n",
    "                col = sum((ord(c) - ord('A') + 1) * (26 ** i) \n",
    "                         for i, c in enumerate(reversed(col_str))) - 1\n",
    "                row = int(row_str) - 1\n",
    "                references.append((row, col))\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return references\n",
    "    \n",
    "    def _detect_layout_relationships(self, cell: 'FullCellInfo', \n",
    "                                   all_cells: List['FullCellInfo'],\n",
    "                                   position_map: Dict) -> List[Dict]:\n",
    "        \"\"\"Détecte les relations de layout (en-têtes, totaux)\"\"\"\n",
    "        relations = []\n",
    "        \n",
    "        # En-têtes : cellule texte au-dessus/à gauche de cellules numériques\n",
    "        if cell.cell_type == 1:  # Texte\n",
    "            # Vérifier les cellules en dessous\n",
    "            for row_offset in range(1, 6):\n",
    "                pos = (cell.row + row_offset, cell.col)\n",
    "                if pos in position_map:\n",
    "                    target_cell = position_map[pos]\n",
    "                    if target_cell.cell_type == 2:  # Numérique\n",
    "                        relations.append({\n",
    "                            'cell_a': cell,\n",
    "                            'cell_b': target_cell,\n",
    "                            'type': 'LAYOUT_HEADER',\n",
    "                            'is_positive': True\n",
    "                        })\n",
    "                        break  # Un seul en-tête par colonne\n",
    "        \n",
    "        # Totaux : cellule avec formule SUM et cellules dans la plage\n",
    "        if cell.formula and 'SUM(' in cell.formula.upper():\n",
    "            # Extraire la plage de la fonction SUM\n",
    "            sum_ranges = self._extract_sum_ranges(cell.formula)\n",
    "            for start_pos, end_pos in sum_ranges:\n",
    "                if start_pos in position_map:\n",
    "                    relations.append({\n",
    "                        'cell_a': cell,\n",
    "                        'cell_b': position_map[start_pos],\n",
    "                        'type': 'LAYOUT_TOTAL',\n",
    "                        'is_positive': True\n",
    "                    })\n",
    "        \n",
    "        return relations\n",
    "    \n",
    "    def _detect_semantic_relationships(self, cell: 'FullCellInfo', \n",
    "                                     all_cells: List['FullCellInfo']) -> List[Dict]:\n",
    "        \"\"\"Détecte les relations sémantiques\"\"\"\n",
    "        relations = []\n",
    "        \n",
    "        if not cell.raw_value:\n",
    "            return relations\n",
    "        \n",
    "        cell_value = str(cell.raw_value).lower()\n",
    "        \n",
    "        # Séquences temporelles (dates, mois)\n",
    "        if self._is_temporal(cell_value):\n",
    "            for other_cell in all_cells:\n",
    "                if other_cell == cell or not other_cell.raw_value:\n",
    "                    continue\n",
    "                \n",
    "                other_value = str(other_cell.raw_value).lower()\n",
    "                if (self._is_temporal(other_value) and \n",
    "                    abs(other_cell.row - cell.row) + abs(other_cell.col - cell.col) <= 3):\n",
    "                    relations.append({\n",
    "                        'cell_a': cell,\n",
    "                        'cell_b': other_cell,\n",
    "                        'type': 'TEMPORAL_SEQUENCE',\n",
    "                        'is_positive': True\n",
    "                    })\n",
    "        \n",
    "        # Similarité de contenu\n",
    "        for other_cell in all_cells:\n",
    "            if other_cell == cell or not other_cell.raw_value:\n",
    "                continue\n",
    "            \n",
    "            other_value = str(other_cell.raw_value).lower()\n",
    "            if self._compute_similarity(cell_value, other_value) > 0.7:\n",
    "                relations.append({\n",
    "                    'cell_a': cell,\n",
    "                    'cell_b': other_cell,\n",
    "                    'type': 'SEMANTIC_SIMILAR',\n",
    "                    'is_positive': True\n",
    "                })\n",
    "        \n",
    "        return relations\n",
    "    \n",
    "    def _extract_sum_ranges(self, formula: str) -> List[Tuple[Tuple[int, int], Tuple[int, int]]]:\n",
    "        \"\"\"Extrait les plages de la fonction SUM\"\"\"\n",
    "        ranges = []\n",
    "        \n",
    "        # Pattern pour SUM(A1:A10)\n",
    "        sum_pattern = r'SUM\\(([A-Z]+\\d+):([A-Z]+\\d+)\\)'\n",
    "        matches = re.findall(sum_pattern, formula.upper())\n",
    "        \n",
    "        for start_ref, end_ref in matches:\n",
    "            try:\n",
    "                start_pos = self._parse_cell_reference(start_ref)\n",
    "                end_pos = self._parse_cell_reference(end_ref)\n",
    "                ranges.append((start_pos, end_pos))\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return ranges\n",
    "    \n",
    "    def _parse_cell_reference(self, ref: str) -> Tuple[int, int]:\n",
    "        \"\"\"Parse une référence comme A1 en (row, col)\"\"\"\n",
    "        match = re.match(r'([A-Z]+)(\\d+)', ref)\n",
    "        if match:\n",
    "            col_str, row_str = match.groups()\n",
    "            col = sum((ord(c) - ord('A') + 1) * (26 ** i) \n",
    "                     for i, c in enumerate(reversed(col_str))) - 1\n",
    "            row = int(row_str) - 1\n",
    "            return (row, col)\n",
    "        raise ValueError(f\"Invalid cell reference: {ref}\")\n",
    "    \n",
    "    def _is_temporal(self, value: str) -> bool:\n",
    "        \"\"\"Vérifie si une valeur est temporelle\"\"\"\n",
    "        temporal_patterns = [\n",
    "            r'\\d{1,2}/\\d{1,2}/\\d{4}',  # MM/DD/YYYY\n",
    "            r'\\d{4}-\\d{2}-\\d{2}',      # YYYY-MM-DD\n",
    "            r'(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)',  # Mois\n",
    "            r'(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre)',\n",
    "            r'(monday|tuesday|wednesday|thursday|friday|saturday|sunday)',  # Jours\n",
    "            r'(lundi|mardi|mercredi|jeudi|vendredi|samedi|dimanche)'\n",
    "        ]\n",
    "        \n",
    "        return any(re.search(pattern, value.lower()) for pattern in temporal_patterns)\n",
    "    \n",
    "    def _compute_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calcule la similarité entre deux textes\"\"\"\n",
    "        if not text1 or not text2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Similarité de Jaccard simple\n",
    "        words1 = set(text1.split())\n",
    "        words2 = set(text2.split())\n",
    "        \n",
    "        intersection = len(words1 & words2)\n",
    "        union = len(words1 | words2)\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def _sample_negative_relationships(self, cells: List['FullCellInfo'], \n",
    "                                     num_samples: int) -> List[Dict]:\n",
    "        \"\"\"Échantillonne des relations négatives\"\"\"\n",
    "        negative_relations = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            cell_a, cell_b = random.sample(cells, 2)\n",
    "            \n",
    "            # S'assurer qu'il n'y a pas de vraie relation\n",
    "            if (abs(cell_a.row - cell_b.row) > 5 or \n",
    "                abs(cell_a.col - cell_b.col) > 5):\n",
    "                negative_relations.append({\n",
    "                    'cell_a': cell_a,\n",
    "                    'cell_b': cell_b,\n",
    "                    'type': 'NO_RELATIONSHIP',\n",
    "                    'is_positive': False\n",
    "                })\n",
    "        \n",
    "        return negative_relations\n",
    "\n",
    "# ========================================\n",
    "# 3. SHEET STRUCTURE PREDICTION\n",
    "# ========================================\n",
    "\n",
    "class SheetStructureTask:\n",
    "    \"\"\"Tâche : Prédire la structure et l'organisation des feuilles\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.structure_types = [\n",
    "            'TABLE',           # Tableau de données\n",
    "            'FORM',           # Formulaire\n",
    "            'DASHBOARD',      # Tableau de bord\n",
    "            'CALCULATION',    # Feuille de calcul\n",
    "            'TEMPLATE',       # Modèle\n",
    "            'REPORT'          # Rapport\n",
    "        ]\n",
    "        \n",
    "        self.region_types = [\n",
    "            'HEADER',         # En-tête\n",
    "            'DATA',           # Données\n",
    "            'TOTALS',         # Totaux\n",
    "            'LABELS',         # Étiquettes\n",
    "            'INPUT',          # Zone de saisie\n",
    "            'OUTPUT',         # Zone de résultat\n",
    "            'EMPTY'           # Zone vide\n",
    "        ]\n",
    "    \n",
    "    def create_training_samples(self, cells: List['FullCellInfo']) -> List[Dict]:\n",
    "        \"\"\"Crée des échantillons pour la prédiction de structure\"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        # Grouper par feuille\n",
    "        sheets = {}\n",
    "        for cell in cells:\n",
    "            if cell.sheet_name not in sheets:\n",
    "                sheets[cell.sheet_name] = []\n",
    "            sheets[cell.sheet_name].append(cell)\n",
    "        \n",
    "        for sheet_name, sheet_cells in sheets.items():\n",
    "            # 1. Classification de la structure globale\n",
    "            sheet_structure = self._analyze_sheet_structure(sheet_cells)\n",
    "            samples.append({\n",
    "                'task': 'sheet_classification',\n",
    "                'sheet_name': sheet_name,\n",
    "                'cells': sheet_cells,\n",
    "                'structure_type': sheet_structure['type'],\n",
    "                'features': sheet_structure['features']\n",
    "            })\n",
    "            \n",
    "            # 2. Segmentation en régions\n",
    "            regions = self._segment_sheet_regions(sheet_cells)\n",
    "            for region in regions:\n",
    "                samples.append({\n",
    "                    'task': 'region_classification',\n",
    "                    'sheet_name': sheet_name,\n",
    "                    'region_cells': region['cells'],\n",
    "                    'region_type': region['type'],\n",
    "                    'bounding_box': region['bbox']\n",
    "                })\n",
    "            \n",
    "            # 3. Prédiction de continuation de patterns\n",
    "            pattern_samples = self._create_pattern_continuation_samples(sheet_cells)\n",
    "            samples.extend(pattern_samples)\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _analyze_sheet_structure(self, cells: List['FullCellInfo']) -> Dict[str, Any]:\n",
    "        \"\"\"Analyse la structure globale d'une feuille\"\"\"\n",
    "        features = {\n",
    "            'num_cells': len(cells),\n",
    "            'num_formulas': sum(1 for c in cells if c.formula),\n",
    "            'num_numbers': sum(1 for c in cells if c.cell_type == 2),\n",
    "            'num_text': sum(1 for c in cells if c.cell_type == 1),\n",
    "            'has_headers': False,\n",
    "            'has_totals': False,\n",
    "            'max_row': max(c.row for c in cells) if cells else 0,\n",
    "            'max_col': max(c.col for c in cells) if cells else 0\n",
    "        }\n",
    "        \n",
    "        # Détection d'en-têtes (première ligne avec du texte)\n",
    "        first_row_cells = [c for c in cells if c.row == 0]\n",
    "        if any(c.cell_type == 1 for c in first_row_cells):\n",
    "            features['has_headers'] = True\n",
    "        \n",
    "        # Détection de totaux (formules SUM en bas)\n",
    "        max_row = features['max_row']\n",
    "        last_rows = [c for c in cells if c.row >= max_row - 2]\n",
    "        if any('SUM(' in (c.formula or '') for c in last_rows):\n",
    "            features['has_totals'] = True\n",
    "        \n",
    "        # Classification basée sur les features\n",
    "        structure_type = self._classify_structure(features)\n",
    "        \n",
    "        return {\n",
    "            'type': structure_type,\n",
    "            'features': features\n",
    "        }\n",
    "    \n",
    "    def _classify_structure(self, features: Dict[str, Any]) -> str:\n",
    "        \"\"\"Classifie la structure basée sur les caractéristiques\"\"\"\n",
    "        if (features['has_headers'] and features['has_totals'] and \n",
    "            features['num_numbers'] > features['num_text']):\n",
    "            return 'TABLE'\n",
    "        \n",
    "        elif features['num_formulas'] > features['num_cells'] * 0.3:\n",
    "            return 'CALCULATION'\n",
    "        \n",
    "        elif (features['num_text'] > features['num_numbers'] and \n",
    "              not features['has_totals']):\n",
    "            return 'FORM'\n",
    "        \n",
    "        elif features['num_formulas'] > 0 and features['has_totals']:\n",
    "            return 'REPORT'\n",
    "        \n",
    "        else:\n",
    "            return 'TEMPLATE'\n",
    "    \n",
    "    def _segment_sheet_regions(self, cells: List['FullCellInfo']) -> List[Dict]:\n",
    "        \"\"\"Segmente la feuille en régions logiques\"\"\"\n",
    "        if not cells:\n",
    "            return []\n",
    "        \n",
    "        # Créer une grille pour la segmentation\n",
    "        max_row = max(c.row for c in cells)\n",
    "        max_col = max(c.col for c in cells)\n",
    "        \n",
    "        # Algorithme simple de segmentation par zones\n",
    "        regions = []\n",
    "        \n",
    "        # Région d'en-tête (première ligne)\n",
    "        header_cells = [c for c in cells if c.row == 0 and c.cell_type == 1]\n",
    "        if header_cells:\n",
    "            regions.append({\n",
    "                'type': 'HEADER',\n",
    "                'cells': header_cells,\n",
    "                'bbox': (0, 0, 1, max_col + 1)\n",
    "            })\n",
    "        \n",
    "        # Région de données (milieu)\n",
    "        data_cells = [c for c in cells if 1 <= c.row <= max_row - 2]\n",
    "        if data_cells:\n",
    "            regions.append({\n",
    "                'type': 'DATA',\n",
    "                'cells': data_cells,\n",
    "                'bbox': (1, 0, max_row - 1, max_col + 1)\n",
    "            })\n",
    "        \n",
    "        # Région de totaux (dernières lignes)\n",
    "        total_cells = [c for c in cells if c.row >= max_row - 1 and c.formula]\n",
    "        if total_cells:\n",
    "            regions.append({\n",
    "                'type': 'TOTALS',\n",
    "                'cells': total_cells,\n",
    "                'bbox': (max_row - 1, 0, max_row + 1, max_col + 1)\n",
    "            })\n",
    "        \n",
    "        return regions\n",
    "    \n",
    "    def _create_pattern_continuation_samples(self, cells: List['FullCellInfo']) -> List[Dict]:\n",
    "        \"\"\"Crée des échantillons pour la continuation de patterns\"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        # Détecter des séquences dans les lignes\n",
    "        rows = {}\n",
    "        for cell in cells:\n",
    "            if cell.row not in rows:\n",
    "                rows[cell.row] = []\n",
    "            rows[cell.row].append(cell)\n",
    "        \n",
    "        for row, row_cells in rows.items():\n",
    "            row_cells.sort(key=lambda c: c.col)\n",
    "            \n",
    "            if len(row_cells) >= 3:\n",
    "                # Masquer la dernière cellule et prédire\n",
    "                context_cells = row_cells[:-1]\n",
    "                target_cell = row_cells[-1]\n",
    "                \n",
    "                # Détecter le pattern\n",
    "                pattern = self._detect_pattern(context_cells)\n",
    "                \n",
    "                if pattern['type'] != 'NONE':\n",
    "                    samples.append({\n",
    "                        'task': 'pattern_continuation',\n",
    "                        'context_cells': context_cells,\n",
    "                        'target_cell': target_cell,\n",
    "                        'pattern': pattern,\n",
    "                        'direction': 'horizontal'\n",
    "                    })\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _detect_pattern(self, cells: List['FullCellInfo']) -> Dict[str, Any]:\n",
    "        \"\"\"Détecte un pattern dans une séquence de cellules\"\"\"\n",
    "        if len(cells) < 2:\n",
    "            return {'type': 'NONE'}\n",
    "        \n",
    "        # Pattern numérique (séquence arithmétique)\n",
    "        if all(c.cell_type == 2 and c.raw_value for c in cells):\n",
    "            try:\n",
    "                values = [float(c.raw_value) for c in cells]\n",
    "                diffs = [values[i+1] - values[i] for i in range(len(values)-1)]\n",
    "                \n",
    "                if all(abs(d - diffs[0]) < 0.01 for d in diffs):\n",
    "                    return {\n",
    "                        'type': 'ARITHMETIC',\n",
    "                        'step': diffs[0],\n",
    "                        'start': values[0]\n",
    "                    }\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Pattern de formule\n",
    "        formulas = [c.formula for c in cells if c.formula]\n",
    "        if len(formulas) >= 2:\n",
    "            # Vérifier si c'est le même pattern de formule\n",
    "            pattern_similarity = self._compare_formula_patterns(formulas)\n",
    "            if pattern_similarity > 0.8:\n",
    "                return {\n",
    "                    'type': 'FORMULA_PATTERN',\n",
    "                    'base_formula': formulas[0],\n",
    "                    'similarity': pattern_similarity\n",
    "                }\n",
    "        \n",
    "        # Pattern de texte (dates, mois, etc.)\n",
    "        text_values = [str(c.raw_value) for c in cells if c.cell_type == 1]\n",
    "        if len(text_values) >= 2:\n",
    "            text_pattern = self._detect_text_pattern(text_values)\n",
    "            if text_pattern['type'] != 'NONE':\n",
    "                return text_pattern\n",
    "        \n",
    "        return {'type': 'NONE'}\n",
    "    \n",
    "    def _compare_formula_patterns(self, formulas: List[str]) -> float:\n",
    "        \"\"\"Compare la similarité des patterns de formules\"\"\"\n",
    "        if len(formulas) < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Extraire la structure (fonctions, opérateurs)\n",
    "        patterns = []\n",
    "        for formula in formulas:\n",
    "            pattern = re.sub(r'[A-Z]+\\d+', 'CELL', formula.upper())\n",
    "            pattern = re.sub(r'\\d+', 'NUM', pattern)\n",
    "            patterns.append(pattern)\n",
    "        \n",
    "        # Calculer similarité\n",
    "        base_pattern = patterns[0]\n",
    "        similarities = [1.0 if p == base_pattern else 0.0 for p in patterns[1:]]\n",
    "        \n",
    "        return sum(similarities) / len(similarities)\n",
    "    \n",
    "    def _detect_text_pattern(self, texts: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Détecte des patterns dans le texte\"\"\"\n",
    "        # Mois\n",
    "        months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun',\n",
    "                 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "        \n",
    "        if all(any(month in text.lower() for month in months) for text in texts):\n",
    "            return {\n",
    "                'type': 'MONTHS',\n",
    "                'language': 'en'\n",
    "            }\n",
    "        \n",
    "        # Dates\n",
    "        date_pattern = r'\\d{1,2}/\\d{1,2}/\\d{4}'\n",
    "        if all(re.search(date_pattern, text) for text in texts):\n",
    "            return {\n",
    "                'type': 'DATES',\n",
    "                'format': 'MM/DD/YYYY'\n",
    "            }\n",
    "        \n",
    "        return {'type': 'NONE'}\n",
    "\n",
    "# ========================================\n",
    "# 4. MULTI-TASK TRAINING FRAMEWORK\n",
    "# ========================================\n",
    "\n",
    "class ExcelMultiTaskTrainer:\n",
    "    \"\"\"Framework d'entraînement multi-tâches pour Excel Transformer\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_model: 'ExcelGraphTransformer',\n",
    "                 task_weights: Dict[str, float] = None):\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        # Poids des tâches\n",
    "        self.task_weights = task_weights or {\n",
    "            'masked_prediction': 1.0,\n",
    "            'formula_generation': 0.8,\n",
    "            'relationship_prediction': 0.6,\n",
    "            'structure_prediction': 0.4\n",
    "        }\n",
    "        \n",
    "        # Têtes de tâches spécialisées\n",
    "        self.task_heads = nn.ModuleDict({\n",
    "            'formula_generation': FormulaGenerationHead(base_model.d_model),\n",
    "            'relationship_prediction': RelationshipPredictionHead(base_model.d_model),\n",
    "            'structure_prediction': StructurePredictionHead(base_model.d_model)\n",
    "        })\n",
    "        \n",
    "        # Gestionnaires de tâches\n",
    "        self.formula_task = FormulaGenerationTask()\n",
    "        self.relationship_task = CellRelationshipTask()\n",
    "        self.structure_task = SheetStructureTask()\n",
    "    \n",
    "    def create_multi_task_dataset(self, json_files: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Crée un dataset multi-tâches\"\"\"\n",
    "        all_samples = []\n",
    "        \n",
    "        for json_data in json_files:\n",
    "            try:\n",
    "                # Extraire les cellules\n",
    "                cells = self._extract_cells(json_data)\n",
    "                \n",
    "                # Générer des échantillons pour chaque tâche\n",
    "                formula_samples = self.formula_task.create_training_samples(cells)\n",
    "                relationship_samples = self.relationship_task.create_training_samples(cells)\n",
    "                structure_samples = self.structure_task.create_training_samples(cells)\n",
    "                \n",
    "                # Ajouter l'ID du fichier\n",
    "                for sample in formula_samples + relationship_samples + structure_samples:\n",
    "                    sample['file_id'] = id(json_data)\n",
    "                    sample['cells_context'] = cells\n",
    "                \n",
    "                all_samples.extend(formula_samples)\n",
    "                all_samples.extend(relationship_samples) \n",
    "                all_samples.extend(structure_samples)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erreur traitement fichier: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Mélanger les échantillons\n",
    "        random.shuffle(all_samples)\n",
    "        return all_samples\n",
    "    \n",
    "    def train_multi_task_epoch(self, dataloader, optimizer) -> Dict[str, float]:\n",
    "        \"\"\"Entraîne une époque multi-tâches\"\"\"\n",
    "        task_losses = {}\n",
    "        task_counts = {}\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss = 0.0\n",
    "            batch_task_losses = {}\n",
    "            \n",
    "            # Traiter chaque échantillon du batch\n",
    "            for sample in batch if isinstance(batch, list) else [batch]:\n",
    "                task_type = sample['task']\n",
    "                \n",
    "                try:\n",
    "                    # Forward pass selon la tâche\n",
    "                    if task_type in ['formula_completion', 'formula_generation']:\n",
    "                        loss = self._compute_formula_loss(sample)\n",
    "                    elif task_type == 'relationship_prediction':\n",
    "                        loss = self._compute_relationship_loss(sample)\n",
    "                    elif task_type in ['sheet_classification', 'region_classification', 'pattern_continuation']:\n",
    "                        loss = self._compute_structure_loss(sample)\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    # Pondérer la loss\n",
    "                    task_category = self._get_task_category(task_type)\n",
    "                    weighted_loss = loss * self.task_weights.get(task_category, 1.0)\n",
    "                    \n",
    "                    total_loss += weighted_loss\n",
    "                    \n",
    "                    # Tracking\n",
    "                    if task_category not in batch_task_losses:\n",
    "                        batch_task_losses[task_category] = []\n",
    "                    batch_task_losses[task_category].append(loss.item())\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur tâche {task_type}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Backward pass\n",
    "            if total_loss > 0:\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.base_model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Agréger les losses\n",
    "            for task_cat, losses in batch_task_losses.items():\n",
    "                if task_cat not in task_losses:\n",
    "                    task_losses[task_cat] = []\n",
    "                    task_counts[task_cat] = 0\n",
    "                task_losses[task_cat].extend(losses)\n",
    "                task_counts[task_cat] += len(losses)\n",
    "        \n",
    "        # Calculer les moyennes\n",
    "        avg_losses = {}\n",
    "        for task_cat, losses in task_losses.items():\n",
    "            avg_losses[task_cat] = sum(losses) / len(losses) if losses else 0.0\n",
    "        \n",
    "        avg_losses['total'] = sum(avg_losses.values())\n",
    "        avg_losses['task_counts'] = task_counts\n",
    "        \n",
    "        return avg_losses\n",
    "    \n",
    "    def _get_task_category(self, task_type: str) -> str:\n",
    "        \"\"\"Catégorise le type de tâche\"\"\"\n",
    "        if task_type in ['formula_completion', 'formula_generation']:\n",
    "            return 'formula_generation'\n",
    "        elif task_type == 'relationship_prediction':\n",
    "            return 'relationship_prediction'\n",
    "        elif task_type in ['sheet_classification', 'region_classification', 'pattern_continuation']:\n",
    "            return 'structure_prediction'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    def _extract_cells(self, json_data: Dict) -> List['FullCellInfo']:\n",
    "        \"\"\"Extrait les cellules du JSON (à adapter selon votre parser)\"\"\"\n",
    "        # À remplacer par votre méthode d'extraction\n",
    "        return []\n",
    "\n",
    "# Têtes de tâches spécialisées\n",
    "class FormulaGenerationHead(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int = 1000):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.generation_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model // 2, vocab_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        return self.generation_head(embeddings)\n",
    "\n",
    "class RelationshipPredictionHead(nn.Module):\n",
    "    def __init__(self, d_model: int, num_relations: int = 7):\n",
    "        super().__init__()\n",
    "        self.relation_classifier = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model),  # Concat de 2 cellules\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model, num_relations)\n",
    "        )\n",
    "    \n",
    "    def forward(self, cell_a_emb: torch.Tensor, cell_b_emb: torch.Tensor) -> torch.Tensor:\n",
    "        combined = torch.cat([cell_a_emb, cell_b_emb], dim=-1)\n",
    "        return self.relation_classifier(combined)\n",
    "\n",
    "class StructurePredictionHead(nn.Module):\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.structure_classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model // 2, 6)  # 6 types de structure\n",
    "        )\n",
    "        \n",
    "        self.region_classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model // 2, 7)  # 7 types de région\n",
    "        )\n",
    "    \n",
    "    def forward(self, sheet_embedding: torch.Tensor, task_type: str) -> torch.Tensor:\n",
    "        if task_type == 'sheet_classification':\n",
    "            return self.structure_classifier(sheet_embedding)\n",
    "        else:\n",
    "            return self.region_classifier(sheet_embedding)\n",
    "\n",
    "# Fonction utilitaire pour lancer l'entraînement multi-tâches\n",
    "def run_multi_task_training(json_files: List[Dict], \n",
    "                          base_model: 'ExcelGraphTransformer',\n",
    "                          num_epochs: int = 10):\n",
    "    \"\"\"Lance l'entraînement multi-tâches\"\"\"\n",
    "    \n",
    "    print(\"🚀 ENTRAÎNEMENT MULTI-TÂCHES\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Créer le trainer multi-tâches\n",
    "    trainer = ExcelMultiTaskTrainer(base_model)\n",
    "    \n",
    "    # Créer le dataset\n",
    "    print(\"📊 Création du dataset multi-tâches...\")\n",
    "    multi_task_samples = trainer.create_multi_task_dataset(json_files)\n",
    "    print(f\"   {len(multi_task_samples)} échantillons générés\")\n",
    "    \n",
    "    # Compter par tâche\n",
    "    task_counts = {}\n",
    "    for sample in multi_task_samples:\n",
    "        task = sample.get('task', 'unknown')\n",
    "        task_counts[task] = task_counts.get(task, 0) + 1\n",
    "    \n",
    "    print(\"\\n📋 Échantillons par tâche:\")\n",
    "    for task, count in task_counts.items():\n",
    "        print(f\"   {task}: {count}\")\n",
    "    \n",
    "    # Dataset et DataLoader (adapter selon vos besoins)\n",
    "    # dataloader = DataLoader(multi_task_samples, batch_size=1, shuffle=True)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(trainer.base_model.parameters()) + list(trainer.task_heads.parameters()),\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎯 Entraînement sur {num_epochs} époques...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nÉpoque {epoch + 1}/{num_epochs}:\")\n",
    "        \n",
    "        # Simuler l'entraînement (à remplacer par votre vraie boucle)\n",
    "        # losses = trainer.train_multi_task_epoch(dataloader, optimizer)\n",
    "        \n",
    "        # Pour la démo, simuler des losses\n",
    "        losses = {\n",
    "            'formula_generation': random.uniform(0.3, 0.8),\n",
    "            'relationship_prediction': random.uniform(0.2, 0.6),\n",
    "            'structure_prediction': random.uniform(0.1, 0.4),\n",
    "            'total': random.uniform(0.8, 1.8)\n",
    "        }\n",
    "        \n",
    "        print(f\"   Losses:\")\n",
    "        for task, loss in losses.items():\n",
    "            if task != 'task_counts':\n",
    "                print(f\"     {task}: {loss:.4f}\")\n",
    "    \n",
    "    print(\"\\n✅ Entraînement multi-tâches terminé!\")\n",
    "    return trainer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎯 Tâches d'entraînement avancées pour Excel Transformer\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n📚 Tâches disponibles:\")\n",
    "    print(\"1. 🧮 Formula Generation & Completion\")\n",
    "    print(\"2. 🔗 Cell Relationship Prediction\") \n",
    "    print(\"3. 📊 Sheet Structure Prediction\")\n",
    "    print(\"4. 🚀 Multi-Task Training Framework\")\n",
    "    print(\"\\nChaque tâche enrichit la compréhension du modèle!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a63b2c-7423-49bd-b928-6ec7e97f4993",
   "metadata": {},
   "source": [
    "Tache de reconstitution d'excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f0679278-78c8-4eb6-b78e-efc265163adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Tâche de Reconstruction Excel\n",
      "========================================\n",
      "\n",
      "📚 Stratégies de décomposition disponibles:\n",
      "1. shuffle_cells\n",
      "2. remove_random\n",
      "3. remove_regions\n",
      "4. scramble_formulas\n",
      "5. break_sheets\n",
      "6. corrupt_structure\n",
      "7. partial_content\n",
      "8. layout_destruction\n",
      "\n",
      "🎯 Cette tâche force le modèle à comprendre:\n",
      "   • La structure logique des feuilles\n",
      "   • Les relations spatiales entre cellules\n",
      "   • Les patterns de contenu et formules\n",
      "   • L'organisation multi-feuilles\n",
      "\n",
      "🚀 Utilisez run_reconstruction_training() pour commencer!\n"
     ]
    }
   ],
   "source": [
    "# Tâche de Reconstruction Excel - Decomposition & Reconstruction\n",
    "# ===========================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Any, Optional, Tuple, Set\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import copy\n",
    "import json\n",
    "\n",
    "class DecompositionStrategy(Enum):\n",
    "    \"\"\"Stratégies de décomposition d'Excel\"\"\"\n",
    "    SHUFFLE_CELLS = \"shuffle_cells\"           # Mélanger les cellules\n",
    "    REMOVE_RANDOM = \"remove_random\"           # Retirer des cellules aléatoirement\n",
    "    REMOVE_REGIONS = \"remove_regions\"         # Retirer des régions entières\n",
    "    SCRAMBLE_FORMULAS = \"scramble_formulas\"   # Mélanger les formules\n",
    "    BREAK_SHEETS = \"break_sheets\"             # Séparer les feuilles\n",
    "    CORRUPT_STRUCTURE = \"corrupt_structure\"   # Corrompre la structure\n",
    "    PARTIAL_CONTENT = \"partial_content\"       # Contenu partiel seulement\n",
    "    LAYOUT_DESTRUCTION = \"layout_destruction\" # Détruire le layout\n",
    "\n",
    "@dataclass\n",
    "class DecomposedExcel:\n",
    "    \"\"\"Représente un Excel décomposé\"\"\"\n",
    "    original_cells: List['FullCellInfo']\n",
    "    decomposed_cells: List['FullCellInfo']\n",
    "    missing_cells: List['FullCellInfo']\n",
    "    corruption_info: Dict[str, Any]\n",
    "    strategy_used: DecompositionStrategy\n",
    "    difficulty_level: float  # 0.0 = facile, 1.0 = très difficile\n",
    "\n",
    "class ExcelReconstructionTask:\n",
    "    \"\"\"Tâche principale de reconstruction d'Excel\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.decomposition_strategies = {\n",
    "            DecompositionStrategy.SHUFFLE_CELLS: self._shuffle_cells_strategy,\n",
    "            DecompositionStrategy.REMOVE_RANDOM: self._remove_random_strategy,\n",
    "            DecompositionStrategy.REMOVE_REGIONS: self._remove_regions_strategy,\n",
    "            DecompositionStrategy.SCRAMBLE_FORMULAS: self._scramble_formulas_strategy,\n",
    "            DecompositionStrategy.BREAK_SHEETS: self._break_sheets_strategy,\n",
    "            DecompositionStrategy.CORRUPT_STRUCTURE: self._corrupt_structure_strategy,\n",
    "            DecompositionStrategy.PARTIAL_CONTENT: self._partial_content_strategy,\n",
    "            DecompositionStrategy.LAYOUT_DESTRUCTION: self._layout_destruction_strategy\n",
    "        }\n",
    "    \n",
    "    def create_reconstruction_samples(self, \n",
    "                                    cells: List['FullCellInfo'],\n",
    "                                    num_samples_per_strategy: int = 3,\n",
    "                                    difficulty_levels: List[float] = None) -> List[Dict]:\n",
    "        \"\"\"Crée des échantillons de reconstruction\"\"\"\n",
    "        \n",
    "        if difficulty_levels is None:\n",
    "            difficulty_levels = [0.2, 0.5, 0.8]  # Facile, Moyen, Difficile\n",
    "        \n",
    "        samples = []\n",
    "        \n",
    "        for strategy in self.decomposition_strategies:\n",
    "            for difficulty in difficulty_levels:\n",
    "                for _ in range(num_samples_per_strategy):\n",
    "                    try:\n",
    "                        # Décomposer l'Excel\n",
    "                        decomposed = self._decompose_excel(cells, strategy, difficulty)\n",
    "                        \n",
    "                        # Créer l'échantillon d'entraînement\n",
    "                        sample = {\n",
    "                            'task': 'excel_reconstruction',\n",
    "                            'strategy': strategy.value,\n",
    "                            'difficulty': difficulty,\n",
    "                            'input_cells': decomposed.decomposed_cells,\n",
    "                            'target_cells': decomposed.missing_cells,\n",
    "                            'original_cells': decomposed.original_cells,\n",
    "                            'corruption_info': decomposed.corruption_info,\n",
    "                            'reconstruction_hints': self._generate_hints(decomposed)\n",
    "                        }\n",
    "                        \n",
    "                        samples.append(sample)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Erreur création échantillon {strategy.value}: {e}\")\n",
    "                        continue\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _decompose_excel(self, \n",
    "                        cells: List['FullCellInfo'], \n",
    "                        strategy: DecompositionStrategy,\n",
    "                        difficulty: float) -> DecomposedExcel:\n",
    "        \"\"\"Décompose un Excel selon la stratégie donnée\"\"\"\n",
    "        \n",
    "        if not cells:\n",
    "            raise ValueError(\"Liste de cellules vide\")\n",
    "        \n",
    "        # Copier les cellules originales\n",
    "        original_cells = copy.deepcopy(cells)\n",
    "        \n",
    "        # Appliquer la stratégie de décomposition\n",
    "        decomposition_func = self.decomposition_strategies[strategy]\n",
    "        result = decomposition_func(original_cells, difficulty)\n",
    "        \n",
    "        return DecomposedExcel(\n",
    "            original_cells=original_cells,\n",
    "            decomposed_cells=result['decomposed_cells'],\n",
    "            missing_cells=result['missing_cells'],\n",
    "            corruption_info=result['corruption_info'],\n",
    "            strategy_used=strategy,\n",
    "            difficulty_level=difficulty\n",
    "        )\n",
    "    \n",
    "    # ========================================\n",
    "    # STRATÉGIES DE DÉCOMPOSITION\n",
    "    # ========================================\n",
    "    \n",
    "    def _shuffle_cells_strategy(self, cells: List['FullCellInfo'], difficulty: float) -> Dict:\n",
    "        \"\"\"Stratégie 1: Mélanger les positions des cellules\"\"\"\n",
    "        \n",
    "        # Plus la difficulté est élevée, plus on mélange\n",
    "        shuffle_ratio = 0.3 + (difficulty * 0.5)  # 30% à 80%\n",
    "        num_to_shuffle = int(len(cells) * shuffle_ratio)\n",
    "        \n",
    "        # Sélectionner les cellules à mélanger\n",
    "        cells_to_shuffle = random.sample(cells, min(num_to_shuffle, len(cells)))\n",
    "        \n",
    "        # Créer la version décomposée\n",
    "        decomposed_cells = copy.deepcopy(cells)\n",
    "        \n",
    "        # Sauvegarder les positions originales\n",
    "        original_positions = {}\n",
    "        for cell in cells_to_shuffle:\n",
    "            original_positions[id(cell)] = (cell.row, cell.col)\n",
    "        \n",
    "        # Mélanger les positions\n",
    "        positions = [(cell.row, cell.col) for cell in cells_to_shuffle]\n",
    "        random.shuffle(positions)\n",
    "        \n",
    "        # Appliquer les nouvelles positions\n",
    "        for i, cell in enumerate(cells_to_shuffle):\n",
    "            for decomp_cell in decomposed_cells:\n",
    "                if (decomp_cell.row == cell.row and \n",
    "                    decomp_cell.col == cell.col and \n",
    "                    decomp_cell.sheet_name == cell.sheet_name):\n",
    "                    decomp_cell.row, decomp_cell.col = positions[i]\n",
    "                    break\n",
    "        \n",
    "        # Les cellules \"manquantes\" sont celles avec les bonnes positions\n",
    "        missing_cells = []\n",
    "        for cell in cells_to_shuffle:\n",
    "            correct_position_cell = copy.deepcopy(cell)\n",
    "            missing_cells.append(correct_position_cell)\n",
    "        \n",
    "        return {\n",
    "            'decomposed_cells': decomposed_cells,\n",
    "            'missing_cells': missing_cells,\n",
    "            'corruption_info': {\n",
    "                'shuffled_count': len(cells_to_shuffle),\n",
    "                'original_positions': original_positions,\n",
    "                'shuffle_ratio': shuffle_ratio\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _remove_random_strategy(self, cells: List['FullCellInfo'], difficulty: float) -> Dict:\n",
    "        \"\"\"Stratégie 2: Retirer des cellules aléatoirement\"\"\"\n",
    "        \n",
    "        # Plus la difficulté est élevée, plus on retire de cellules\n",
    "        removal_ratio = 0.1 + (difficulty * 0.4)  # 10% à 50%\n",
    "        num_to_remove = int(len(cells) * removal_ratio)\n",
    "        \n",
    "        # Privilégier certains types selon la difficulté\n",
    "        if difficulty < 0.3:\n",
    "            # Facile : retirer principalement des cellules vides\n",
    "            candidates = [c for c in cells if c.cell_type == 0]\n",
    "        elif difficulty < 0.7:\n",
    "            # Moyen : retirer des cellules de contenu simple\n",
    "            candidates = [c for c in cells if c.cell_type in [0, 1, 2]]\n",
    "        else:\n",
    "            # Difficile : retirer n'importe quoi, y compris des formules\n",
    "            candidates = cells\n",
    "        \n",
    "        # Assurer qu'on a assez de candidats\n",
    "        if len(candidates) < num_to_remove:\n",
    "            candidates = cells\n",
    "        \n",
    "        cells_to_remove = random.sample(candidates, min(num_to_remove, len(candidates)))\n",
    "        \n",
    "        # Créer la version décomposée (sans les cellules retirées)\n",
    "        decomposed_cells = [c for c in cells if c not in cells_to_remove]\n",
    "        \n",
    "        return {\n",
    "            'decomposed_cells': decomposed_cells,\n",
    "            'missing_cells': cells_to_remove,\n",
    "            'corruption_info': {\n",
    "                'removed_count': len(cells_to_remove),\n",
    "                'removal_ratio': removal_ratio,\n",
    "                'removed_types': [c.cell_type for c in cells_to_remove]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _remove_regions_strategy(self, cells: List['FullCellInfo'], difficulty: float) -> Dict:\n",
    "        \"\"\"Stratégie 3: Retirer des régions entières\"\"\"\n",
    "        \n",
    "        # Identifier les régions logiques\n",
    "        regions = self._identify_regions(cells)\n",
    "        \n",
    "        # Nombre de régions à retirer selon la difficulté\n",
    "        num_regions_to_remove = max(1, int(len(regions) * difficulty * 0.6))\n",
    "        \n",
    "        # Sélectionner les régions à retirer\n",
    "        regions_to_remove = random.sample(regions, min(num_regions_to_remove, len(regions)))\n",
    "        \n",
    "        # Cellules à retirer\n",
    "        cells_to_remove = []\n",
    "        for region in regions_to_remove:\n",
    "            cells_to_remove.extend(region['cells'])\n",
    "        \n",
    "        # Créer la version décomposée\n",
    "        decomposed_cells = [c for c in cells if c not in cells_to_remove]\n",
    "        \n",
    "        return {\n",
    "            'decomposed_cells': decomposed_cells,\n",
    "            'missing_cells': cells_to_remove,\n",
    "            'corruption_info': {\n",
    "                'regions_removed': len(regions_to_remove),\n",
    "                'total_regions': len(regions),\n",
    "                'region_types': [r['type'] for r in regions_to_remove]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _scramble_formulas_strategy(self, cells: List['FullCellInfo'], difficulty: float) -> Dict:\n",
    "        \"\"\"Stratégie 4: Mélanger/corrompre les formules\"\"\"\n",
    "        \n",
    "        # Trouver toutes les cellules avec formules\n",
    "        formula_cells = [c for c in cells if c.formula]\n",
    "        \n",
    "        if not formula_cells:\n",
    "            # Fallback vers remove_random si pas de formules\n",
    "            return self._remove_random_strategy(cells, difficulty)\n",
    "        \n",
    "        # Nombre de formules à corrompre\n",
    "        corruption_ratio = 0.2 + (difficulty * 0.6)  # 20% à 80%\n",
    "        num_to_corrupt = int(len(formula_cells) * corruption_ratio)\n",
    "        \n",
    "        cells_to_corrupt = random.sample(formula_cells, min(num_to_corrupt, len(formula_cells)))\n",
    "        \n",
    "        # Créer la version décomposée\n",
    "        decomposed_cells = copy.deepcopy(cells)\n",
    "        corrupted_formulas = {}\n",
    "        \n",
    "        for cell in cells_to_corrupt:\n",
    "            # Trouver la cellule correspondante dans decomposed_cells\n",
    "            for decomp_cell in decomposed_cells:\n",
    "                if (decomp_cell.row == cell.row and \n",
    "                    decomp_cell.col == cell.col and \n",
    "                    decomp_cell.sheet_name == cell.sheet_name):\n",
    "                    \n",
    "                    original_formula = decomp_cell.formula\n",
    "                    \n",
    "                    # Corrompre la formule selon la difficulté\n",
    "                    if difficulty < 0.4:\n",
    "                        # Facile : retirer juste la formule\n",
    "                        decomp_cell.formula = \"\"\n",
    "                        decomp_cell.cell_type = 0\n",
    "                    elif difficulty < 0.7:\n",
    "                        # Moyen : remplacer par une formule simple\n",
    "                        decomp_cell.formula = \"=SUM(A1:A10)\"\n",
    "                    else:\n",
    "                        # Difficile : formule complètement aléatoire\n",
    "                        decomp_cell.formula = self._generate_random_formula()\n",
    "                    \n",
    "                    corrupted_formulas[id(cell)] = {\n",
    "                        'original': original_formula,\n",
    "                        'corrupted': decomp_cell.formula\n",
    "                    }\n",
    "                    break\n",
    "        \n",
    "        return {\n",
    "            'decomposed_cells': decomposed_cells,\n",
    "            'missing_cells': cells_to_corrupt,  # Les cellules avec les bonnes formules\n",
    "            'corruption_info': {\n",
    "                'corrupted_count': len(cells_to_corrupt),\n",
    "                'corruption_ratio': corruption_ratio,\n",
    "                'corrupted_formulas': corrupted_formulas\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _break_sheets_strategy(self, cells: List['FullCellInfo'], difficulty: float) -> Dict:\n",
    "        \"\"\"Stratégie 5: Séparer/mélanger les feuilles\"\"\"\n",
    "        \n",
    "        # Grouper par feuille\n",
    "        sheets = {}\n",
    "        for cell in cells:\n",
    "            if cell.sheet_name not in sheets:\n",
    "                sheets[cell.sheet_name] = []\n",
    "            sheets[cell.sheet_name].append(cell)\n",
    "        \n",
    "        if len(sheets) <= 1:\n",
    "            # Fallback si une seule feuille\n",
    "            return self._remove_regions_strategy(cells, difficulty)\n",
    "        \n",
    "        # Nombre de feuilles à \"casser\"\n",
    "        num_sheets_to_break = max(1, int(len(sheets) * difficulty))\n",
    "        \n",
    "        sheets_to_break = random.sample(list(sheets.keys()), \n",
    "                                       min(num_sheets_to_break, len(sheets)))\n",
    "        \n",
    "        # Cellules à retirer (feuilles entières)\n",
    "        cells_to_remove = []\n",
    "        for sheet_name in sheets_to_break:\n",
    "            cells_to_remove.extend(sheets[sheet_name])\n",
    "        \n",
    "        # Version décomposée sans ces feuilles\n",
    "        decomposed_cells = [c for c in cells if c.sheet_name not in sheets_to_break]\n",
    "        \n",
    "        return {\n",
    "            'decomposed_cells': decomposed_cells,\n",
    "            'missing_cells': cells_to_remove,\n",
    "            'corruption_info': {\n",
    "                'sheets_removed': sheets_to_break,\n",
    "                'total_sheets': len(sheets),\n",
    "                'cells_per_sheet': {name: len(sheet_cells) for name, sheet_cells in sheets.items()}\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _corrupt_structure_strategy(self, cells: List['FullCellInfo'], difficulty: float) -> Dict:\n",
    "        \"\"\"Stratégie 6: Corrompre la structure (styles, formats, etc.)\"\"\"\n",
    "        \n",
    "        # Nombre de cellules à corrompre structurellement\n",
    "        corruption_ratio = 0.3 + (difficulty * 0.5)  # 30% à 80%\n",
    "        num_to_corrupt = int(len(cells) * corruption_ratio)\n",
    "        \n",
    "        cells_to_corrupt = random.sample(cells, min(num_to_corrupt, len(cells)))\n",
    "        \n",
    "        # Créer la version décomposée\n",
    "        decomposed_cells = copy.deepcopy(cells)\n",
    "        \n",
    "        corruptions_applied = []\n",
    "        \n",
    "        for cell in cells_to_corrupt:\n",
    "            # Trouver la cellule correspondante\n",
    "            for decomp_cell in decomposed_cells:\n",
    "                if (decomp_cell.row == cell.row and \n",
    "                    decomp_cell.col == cell.col and \n",
    "                    decomp_cell.sheet_name == cell.sheet_name):\n",
    "                    \n",
    "                    corruption_type = random.choice([\n",
    "                        'remove_style', 'randomize_colors', 'break_formatting',\n",
    "                        'remove_merge', 'change_type'\n",
    "                    ])\n",
    "                    \n",
    "                    if corruption_type == 'remove_style':\n",
    "                        decomp_cell.style_id = \"\"\n",
    "                        decomp_cell.bold = False\n",
    "                        decomp_cell.italic = False\n",
    "                        decomp_cell.background_color = \"#FFFFFF\"\n",
    "                    \n",
    "                    elif corruption_type == 'randomize_colors':\n",
    "                        decomp_cell.text_color = f\"#{random.randint(0, 0xFFFFFF):06x}\"\n",
    "                        decomp_cell.background_color = f\"#{random.randint(0, 0xFFFFFF):06x}\"\n",
    "                    \n",
    "                    elif corruption_type == 'break_formatting':\n",
    "                        decomp_cell.horizontal_align = random.randint(0, 4)\n",
    "                        decomp_cell.font_size = random.uniform(8, 24)\n",
    "                    \n",
    "                    elif corruption_type == 'remove_merge':\n",
    "                        decomp_cell.is_merged = False\n",
    "                        decomp_cell.merge_range = (0, 0, 0, 0)\n",
    "                    \n",
    "                    elif corruption_type == 'change_type':\n",
    "                        if decomp_cell.cell_type != 0:\n",
    "                            decomp_cell.cell_type = random.choice([0, 1, 2, 3])\n",
    "                    \n",
    "                    corruptions_applied.append({\n",
    "                        'position': (cell.row, cell.col),\n",
    "                        'type': corruption_type\n",
    "                    })\n",
    "                    break\n",
    "        \n",
    "        return {\n",
    "            'decomposed_cells': decomposed_cells,\n",
    "            'missing_cells': cells_to_corrupt,  # Cellules avec la bonne structure\n",
    "            'corruption_info': {\n",
    "                'corrupted_count': len(cells_to_corrupt),\n",
    "                'corruptions_applied': corruptions_applied\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _partial_content_strategy(self, cells: List['FullCellInfo'], difficulty: float) -> Dict:\n",
    "        \"\"\"Stratégie 7: Garder seulement le contenu partiel\"\"\"\n",
    "        \n",
    "        # Créer la version décomposée\n",
    "        decomposed_cells = copy.deepcopy(cells)\n",
    "        cells_with_partial_content = []\n",
    "        \n",
    "        for decomp_cell in decomposed_cells:\n",
    "            if random.random() < difficulty and decomp_cell.raw_value:\n",
    "                # Garder seulement une partie du contenu\n",
    "                content = str(decomp_cell.raw_value)\n",
    "                \n",
    "                if len(content) > 3:\n",
    "                    # Garder 20% à 80% du contenu selon la difficulté\n",
    "                    keep_ratio = 1.0 - difficulty * 0.8\n",
    "                    keep_length = max(1, int(len(content) * keep_ratio))\n",
    "                    \n",
    "                    if random.choice([True, False]):\n",
    "                        # Garder le début\n",
    "                        decomp_cell.raw_value = content[:keep_length] + \"...\"\n",
    "                    else:\n",
    "                        # Garder la fin\n",
    "                        decomp_cell.raw_value = \"...\" + content[-keep_length:]\n",
    "                    \n",
    "                    cells_with_partial_content.append(decomp_cell)\n",
    "        \n",
    "        # Les cellules \"manquantes\" sont celles avec le contenu complet\n",
    "        missing_cells = []\n",
    "        for original_cell in cells:\n",
    "            for partial_cell in cells_with_partial_content:\n",
    "                if (original_cell.row == partial_cell.row and \n",
    "                    original_cell.col == partial_cell.col and\n",
    "                    original_cell.sheet_name == partial_cell.sheet_name):\n",
    "                    missing_cells.append(original_cell)\n",
    "                    break\n",
    "        \n",
    "        return {\n",
    "            'decomposed_cells': decomposed_cells,\n",
    "            'missing_cells': missing_cells,\n",
    "            'corruption_info': {\n",
    "                'partial_content_count': len(cells_with_partial_content),\n",
    "                'difficulty_applied': difficulty\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _layout_destruction_strategy(self, cells: List['FullCellInfo'], difficulty: float) -> Dict:\n",
    "        \"\"\"Stratégie 8: Détruire complètement le layout\"\"\"\n",
    "        \n",
    "        # Cette stratégie est la plus difficile - mélange tout\n",
    "        decomposed_cells = copy.deepcopy(cells)\n",
    "        \n",
    "        # 1. Mélanger les positions\n",
    "        positions = [(cell.row, cell.col) for cell in decomposed_cells]\n",
    "        random.shuffle(positions)\n",
    "        \n",
    "        for i, cell in enumerate(decomposed_cells):\n",
    "            cell.row, cell.col = positions[i]\n",
    "        \n",
    "        # 2. Mélanger les feuilles\n",
    "        sheet_names = list(set(cell.sheet_name for cell in decomposed_cells))\n",
    "        if len(sheet_names) > 1:\n",
    "            for cell in decomposed_cells:\n",
    "                if random.random() < difficulty * 0.5:\n",
    "                    cell.sheet_name = random.choice(sheet_names)\n",
    "        \n",
    "        # 3. Retirer des cellules aléatoirement\n",
    "        removal_count = int(len(decomposed_cells) * difficulty * 0.3)\n",
    "        if removal_count > 0:\n",
    "            cells_to_remove = random.sample(decomposed_cells, removal_count)\n",
    "            decomposed_cells = [c for c in decomposed_cells if c not in cells_to_remove]\n",
    "        else:\n",
    "            cells_to_remove = []\n",
    "        \n",
    "        return {\n",
    "            'decomposed_cells': decomposed_cells,\n",
    "            'missing_cells': cells + cells_to_remove,  # Toutes les cellules dans le bon ordre\n",
    "            'corruption_info': {\n",
    "                'total_destruction': True,\n",
    "                'positions_shuffled': len(positions),\n",
    "                'sheets_mixed': len(sheet_names) > 1,\n",
    "                'cells_removed': len(cells_to_remove)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # ========================================\n",
    "    # MÉTHODES UTILITAIRES\n",
    "    # ========================================\n",
    "    \n",
    "    def _identify_regions(self, cells: List['FullCellInfo']) -> List[Dict]:\n",
    "        \"\"\"Identifie les régions logiques dans les cellules\"\"\"\n",
    "        regions = []\n",
    "        \n",
    "        # Grouper par feuille\n",
    "        sheets = {}\n",
    "        for cell in cells:\n",
    "            if cell.sheet_name not in sheets:\n",
    "                sheets[cell.sheet_name] = []\n",
    "            sheets[cell.sheet_name].append(cell)\n",
    "        \n",
    "        for sheet_name, sheet_cells in sheets.items():\n",
    "            if not sheet_cells:\n",
    "                continue\n",
    "            \n",
    "            # Région d'en-têtes (première ligne avec du texte)\n",
    "            header_cells = [c for c in sheet_cells if c.row == 0 and c.cell_type == 1]\n",
    "            if header_cells:\n",
    "                regions.append({\n",
    "                    'type': 'headers',\n",
    "                    'cells': header_cells,\n",
    "                    'sheet': sheet_name\n",
    "                })\n",
    "            \n",
    "            # Région de données (cellules numériques groupées)\n",
    "            data_cells = [c for c in sheet_cells if c.cell_type == 2]\n",
    "            if len(data_cells) > 3:\n",
    "                regions.append({\n",
    "                    'type': 'data',\n",
    "                    'cells': data_cells,\n",
    "                    'sheet': sheet_name\n",
    "                })\n",
    "            \n",
    "            # Région de formules\n",
    "            formula_cells = [c for c in sheet_cells if c.formula]\n",
    "            if formula_cells:\n",
    "                regions.append({\n",
    "                    'type': 'formulas',\n",
    "                    'cells': formula_cells,\n",
    "                    'sheet': sheet_name\n",
    "                })\n",
    "        \n",
    "        return regions\n",
    "    \n",
    "    def _generate_random_formula(self) -> str:\n",
    "        \"\"\"Génère une formule aléatoire pour la corruption\"\"\"\n",
    "        functions = ['SUM', 'AVERAGE', 'COUNT', 'MAX', 'MIN', 'IF']\n",
    "        ranges = ['A1:A10', 'B1:B5', 'C1:C20', 'D1:D15']\n",
    "        \n",
    "        func = random.choice(functions)\n",
    "        range_ref = random.choice(ranges)\n",
    "        \n",
    "        return f\"={func}({range_ref})\"\n",
    "    \n",
    "    def _generate_hints(self, decomposed: DecomposedExcel) -> Dict[str, Any]:\n",
    "        \"\"\"Génère des indices pour aider la reconstruction\"\"\"\n",
    "        hints = {\n",
    "            'strategy_hint': decomposed.strategy_used.value,\n",
    "            'difficulty_level': decomposed.difficulty_level,\n",
    "            'missing_count': len(decomposed.missing_cells),\n",
    "            'remaining_count': len(decomposed.decomposed_cells)\n",
    "        }\n",
    "        \n",
    "        # Indices spécifiques selon la stratégie\n",
    "        if decomposed.strategy_used == DecompositionStrategy.SHUFFLE_CELLS:\n",
    "            hints['hint_type'] = 'positions_mixed'\n",
    "            hints['spatial_analysis_needed'] = True\n",
    "        \n",
    "        elif decomposed.strategy_used == DecompositionStrategy.REMOVE_RANDOM:\n",
    "            hints['hint_type'] = 'missing_cells'\n",
    "            hints['content_analysis_needed'] = True\n",
    "        \n",
    "        elif decomposed.strategy_used == DecompositionStrategy.REMOVE_REGIONS:\n",
    "            hints['hint_type'] = 'missing_regions'\n",
    "            hints['structural_analysis_needed'] = True\n",
    "        \n",
    "        elif decomposed.strategy_used == DecompositionStrategy.SCRAMBLE_FORMULAS:\n",
    "            hints['hint_type'] = 'corrupted_formulas'\n",
    "            hints['formula_analysis_needed'] = True\n",
    "        \n",
    "        # Indices de complexité\n",
    "        if decomposed.difficulty_level > 0.7:\n",
    "            hints['complexity'] = 'high'\n",
    "            hints['multiple_strategies_possible'] = True\n",
    "        elif decomposed.difficulty_level > 0.4:\n",
    "            hints['complexity'] = 'medium'\n",
    "        else:\n",
    "            hints['complexity'] = 'low'\n",
    "        \n",
    "        return hints\n",
    "\n",
    "# ========================================\n",
    "# MODÈLE DE RECONSTRUCTION\n",
    "# ========================================\n",
    "\n",
    "class ExcelReconstructionModel(nn.Module):\n",
    "    \"\"\"Modèle spécialisé pour la reconstruction d'Excel\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_transformer: 'ExcelGraphTransformer',\n",
    "                 max_missing_cells: int = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_transformer = base_transformer\n",
    "        self.d_model = base_transformer.d_model\n",
    "        self.max_missing_cells = max_missing_cells\n",
    "        \n",
    "        # Encodeur de stratégie de décomposition\n",
    "        self.strategy_encoder = nn.Embedding(len(DecompositionStrategy), 32)\n",
    "        \n",
    "        # Encodeur de difficulté\n",
    "        self.difficulty_encoder = nn.Linear(1, 16)\n",
    "        \n",
    "        # Tête de reconstruction pour prédire les cellules manquantes\n",
    "        self.reconstruction_head = nn.Sequential(\n",
    "            nn.Linear(self.d_model + 32 + 16, self.d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.d_model, self.d_model)\n",
    "        )\n",
    "        \n",
    "        # Prédicteur de position (où placer les cellules)\n",
    "        self.position_predictor = nn.Sequential(\n",
    "            nn.Linear(self.d_model, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)  # (row, col)\n",
    "        )\n",
    "        \n",
    "        # Prédicteur de contenu (que mettre dans les cellules)\n",
    "        self.content_predictor = nn.Sequential(\n",
    "            nn.Linear(self.d_model, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1000)  # Vocabulaire de contenu\n",
    "        )\n",
    "        \n",
    "        # Classificateur de type de cellule\n",
    "        self.type_classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 4)  # 4 types de cellules\n",
    "        )\n",
    "    \n",
    "    def forward(self, \n",
    "                decomposed_graph: 'ExcelGraph',\n",
    "                strategy: DecompositionStrategy,\n",
    "                difficulty: float,\n",
    "                hints: Dict[str, Any] = None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Forward pass pour la reconstruction\"\"\"\n",
    "        \n",
    "        # 1. Encoder le graphe décomposé\n",
    "        transformer_output = self.base_transformer(decomposed_graph)\n",
    "        graph_embeddings = transformer_output['embeddings']  # [1, num_nodes, d_model]\n",
    "        \n",
    "        # 2. Encoder la stratégie et difficulté\n",
    "        strategy_id = list(DecompositionStrategy).index(strategy)\n",
    "        strategy_emb = self.strategy_encoder(torch.tensor(strategy_id))  # [32]\n",
    "        \n",
    "        difficulty_emb = self.difficulty_encoder(torch.tensor([[difficulty]]))  # [1, 16]\n",
    "        \n",
    "        # 3. Contexte global\n",
    "        global_context = torch.cat([\n",
    "            strategy_emb.unsqueeze(0),  # [1, 32]\n",
    "            difficulty_emb.squeeze(0)   # [16]\n",
    "        ], dim=-1)  # [1, 48]\n",
    "        \n",
    "        # 4. Pour chaque cellule existante, prédire les cellules manquantes\n",
    "        batch_size, num_nodes, d_model = graph_embeddings.shape\n",
    "        \n",
    "        # Répéter le contexte pour chaque nœud\n",
    "        global_context_expanded = global_context.unsqueeze(1).expand(batch_size, num_nodes, -1)\n",
    "        \n",
    "        # Combiner avec les embeddings du graphe\n",
    "        combined_embeddings = torch.cat([\n",
    "            graph_embeddings,\n",
    "            global_context_expanded\n",
    "        ], dim=-1)  # [batch_size, num_nodes, d_model + 48]\n",
    "        \n",
    "        # 5. Prédire les reconstructions\n",
    "        reconstruction_features = self.reconstruction_head(combined_embeddings)\n",
    "        \n",
    "        # Position des cellules manquantes\n",
    "        predicted_positions = self.position_predictor(reconstruction_features)\n",
    "        \n",
    "        # Contenu des cellules manquantes  \n",
    "        predicted_content = self.content_predictor(reconstruction_features)\n",
    "        \n",
    "        # Type des cellules manquantes\n",
    "        predicted_types = self.type_classifier(reconstruction_features)\n",
    "        \n",
    "        return {\n",
    "            'reconstruction_features': reconstruction_features,\n",
    "            'predicted_positions': predicted_positions,\n",
    "            'predicted_content': predicted_content,\n",
    "            'predicted_types': predicted_types,\n",
    "            'global_context': global_context\n",
    "        }\n",
    "\n",
    "# ========================================\n",
    "# ENTRAÎNEUR SPÉCIALISÉ\n",
    "# ========================================\n",
    "\n",
    "class ReconstructionTrainer:\n",
    "    \"\"\"Entraîneur pour la tâche de reconstruction\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model: ExcelReconstructionModel,\n",
    "                 learning_rate: float = 1e-4):\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Poids des différentes losses\n",
    "        self.loss_weights = {\n",
    "            'position': 1.0,\n",
    "            'content': 0.8,\n",
    "            'type': 0.6\n",
    "        }\n",
    "    \n",
    "    def compute_reconstruction_loss(self, \n",
    "                                  prediction: Dict[str, torch.Tensor],\n",
    "                                  target_cells: List['FullCellInfo']) -> torch.Tensor:\n",
    "        \"\"\"Calcule la loss de reconstruction\"\"\"\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        \n",
    "        if not target_cells:\n",
    "            return torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        # 1. Loss de position\n",
    "        target_positions = torch.tensor([(c.row, c.col) for c in target_cells], \n",
    "                                       dtype=torch.float32)\n",
    "        \n",
    "        if target_positions.size(0) > 0:\n",
    "            predicted_positions = prediction['predicted_positions'][:, :len(target_cells)]\n",
    "            position_loss = F.mse_loss(predicted_positions.squeeze(0), target_positions)\n",
    "            total_loss += self.loss_weights['position'] * position_loss\n",
    "        \n",
    "        # 2. Loss de type\n",
    "        target_types = torch.tensor([c.cell_type for c in target_cells], \n",
    "                                   dtype=torch.long)\n",
    "        \n",
    "        if target_types.size(0) > 0:\n",
    "            predicted_types = prediction['predicted_types'][:, :len(target_cells)]\n",
    "            type_loss = F.cross_entropy(predicted_types.squeeze(0), target_types)\n",
    "            total_loss += self.loss_weights['type'] * type_loss\n",
    "        \n",
    "        # 3. Loss de contenu (simplifiée)\n",
    "        # Note: Ici on pourrait implémenter un encodeur de contenu plus sophistiqué\n",
    "        if target_cells and len(target_cells) > 0:\n",
    "            content_loss = torch.tensor(0.1)  # Placeholder\n",
    "            total_loss += self.loss_weights['content'] * content_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def train_step(self, sample: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"Un pas d'entraînement\"\"\"\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Extraire les données du sample\n",
    "        input_cells = sample['input_cells']\n",
    "        target_cells = sample['target_cells']\n",
    "        strategy = DecompositionStrategy(sample['strategy'])\n",
    "        difficulty = sample['difficulty']\n",
    "        hints = sample.get('reconstruction_hints', {})\n",
    "        \n",
    "        # Créer le graphe d'entrée (vous devez adapter cette partie)\n",
    "        # input_graph = create_graph_from_cells(input_cells)\n",
    "        \n",
    "        # Pour la démo, créer un graphe fictif\n",
    "        input_graph = self._create_dummy_graph(len(input_cells))\n",
    "        \n",
    "        # Forward pass\n",
    "        prediction = self.model(input_graph, strategy, difficulty, hints)\n",
    "        \n",
    "        # Calculer la loss\n",
    "        loss = self.compute_reconstruction_loss(prediction, target_cells)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'loss': loss.item(),\n",
    "            'strategy': strategy.value,\n",
    "            'difficulty': difficulty,\n",
    "            'num_missing': len(target_cells)\n",
    "        }\n",
    "    \n",
    "    def _create_dummy_graph(self, num_cells: int) -> 'ExcelGraph':\n",
    "        \"\"\"Crée un graphe fictif pour la démo\"\"\"\n",
    "        # À remplacer par votre vraie méthode de création de graphe\n",
    "        from dataclasses import dataclass\n",
    "        \n",
    "        @dataclass\n",
    "        class DummyGraph:\n",
    "            cell_embeddings: torch.Tensor\n",
    "            edge_embeddings: torch.Tensor\n",
    "            edge_indices: torch.Tensor\n",
    "            edge_types: list\n",
    "            edge_weights: torch.Tensor\n",
    "            cell_positions: list\n",
    "            \n",
    "            @property\n",
    "            def num_nodes(self):\n",
    "                return self.cell_embeddings.size(0)\n",
    "            \n",
    "            @property \n",
    "            def num_edges(self):\n",
    "                return self.edge_embeddings.size(0)\n",
    "        \n",
    "        return DummyGraph(\n",
    "            cell_embeddings=torch.randn(num_cells, self.model.d_model),\n",
    "            edge_embeddings=torch.randn(0, 64),\n",
    "            edge_indices=torch.empty(2, 0, dtype=torch.long),\n",
    "            edge_types=[],\n",
    "            edge_weights=torch.empty(0),\n",
    "            cell_positions=[(i, 0) for i in range(num_cells)]\n",
    "        )\n",
    "\n",
    "# ========================================\n",
    "# ÉVALUATEUR DE RECONSTRUCTION\n",
    "# ========================================\n",
    "\n",
    "class ReconstructionEvaluator:\n",
    "    \"\"\"Évaluateur pour mesurer la qualité de reconstruction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'position_accuracy': 0.0,\n",
    "            'content_accuracy': 0.0,\n",
    "            'type_accuracy': 0.0,\n",
    "            'structural_similarity': 0.0,\n",
    "            'complete_reconstruction': 0.0\n",
    "        }\n",
    "    \n",
    "    def evaluate_reconstruction(self, \n",
    "                              predicted_cells: List['FullCellInfo'],\n",
    "                              target_cells: List['FullCellInfo']) -> Dict[str, float]:\n",
    "        \"\"\"Évalue la qualité de la reconstruction\"\"\"\n",
    "        \n",
    "        if not predicted_cells or not target_cells:\n",
    "            return {metric: 0.0 for metric in self.metrics}\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. Précision de position\n",
    "        results['position_accuracy'] = self._compute_position_accuracy(\n",
    "            predicted_cells, target_cells)\n",
    "        \n",
    "        # 2. Précision de contenu\n",
    "        results['content_accuracy'] = self._compute_content_accuracy(\n",
    "            predicted_cells, target_cells)\n",
    "        \n",
    "        # 3. Précision de type\n",
    "        results['type_accuracy'] = self._compute_type_accuracy(\n",
    "            predicted_cells, target_cells)\n",
    "        \n",
    "        # 4. Similarité structurelle\n",
    "        results['structural_similarity'] = self._compute_structural_similarity(\n",
    "            predicted_cells, target_cells)\n",
    "        \n",
    "        # 5. Reconstruction complète\n",
    "        results['complete_reconstruction'] = self._compute_complete_reconstruction(\n",
    "            predicted_cells, target_cells)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _compute_position_accuracy(self, predicted: List, target: List) -> float:\n",
    "        \"\"\"Calcule la précision des positions\"\"\"\n",
    "        if not predicted or not target:\n",
    "            return 0.0\n",
    "        \n",
    "        correct = 0\n",
    "        total = min(len(predicted), len(target))\n",
    "        \n",
    "        for i in range(total):\n",
    "            if (predicted[i].row == target[i].row and \n",
    "                predicted[i].col == target[i].col):\n",
    "                correct += 1\n",
    "        \n",
    "        return correct / total if total > 0 else 0.0\n",
    "    \n",
    "    def _compute_content_accuracy(self, predicted: List, target: List) -> float:\n",
    "        \"\"\"Calcule la précision du contenu\"\"\"\n",
    "        if not predicted or not target:\n",
    "            return 0.0\n",
    "        \n",
    "        correct = 0\n",
    "        total = min(len(predicted), len(target))\n",
    "        \n",
    "        for i in range(total):\n",
    "            pred_content = str(predicted[i].raw_value) if predicted[i].raw_value else \"\"\n",
    "            target_content = str(target[i].raw_value) if target[i].raw_value else \"\"\n",
    "            \n",
    "            if pred_content == target_content:\n",
    "                correct += 1\n",
    "        \n",
    "        return correct / total if total > 0 else 0.0\n",
    "    \n",
    "    def _compute_type_accuracy(self, predicted: List, target: List) -> float:\n",
    "        \"\"\"Calcule la précision des types\"\"\"\n",
    "        if not predicted or not target:\n",
    "            return 0.0\n",
    "        \n",
    "        correct = 0\n",
    "        total = min(len(predicted), len(target))\n",
    "        \n",
    "        for i in range(total):\n",
    "            if predicted[i].cell_type == target[i].cell_type:\n",
    "                correct += 1\n",
    "        \n",
    "        return correct / total if total > 0 else 0.0\n",
    "    \n",
    "    def _compute_structural_similarity(self, predicted: List, target: List) -> float:\n",
    "        \"\"\"Calcule la similarité structurelle\"\"\"\n",
    "        # Métrique composite basée sur la distribution des types, positions, etc.\n",
    "        if not predicted or not target:\n",
    "            return 0.0\n",
    "        \n",
    "        # Distribution des types\n",
    "        pred_types = [c.cell_type for c in predicted]\n",
    "        target_types = [c.cell_type for c in target]\n",
    "        \n",
    "        pred_type_dist = {t: pred_types.count(t) for t in range(4)}\n",
    "        target_type_dist = {t: target_types.count(t) for t in range(4)}\n",
    "        \n",
    "        # Similarité des distributions\n",
    "        similarity = 0.0\n",
    "        for t in range(4):\n",
    "            pred_ratio = pred_type_dist[t] / len(predicted) if predicted else 0\n",
    "            target_ratio = target_type_dist[t] / len(target) if target else 0\n",
    "            similarity += 1.0 - abs(pred_ratio - target_ratio)\n",
    "        \n",
    "        return similarity / 4.0\n",
    "    \n",
    "    def _compute_complete_reconstruction(self, predicted: List, target: List) -> float:\n",
    "        \"\"\"Calcule le score de reconstruction complète\"\"\"\n",
    "        if not predicted or not target:\n",
    "            return 0.0\n",
    "        \n",
    "        # Pourcentage de cellules parfaitement reconstruites\n",
    "        perfect_matches = 0\n",
    "        total = min(len(predicted), len(target))\n",
    "        \n",
    "        for i in range(total):\n",
    "            if (predicted[i].row == target[i].row and\n",
    "                predicted[i].col == target[i].col and\n",
    "                predicted[i].cell_type == target[i].cell_type and\n",
    "                str(predicted[i].raw_value) == str(target[i].raw_value)):\n",
    "                perfect_matches += 1\n",
    "        \n",
    "        return perfect_matches / total if total > 0 else 0.0\n",
    "\n",
    "# ========================================\n",
    "# FONCTION PRINCIPALE D'ENTRAÎNEMENT\n",
    "# ========================================\n",
    "\n",
    "def run_reconstruction_training(json_files: List[Dict],\n",
    "                              base_transformer: 'ExcelGraphTransformer',\n",
    "                              num_epochs: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Lance l'entraînement de reconstruction\"\"\"\n",
    "    \n",
    "    print(\"🔧 ENTRAÎNEMENT DE RECONSTRUCTION EXCEL\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Créer la tâche de reconstruction\n",
    "    reconstruction_task = ExcelReconstructionTask()\n",
    "    \n",
    "    # 2. Générer les échantillons\n",
    "    print(\"📊 Génération des échantillons de reconstruction...\")\n",
    "    all_samples = []\n",
    "    \n",
    "    for i, json_data in enumerate(json_files[:3]):  # Limiter pour la démo\n",
    "        try:\n",
    "            # Extraire les cellules (adapter selon votre parser)\n",
    "            cells = []  # À remplacer par votre extraction\n",
    "            \n",
    "            # Pour la démo, créer des cellules fictives\n",
    "            cells = create_dummy_cells(20)\n",
    "            \n",
    "            # Générer les échantillons de reconstruction\n",
    "            samples = reconstruction_task.create_reconstruction_samples(\n",
    "                cells, \n",
    "                num_samples_per_strategy=2,\n",
    "                difficulty_levels=[0.3, 0.7]\n",
    "            )\n",
    "            \n",
    "            all_samples.extend(samples)\n",
    "            print(f\"  Fichier {i+1}: {len(samples)} échantillons générés\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur fichier {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"📋 Total: {len(all_samples)} échantillons de reconstruction\")\n",
    "    \n",
    "    # 3. Statistiques des échantillons\n",
    "    strategy_counts = {}\n",
    "    for sample in all_samples:\n",
    "        strategy = sample['strategy']\n",
    "        strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1\n",
    "    \n",
    "    print(\"\\n🎯 Échantillons par stratégie:\")\n",
    "    for strategy, count in strategy_counts.items():\n",
    "        print(f\"  {strategy}: {count}\")\n",
    "    \n",
    "    # 4. Créer le modèle de reconstruction\n",
    "    print(\"\\n🧠 Initialisation du modèle de reconstruction...\")\n",
    "    reconstruction_model = ExcelReconstructionModel(base_transformer)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in reconstruction_model.parameters())\n",
    "    print(f\"   Paramètres totaux: {total_params:,}\")\n",
    "    \n",
    "    # 5. Créer l'entraîneur\n",
    "    trainer = ReconstructionTrainer(reconstruction_model)\n",
    "    \n",
    "    # 6. Entraînement\n",
    "    print(f\"\\n🚀 Entraînement sur {num_epochs} époques...\")\n",
    "    \n",
    "    training_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        strategy_performance = {}\n",
    "        \n",
    "        print(f\"\\nÉpoque {epoch + 1}/{num_epochs}:\")\n",
    "        \n",
    "        # Mélanger les échantillons\n",
    "        random.shuffle(all_samples)\n",
    "        \n",
    "        for i, sample in enumerate(all_samples):\n",
    "            try:\n",
    "                # Pas d'entraînement\n",
    "                step_result = trainer.train_step(sample)\n",
    "                epoch_losses.append(step_result['loss'])\n",
    "                \n",
    "                # Tracking par stratégie\n",
    "                strategy = step_result['strategy']\n",
    "                if strategy not in strategy_performance:\n",
    "                    strategy_performance[strategy] = []\n",
    "                strategy_performance[strategy].append(step_result['loss'])\n",
    "                \n",
    "                # Log périodique\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    avg_loss = sum(epoch_losses[-10:]) / 10\n",
    "                    print(f\"  Batch {i+1:3d}: Loss {avg_loss:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Erreur batch {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Résumé de l'époque\n",
    "        avg_epoch_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0\n",
    "        \n",
    "        print(f\"\\n  📊 Résumé époque {epoch + 1}:\")\n",
    "        print(f\"     Loss moyenne: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        # Performance par stratégie\n",
    "        for strategy, losses in strategy_performance.items():\n",
    "            avg_strategy_loss = sum(losses) / len(losses)\n",
    "            print(f\"     {strategy}: {avg_strategy_loss:.4f}\")\n",
    "        \n",
    "        training_history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'avg_loss': avg_epoch_loss,\n",
    "            'strategy_performance': {s: sum(l)/len(l) for s, l in strategy_performance.items()}\n",
    "        })\n",
    "    \n",
    "    print(f\"\\n✅ Entraînement terminé!\")\n",
    "    \n",
    "    # 7. Évaluation finale\n",
    "    print(\"\\n🔍 Évaluation finale...\")\n",
    "    evaluator = ReconstructionEvaluator()\n",
    "    \n",
    "    # Test sur quelques échantillons\n",
    "    test_samples = all_samples[:5]\n",
    "    evaluation_results = []\n",
    "    \n",
    "    for sample in test_samples:\n",
    "        # Simulation d'évaluation\n",
    "        dummy_predicted = create_dummy_cells(len(sample['target_cells']))\n",
    "        target_cells = sample['target_cells']\n",
    "        \n",
    "        eval_result = evaluator.evaluate_reconstruction(dummy_predicted, target_cells)\n",
    "        evaluation_results.append(eval_result)\n",
    "    \n",
    "    # Moyenne des métriques\n",
    "    avg_metrics = {}\n",
    "    if evaluation_results:\n",
    "        for metric in evaluation_results[0].keys():\n",
    "            avg_metrics[metric] = sum(r[metric] for r in evaluation_results) / len(evaluation_results)\n",
    "        \n",
    "        print(\"📈 Métriques moyennes:\")\n",
    "        for metric, value in avg_metrics.items():\n",
    "            print(f\"   {metric}: {value:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': reconstruction_model,\n",
    "        'trainer': trainer,\n",
    "        'training_history': training_history,\n",
    "        'evaluation_results': avg_metrics,\n",
    "        'total_samples': len(all_samples),\n",
    "        'strategies_tested': list(strategy_counts.keys())\n",
    "    }\n",
    "\n",
    "def create_dummy_cells(num_cells: int) -> List:\n",
    "    \"\"\"Crée des cellules fictives pour la démo\"\"\"\n",
    "    # À remplacer par votre vraie création de cellules\n",
    "    cells = []\n",
    "    for i in range(num_cells):\n",
    "        # Créer un objet cellule fictif\n",
    "        cell = type('DummyCell', (), {\n",
    "            'row': i,\n",
    "            'col': 0,\n",
    "            'sheet_name': 'Sheet1',\n",
    "            'raw_value': f'Value{i}',\n",
    "            'formula': '=SUM(A1:A5)' if i % 5 == 0 else '',\n",
    "            'cell_type': i % 4,\n",
    "            'style_id': '',\n",
    "            'bold': False,\n",
    "            'italic': False,\n",
    "            'background_color': '#FFFFFF',\n",
    "            'is_merged': False,\n",
    "            'merge_range': (0, 0, 0, 0)\n",
    "        })()\n",
    "        cells.append(cell)\n",
    "    return cells\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🔧 Tâche de Reconstruction Excel\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"\\n📚 Stratégies de décomposition disponibles:\")\n",
    "    for i, strategy in enumerate(DecompositionStrategy, 1):\n",
    "        print(f\"{i}. {strategy.value}\")\n",
    "    \n",
    "    print(\"\\n🎯 Cette tâche force le modèle à comprendre:\")\n",
    "    print(\"   • La structure logique des feuilles\")\n",
    "    print(\"   • Les relations spatiales entre cellules\")\n",
    "    print(\"   • Les patterns de contenu et formules\")\n",
    "    print(\"   • L'organisation multi-feuilles\")\n",
    "    \n",
    "    print(\"\\n🚀 Utilisez run_reconstruction_training() pour commencer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d983e247-dc99-487f-aa9a-d7a37eecd745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du tokenizer...\n",
      "Chargement des données depuis le dossier 'data'...\n",
      "Fichiers JSON trouvés dans 'embedding/data':\n",
      "  - 001c766dd64f5fdad3768694b0d382b6d988d94ba248c21fb0e7930b27dacf28.json\n",
      "  - 0017285f44fbc2ca3e36d830e95a0cb049eb474cf1e17ff400f8074b6c2cea1f.json\n",
      "  - 002640c96a51ecb5b9d6ce18816fd01edc27c079c7667156dfa67fc706a703d7.json\n",
      "  - 0024d5eb932a3ac70e343098d6a0379394997851df9d83e0850669e23320075e.json\n",
      "  - 00350d054fe0d85c109eb3607d8aa1dfbef4be0ecbe8e670b7cc8f2f8b236972.json\n",
      "  - 00013ad39833fc129f5b79553f6d6389dad9b192130ebd754f64fc47c01cef82.json\n",
      "  - 0014b1c74f12b7f42dc7c267c6b56e3c90dac181ab147e71441d5f6b78c99ac8.json\n",
      "  - 000b7dd83566a50f96006b5948980639f3382f65b3a34d6bf757ddd3d01bf003.json\n",
      "  - 0022c921e3505c0980a6f3b19c9334cc889d4715bf58783beccf68c5feb7b161.json\n",
      "  - 0003d90ad249104a7ba0fb6bab08e8b9e70746e0cd2c3b30a006935b55f2a07b.json\n",
      "  - 003593d1064dcd41f60b311efc7d7d0fca9c433d5f244481a3e4db279a5083ef.json\n",
      "✓ Chargé: 001c766dd64f5fdad3768694b0d382b6d988d94ba248c21fb0e7930b27dacf28.json\n",
      "✓ Chargé: 0017285f44fbc2ca3e36d830e95a0cb049eb474cf1e17ff400f8074b6c2cea1f.json\n",
      "✓ Chargé: 002640c96a51ecb5b9d6ce18816fd01edc27c079c7667156dfa67fc706a703d7.json\n",
      "✓ Chargé: 0024d5eb932a3ac70e343098d6a0379394997851df9d83e0850669e23320075e.json\n",
      "✓ Chargé: 00350d054fe0d85c109eb3607d8aa1dfbef4be0ecbe8e670b7cc8f2f8b236972.json\n",
      "✓ Chargé: 00013ad39833fc129f5b79553f6d6389dad9b192130ebd754f64fc47c01cef82.json\n",
      "✓ Chargé: 0014b1c74f12b7f42dc7c267c6b56e3c90dac181ab147e71441d5f6b78c99ac8.json\n",
      "✓ Chargé: 000b7dd83566a50f96006b5948980639f3382f65b3a34d6bf757ddd3d01bf003.json\n",
      "✓ Chargé: 0022c921e3505c0980a6f3b19c9334cc889d4715bf58783beccf68c5feb7b161.json\n",
      "✓ Chargé: 0003d90ad249104a7ba0fb6bab08e8b9e70746e0cd2c3b30a006935b55f2a07b.json\n",
      "✓ Chargé: 003593d1064dcd41f60b311efc7d7d0fca9c433d5f244481a3e4db279a5083ef.json\n",
      "Données chargées: 11 classeurs depuis le dossier 'data'\n",
      "Création des DataLoaders...\n",
      "Initialisation du modèle...\n",
      "Début de l'entraînement...\n",
      "\n",
      "Époque 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entraînement: 100%|██████████| 12/12 [00:54<00:00,  4.51s/it, loss=7.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perte moyenne: 8.9090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 5/5 [00:03<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perte validation: 6.6955\n",
      "\n",
      "Époque 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entraînement:  33%|███▎      | 4/12 [00:16<00:33,  4.24s/it, loss=5.86]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ExcelDataset(Dataset):\n",
    "    \"\"\"Dataset pour les données Excel au format JSON\"\"\"\n",
    "    \n",
    "    def __init__(self, excel_jsons: List[Dict], tokenizer, max_length: int = 512):\n",
    "        self.data = excel_jsons\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.tasks = [\n",
    "            'cell_completion',\n",
    "            'formula_prediction', \n",
    "            'column_completion',\n",
    "            'row_completion',\n",
    "            'pattern_recognition',\n",
    "            'data_type_prediction'\n",
    "        ]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) * len(self.tasks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Sélectionner un classeur et une tâche\n",
    "        workbook_idx = idx // len(self.tasks)\n",
    "        task_idx = idx % len(self.tasks)\n",
    "        \n",
    "        workbook = self.data[workbook_idx]\n",
    "        task = self.tasks[task_idx]\n",
    "        \n",
    "        # Générer les données selon la tâche\n",
    "        if task == 'cell_completion':\n",
    "            return self._generate_cell_completion_task(workbook)\n",
    "        elif task == 'formula_prediction':\n",
    "            return self._generate_formula_prediction_task(workbook)\n",
    "        elif task == 'column_completion':\n",
    "            return self._generate_column_completion_task(workbook)\n",
    "        elif task == 'row_completion':\n",
    "            return self._generate_row_completion_task(workbook)\n",
    "        elif task == 'pattern_recognition':\n",
    "            return self._generate_pattern_recognition_task(workbook)\n",
    "        elif task == 'data_type_prediction':\n",
    "            return self._generate_data_type_prediction_task(workbook)\n",
    "    \n",
    "    def _serialize_workbook_context(self, workbook: Dict, mask_positions: List = None) -> str:\n",
    "        \"\"\"Sérialise le contexte du classeur Excel\"\"\"\n",
    "        context = []\n",
    "        \n",
    "        for sheet_name, sheet_data in workbook.items():\n",
    "            if isinstance(sheet_data, dict) and 'cells' in sheet_data:\n",
    "                context.append(f\"[SHEET:{sheet_name}]\")\n",
    "                \n",
    "                # Trier les cellules par position\n",
    "                cells = sheet_data['cells']\n",
    "                sorted_cells = sorted(cells.items(), key=lambda x: (\n",
    "                    int(x[0].split('_')[0]) if '_' in x[0] else 0,\n",
    "                    int(x[0].split('_')[1]) if '_' in x[0] else 0\n",
    "                ))\n",
    "                \n",
    "                for cell_ref, cell_data in sorted_cells:\n",
    "                    if mask_positions and cell_ref in mask_positions:\n",
    "                        context.append(f\"{cell_ref}:[MASK]\")\n",
    "                    else:\n",
    "                        value = cell_data.get('value', '')\n",
    "                        formula = cell_data.get('formula', '')\n",
    "                        data_type = cell_data.get('type', 'text')\n",
    "                        \n",
    "                        cell_repr = f\"{cell_ref}:\"\n",
    "                        if formula:\n",
    "                            cell_repr += f\"={formula}\"\n",
    "                        else:\n",
    "                            cell_repr += f\"{value}({data_type})\"\n",
    "                        context.append(cell_repr)\n",
    "        \n",
    "        return \" \".join(context)\n",
    "    \n",
    "    def _generate_cell_completion_task(self, workbook: Dict) -> Dict:\n",
    "        \"\"\"Tâche: compléter une cellule masquée\"\"\"\n",
    "        # Choisir une feuille au hasard\n",
    "        sheet_names = [k for k, v in workbook.items() if isinstance(v, dict) and 'cells' in v]\n",
    "        if not sheet_names:\n",
    "            return self._generate_empty_task()\n",
    "            \n",
    "        sheet_name = random.choice(sheet_names)\n",
    "        cells = workbook[sheet_name]['cells']\n",
    "        \n",
    "        if not cells:\n",
    "            return self._generate_empty_task()\n",
    "        \n",
    "        # Choisir une cellule au hasard à masquer\n",
    "        cell_ref = random.choice(list(cells.keys()))\n",
    "        target_cell = cells[cell_ref]\n",
    "        \n",
    "        # Créer le contexte avec la cellule masquée\n",
    "        context = self._serialize_workbook_context(workbook, mask_positions=[cell_ref])\n",
    "        \n",
    "        # Target est la valeur de la cellule\n",
    "        target = target_cell.get('value', '')\n",
    "        if target_cell.get('formula'):\n",
    "            target = f\"={target_cell['formula']}\"\n",
    "        \n",
    "        return self._tokenize_task(context, str(target), 'cell_completion')\n",
    "    \n",
    "    def _generate_formula_prediction_task(self, workbook: Dict) -> Dict:\n",
    "        \"\"\"Tâche: prédire une formule\"\"\"\n",
    "        sheet_names = [k for k, v in workbook.items() if isinstance(v, dict) and 'cells' in v]\n",
    "        if not sheet_names:\n",
    "            return self._generate_empty_task()\n",
    "            \n",
    "        sheet_name = random.choice(sheet_names)\n",
    "        cells = workbook[sheet_name]['cells']\n",
    "        \n",
    "        # Chercher des cellules avec des formules\n",
    "        formula_cells = {k: v for k, v in cells.items() if v.get('formula')}\n",
    "        if not formula_cells:\n",
    "            return self._generate_empty_task()\n",
    "        \n",
    "        cell_ref = random.choice(list(formula_cells.keys()))\n",
    "        target_cell = formula_cells[cell_ref]\n",
    "        \n",
    "        context = self._serialize_workbook_context(workbook, mask_positions=[cell_ref])\n",
    "        target = target_cell['formula']\n",
    "        \n",
    "        return self._tokenize_task(context, target, 'formula_prediction')\n",
    "    \n",
    "    def _generate_column_completion_task(self, workbook: Dict) -> Dict:\n",
    "        \"\"\"Tâche: compléter une colonne\"\"\"\n",
    "        sheet_names = [k for k, v in workbook.items() if isinstance(v, dict) and 'cells' in v]\n",
    "        if not sheet_names:\n",
    "            return self._generate_empty_task()\n",
    "            \n",
    "        sheet_name = random.choice(sheet_names)\n",
    "        cells = workbook[sheet_name]['cells']\n",
    "        \n",
    "        # Grouper les cellules par colonne\n",
    "        columns = {}\n",
    "        for cell_ref, cell_data in cells.items():\n",
    "            if '_' in cell_ref:\n",
    "                row, col = cell_ref.split('_')\n",
    "                if col not in columns:\n",
    "                    columns[col] = {}\n",
    "                columns[col][row] = cell_data\n",
    "        \n",
    "        if not columns:\n",
    "            return self._generate_empty_task()\n",
    "        \n",
    "        # Choisir une colonne avec plusieurs cellules\n",
    "        valid_columns = {k: v for k, v in columns.items() if len(v) > 2}\n",
    "        if not valid_columns:\n",
    "            return self._generate_empty_task()\n",
    "        \n",
    "        col = random.choice(list(valid_columns.keys()))\n",
    "        column_cells = valid_columns[col]\n",
    "        \n",
    "        # Masquer une partie de la colonne\n",
    "        rows = list(column_cells.keys())\n",
    "        mask_row = random.choice(rows)\n",
    "        mask_positions = [f\"{mask_row}_{col}\"]\n",
    "        \n",
    "        context = self._serialize_workbook_context(workbook, mask_positions=mask_positions)\n",
    "        target = str(column_cells[mask_row].get('value', ''))\n",
    "        \n",
    "        return self._tokenize_task(context, target, 'column_completion')\n",
    "    \n",
    "    def _generate_row_completion_task(self, workbook: Dict) -> Dict:\n",
    "        \"\"\"Tâche: compléter une ligne\"\"\"\n",
    "        sheet_names = [k for k, v in workbook.items() if isinstance(v, dict) and 'cells' in v]\n",
    "        if not sheet_names:\n",
    "            return self._generate_empty_task()\n",
    "            \n",
    "        sheet_name = random.choice(sheet_names)\n",
    "        cells = workbook[sheet_name]['cells']\n",
    "        \n",
    "        # Grouper les cellules par ligne\n",
    "        rows = {}\n",
    "        for cell_ref, cell_data in cells.items():\n",
    "            if '_' in cell_ref:\n",
    "                row, col = cell_ref.split('_')\n",
    "                if row not in rows:\n",
    "                    rows[row] = {}\n",
    "                rows[row][col] = cell_data\n",
    "        \n",
    "        if not rows:\n",
    "            return self._generate_empty_task()\n",
    "        \n",
    "        # Choisir une ligne avec plusieurs cellules\n",
    "        valid_rows = {k: v for k, v in rows.items() if len(v) > 2}\n",
    "        if not valid_rows:\n",
    "            return self._generate_empty_task()\n",
    "        \n",
    "        row = random.choice(list(valid_rows.keys()))\n",
    "        row_cells = valid_rows[row]\n",
    "        \n",
    "        # Masquer une cellule de la ligne\n",
    "        cols = list(row_cells.keys())\n",
    "        mask_col = random.choice(cols)\n",
    "        mask_positions = [f\"{row}_{mask_col}\"]\n",
    "        \n",
    "        context = self._serialize_workbook_context(workbook, mask_positions=mask_positions)\n",
    "        target = str(row_cells[mask_col].get('value', ''))\n",
    "        \n",
    "        return self._tokenize_task(context, target, 'row_completion')\n",
    "    \n",
    "    def _generate_pattern_recognition_task(self, workbook: Dict) -> Dict:\n",
    "        \"\"\"Tâche: reconnaître et continuer un motif\"\"\"\n",
    "        sheet_names = [k for k, v in workbook.items() if isinstance(v, dict) and 'cells' in v]\n",
    "        if not sheet_names:\n",
    "            return self._generate_empty_task()\n",
    "            \n",
    "        sheet_name = random.choice(sheet_names)\n",
    "        cells = workbook[sheet_name]['cells']\n",
    "        \n",
    "        # Chercher des séquences numériques\n",
    "        numeric_cells = []\n",
    "        for cell_ref, cell_data in cells.items():\n",
    "            value = cell_data.get('value', '')\n",
    "            try:\n",
    "                float(value)\n",
    "                if '_' in cell_ref:\n",
    "                    row, col = cell_ref.split('_')\n",
    "                    numeric_cells.append((int(row), int(col), float(value), cell_ref))\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if len(numeric_cells) < 3:\n",
    "            return self._generate_empty_task()\n",
    "        \n",
    "        # Trier par position et chercher une séquence\n",
    "        numeric_cells.sort()\n",
    "        \n",
    "        # Prendre les 3 premiers et masquer le suivant\n",
    "        sequence = numeric_cells[:3]\n",
    "        if len(numeric_cells) > 3:\n",
    "            target_cell = numeric_cells[3]\n",
    "            mask_positions = [target_cell[3]]\n",
    "            target = str(target_cell[2])\n",
    "        else:\n",
    "            # Créer une position suivante artificielle\n",
    "            last_row, last_col = sequence[-1][0], sequence[-1][1]\n",
    "            next_pos = f\"{last_row+1}_{last_col}\"\n",
    "            mask_positions = [next_pos]\n",
    "            # Prédire le motif (simple progression arithmétique)\n",
    "            values = [cell[2] for cell in sequence]\n",
    "            if len(values) >= 2:\n",
    "                diff = values[1] - values[0]\n",
    "                target = str(values[-1] + diff)\n",
    "            else:\n",
    "                target = str(values[-1])\n",
    "        \n",
    "        context = self._serialize_workbook_context(workbook, mask_positions=mask_positions)\n",
    "        return self._tokenize_task(context, target, 'pattern_recognition')\n",
    "    \n",
    "    def _generate_data_type_prediction_task(self, workbook: Dict) -> Dict:\n",
    "        \"\"\"Tâche: prédire le type de données d'une cellule\"\"\"\n",
    "        sheet_names = [k for k, v in workbook.items() if isinstance(v, dict) and 'cells' in v]\n",
    "        if not sheet_names:\n",
    "            return self._generate_empty_task()\n",
    "            \n",
    "        sheet_name = random.choice(sheet_names)\n",
    "        cells = workbook[sheet_name]['cells']\n",
    "        \n",
    "        if not cells:\n",
    "            return self._generate_empty_task()\n",
    "        \n",
    "        cell_ref = random.choice(list(cells.keys()))\n",
    "        target_cell = cells[cell_ref]\n",
    "        \n",
    "        # Créer un contexte sans le type de données\n",
    "        context_workbook = workbook.copy()\n",
    "        context_workbook[sheet_name]['cells'][cell_ref] = {\n",
    "            'value': target_cell.get('value', ''),\n",
    "            'formula': target_cell.get('formula', '')\n",
    "        }\n",
    "        \n",
    "        context = self._serialize_workbook_context(context_workbook)\n",
    "        target = target_cell.get('type', 'text')\n",
    "        \n",
    "        return self._tokenize_task(context, target, 'data_type_prediction')\n",
    "    \n",
    "    def _generate_empty_task(self) -> Dict:\n",
    "        \"\"\"Génère une tâche vide en cas d'erreur\"\"\"\n",
    "        return self._tokenize_task(\"\", \"\", 'empty')\n",
    "    \n",
    "    def _tokenize_task(self, context: str, target: str, task_type: str) -> Dict:\n",
    "        \"\"\"Tokenise une tâche\"\"\"\n",
    "        # Ajouter le préfixe de tâche\n",
    "        input_text = f\"[TASK:{task_type}] {context}\"\n",
    "        \n",
    "        # Tokeniser l'entrée\n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokeniser la cible\n",
    "        target_encoding = self.tokenizer(\n",
    "            target,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'target_ids': target_encoding['input_ids'].squeeze(),\n",
    "            'target_attention_mask': target_encoding['attention_mask'].squeeze(),\n",
    "            'task_type': task_type\n",
    "        }\n",
    "\n",
    "class ExcelTransformer(nn.Module):\n",
    "    \"\"\"Modèle Transformer pour la compréhension d'Excel\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'distilbert-base-uncased', hidden_size: int = 768):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Têtes de tâches spécialisées\n",
    "        self.task_heads = nn.ModuleDict({\n",
    "            'cell_completion': nn.Linear(hidden_size, self.encoder.config.vocab_size),\n",
    "            'formula_prediction': nn.Linear(hidden_size, self.encoder.config.vocab_size),\n",
    "            'column_completion': nn.Linear(hidden_size, self.encoder.config.vocab_size),\n",
    "            'row_completion': nn.Linear(hidden_size, self.encoder.config.vocab_size),\n",
    "            'pattern_recognition': nn.Linear(hidden_size, self.encoder.config.vocab_size),\n",
    "            'data_type_prediction': nn.Linear(hidden_size, 10)  # Types de données limités\n",
    "        })\n",
    "        \n",
    "        # Couche de projection pour la génération\n",
    "        self.decoder = nn.Linear(hidden_size, self.encoder.config.vocab_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, task_type=None, target_ids=None):\n",
    "        # Encodage du contexte\n",
    "        encoder_outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        hidden_states = encoder_outputs.last_hidden_state\n",
    "        pooled_output = hidden_states[:, 0]  # [CLS] token\n",
    "        \n",
    "        # Appliquer dropout\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Sortie selon la tâche\n",
    "        if task_type and task_type in self.task_heads:\n",
    "            if task_type == 'data_type_prediction':\n",
    "                # Classification des types\n",
    "                logits = self.task_heads[task_type](pooled_output)\n",
    "                return logits\n",
    "            else:\n",
    "                # Génération de texte\n",
    "                logits = self.task_heads[task_type](pooled_output)\n",
    "                return logits\n",
    "        else:\n",
    "            # Génération générale\n",
    "            logits = self.decoder(pooled_output)\n",
    "            return logits\n",
    "\n",
    "class ExcelTrainer:\n",
    "    \"\"\"Classe pour l'entraînement du modèle\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.data_types = ['text', 'number', 'date', 'boolean', 'formula', 'percentage', 'currency', 'time', 'error', 'empty']\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs=3, lr=2e-5):\n",
    "        \"\"\"Entraîne le modèle\"\"\"\n",
    "        optimizer = AdamW(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=0.1 * total_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nÉpoque {epoch + 1}/{epochs}\")\n",
    "            \n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=\"Entraînement\")\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Déplacer les données vers le device\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                target_ids = batch['target_ids'].to(self.device)\n",
    "                task_types = batch['task_type']\n",
    "                \n",
    "                # Forward pass\n",
    "                loss = self._compute_loss(input_ids, attention_mask, target_ids, task_types)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Perte moyenne: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Validation\n",
    "            if val_loader:\n",
    "                val_loss = self.evaluate(val_loader)\n",
    "                print(f\"Perte validation: {val_loss:.4f}\")\n",
    "    \n",
    "    def _compute_loss(self, input_ids, attention_mask, target_ids, task_types):\n",
    "        \"\"\"Calcule la perte selon le type de tâche\"\"\"\n",
    "        batch_size = input_ids.size(0)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            task_type = task_types[i]\n",
    "            \n",
    "            # Forward pass pour cet exemple\n",
    "            if task_type == 'data_type_prediction':\n",
    "                logits = self.model(\n",
    "                    input_ids[i:i+1], \n",
    "                    attention_mask[i:i+1], \n",
    "                    task_type=task_type\n",
    "                )\n",
    "                \n",
    "                # Convertir le target en index de classe\n",
    "                target_text = self.tokenizer.decode(target_ids[i], skip_special_tokens=True)\n",
    "                if target_text in self.data_types:\n",
    "                    target_idx = self.data_types.index(target_text)\n",
    "                else:\n",
    "                    target_idx = 0  # Défaut: text\n",
    "                \n",
    "                target_tensor = torch.tensor([target_idx], device=self.device)\n",
    "                loss = F.cross_entropy(logits, target_tensor)\n",
    "            else:\n",
    "                # Tâches de génération\n",
    "                logits = self.model(\n",
    "                    input_ids[i:i+1], \n",
    "                    attention_mask[i:i+1], \n",
    "                    task_type=task_type\n",
    "                )\n",
    "                \n",
    "                # Utiliser seulement le premier token de la cible pour simplifier\n",
    "                target_token = target_ids[i, 0:1]  # Premier token non-padding\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, logits.size(-1)), \n",
    "                    target_token.view(-1),\n",
    "                    ignore_index=self.tokenizer.pad_token_id\n",
    "                )\n",
    "            \n",
    "            total_loss += loss\n",
    "        \n",
    "        return total_loss / batch_size\n",
    "    \n",
    "    def evaluate(self, val_loader):\n",
    "        \"\"\"Évalue le modèle\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                target_ids = batch['target_ids'].to(self.device)\n",
    "                task_types = batch['task_type']\n",
    "                \n",
    "                loss = self._compute_loss(input_ids, attention_mask, target_ids, task_types)\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        self.model.train()\n",
    "        return total_loss / len(val_loader)\n",
    "    \n",
    "    def predict(self, context: str, task_type: str) -> str:\n",
    "        \"\"\"Fait une prédiction\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        input_text = f\"[TASK:{task_type}] {context}\"\n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.model(\n",
    "                encoding['input_ids'],\n",
    "                encoding['attention_mask'],\n",
    "                task_type=task_type\n",
    "            )\n",
    "            \n",
    "            if task_type == 'data_type_prediction':\n",
    "                pred_idx = torch.argmax(logits, dim=-1).item()\n",
    "                return self.data_types[pred_idx]\n",
    "            else:\n",
    "                # Pour la génération, prendre le token le plus probable\n",
    "                pred_token_id = torch.argmax(logits, dim=-1).item()\n",
    "                return self.tokenizer.decode([pred_token_id], skip_special_tokens=True)\n",
    "\n",
    "def load_excel_data_from_folder(data_folder: str = 'data') -> List[Dict]:\n",
    "    \"\"\"Charge tous les fichiers JSON du dossier data\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Vérifier que le dossier existe\n",
    "    if not os.path.exists(data_folder):\n",
    "        print(f\"Le dossier '{data_folder}' n'existe pas!\")\n",
    "        return data\n",
    "    \n",
    "    # Rechercher tous les fichiers JSON dans le dossier\n",
    "    json_pattern = os.path.join(data_folder, '*.json')\n",
    "    json_files = glob.glob(json_pattern)\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"Aucun fichier JSON trouvé dans le dossier '{data_folder}'\")\n",
    "        return data\n",
    "    \n",
    "    print(f\"Fichiers JSON trouvés dans '{data_folder}':\")\n",
    "    for file_path in json_files:\n",
    "        print(f\"  - {os.path.basename(file_path)}\")\n",
    "    \n",
    "    # Charger chaque fichier JSON\n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                workbook_data = json.load(f)\n",
    "                data.append(workbook_data)\n",
    "                print(f\"✓ Chargé: {os.path.basename(file_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Erreur lors du chargement de {os.path.basename(file_path)}: {e}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_excel_data(json_files: List[str]) -> List[Dict]:\n",
    "    \"\"\"Charge les données Excel depuis une liste de fichiers JSON (fonction conservée pour compatibilité)\"\"\"\n",
    "    data = []\n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                workbook_data = json.load(f)\n",
    "                data.append(workbook_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement de {file_path}: {e}\")\n",
    "    return data\n",
    "\n",
    "def create_data_loaders(excel_data: List[Dict], tokenizer, batch_size: int = 8, test_size: float = 0.2):\n",
    "    \"\"\"Crée les DataLoaders pour l'entraînement et la validation\"\"\"\n",
    "    \n",
    "    # Split des données\n",
    "    train_data, val_data = train_test_split(excel_data, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Créer les datasets\n",
    "    train_dataset = ExcelDataset(train_data, tokenizer)\n",
    "    val_dataset = ExcelDataset(val_data, tokenizer)\n",
    "    \n",
    "    # Créer les dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: {\n",
    "            'input_ids': torch.stack([item['input_ids'] for item in x]),\n",
    "            'attention_mask': torch.stack([item['attention_mask'] for item in x]),\n",
    "            'target_ids': torch.stack([item['target_ids'] for item in x]),\n",
    "            'target_attention_mask': torch.stack([item['target_attention_mask'] for item in x]),\n",
    "            'task_type': [item['task_type'] for item in x]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: {\n",
    "            'input_ids': torch.stack([item['input_ids'] for item in x]),\n",
    "            'attention_mask': torch.stack([item['attention_mask'] for item in x]),\n",
    "            'target_ids': torch.stack([item['target_ids'] for item in x]),\n",
    "            'target_attention_mask': torch.stack([item['target_attention_mask'] for item in x]),\n",
    "            'task_type': [item['task_type'] for item in x]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Exemple d'utilisation\n",
    "def main():\n",
    "    \"\"\"Fonction principale pour lancer l'entraînement\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    MODEL_NAME = 'distilbert-base-uncased'\n",
    "    BATCH_SIZE = 4\n",
    "    EPOCHS = 3\n",
    "    LEARNING_RATE = 2e-5\n",
    "    \n",
    "    # Charger le tokenizer\n",
    "    print(\"Chargement du tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Ajouter des tokens spéciaux si nécessaire\n",
    "    special_tokens = ['[MASK]', '[SHEET:', ']', '[TASK:', '=']\n",
    "    tokenizer.add_tokens(special_tokens)\n",
    "    \n",
    "    # Charger les données depuis le dossier data\n",
    "    print(\"Chargement des données depuis le dossier 'data'...\")\n",
    "    excel_data = load_excel_data_from_folder('embedding/data')\n",
    "    \n",
    "    if len(excel_data) == 0:\n",
    "        print(\"Aucune donnée trouvée. Création de données d'exemple...\")\n",
    "        # Créer des données d'exemple\n",
    "        excel_data = [\n",
    "            {\n",
    "                'Sheet1': {\n",
    "                    'cells': {\n",
    "                        '1_1': {'value': 'Nom', 'type': 'text'},\n",
    "                        '1_2': {'value': 'Age', 'type': 'text'},\n",
    "                        '1_3': {'value': 'Salaire', 'type': 'text'},\n",
    "                        '2_1': {'value': 'Alice', 'type': 'text'},\n",
    "                        '2_2': {'value': '25', 'type': 'number'},\n",
    "                        '2_3': {'value': '50000', 'type': 'currency'},\n",
    "                        '3_1': {'value': 'Bob', 'type': 'text'},\n",
    "                        '3_2': {'value': '30', 'type': 'number'},\n",
    "                        '3_3': {'value': '60000', 'type': 'currency'},\n",
    "                        '4_1': {'value': 'Charlie', 'type': 'text'},\n",
    "                        '4_2': {'value': '35', 'type': 'number'},\n",
    "                        '4_3': {'value': '=B4*2000', 'formula': 'B4*2000', 'type': 'formula'},\n",
    "                    }\n",
    "                },\n",
    "                'Sheet2': {\n",
    "                    'cells': {\n",
    "                        '1_1': {'value': 'Produit', 'type': 'text'},\n",
    "                        '1_2': {'value': 'Prix', 'type': 'text'},\n",
    "                        '1_3': {'value': 'Quantité', 'type': 'text'},\n",
    "                        '1_4': {'value': 'Total', 'type': 'text'},\n",
    "                        '2_1': {'value': 'Pommes', 'type': 'text'},\n",
    "                        '2_2': {'value': '1.50', 'type': 'currency'},\n",
    "                        '2_3': {'value': '10', 'type': 'number'},\n",
    "                        '2_4': {'value': '=B2*C2', 'formula': 'B2*C2', 'type': 'formula'},\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'Feuille1': {\n",
    "                    'cells': {\n",
    "                        '1_1': {'value': 'Date', 'type': 'text'},\n",
    "                        '1_2': {'value': 'Ventes', 'type': 'text'},\n",
    "                        '2_1': {'value': '2024-01-01', 'type': 'date'},\n",
    "                        '2_2': {'value': '1000', 'type': 'number'},\n",
    "                        '3_1': {'value': '2024-01-02', 'type': 'date'},\n",
    "                        '3_2': {'value': '1200', 'type': 'number'},\n",
    "                        '4_1': {'value': '2024-01-03', 'type': 'date'},\n",
    "                        '4_2': {'value': '1100', 'type': 'number'},\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        print(\"Données d'exemple créées avec plusieurs feuilles et formules\")\n",
    "    else:\n",
    "        print(f\"Données chargées: {len(excel_data)} classeurs depuis le dossier 'data'\")\n",
    "    \n",
    "    # Créer les DataLoaders\n",
    "    print(\"Création des DataLoaders...\")\n",
    "    train_loader, val_loader = create_data_loaders(excel_data, tokenizer, BATCH_SIZE)\n",
    "    \n",
    "    # Initialiser le modèle\n",
    "    print(\"Initialisation du modèle...\")\n",
    "    model = ExcelTransformer(MODEL_NAME)\n",
    "    \n",
    "    # Redimensionner les embeddings si de nouveaux tokens ont été ajoutés\n",
    "    model.encoder.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Créer le trainer\n",
    "    trainer = ExcelTrainer(model, tokenizer)\n",
    "    \n",
    "    # Lancer l'entraînement\n",
    "    print(\"Début de l'entraînement...\")\n",
    "    trainer.train(train_loader, val_loader, epochs=EPOCHS, lr=LEARNING_RATE)\n",
    "    \n",
    "    # Sauvegarder le modèle\n",
    "    print(\"Sauvegarde du modèle...\")\n",
    "    torch.save(model.state_dict(), 'excel_transformer.pth')\n",
    "    tokenizer.save_pretrained('excel_tokenizer')\n",
    "    \n",
    "    print(\"Entraînement terminé!\")\n",
    "    \n",
    "    # Exemple de prédiction\n",
    "    print(\"\\nExemple de prédiction:\")\n",
    "    context = \"[SHEET:Sheet1] 1_1:Nom(text) 1_2:Age(text) 2_1:Alice(text) 2_2:[MASK]\"\n",
    "    prediction = trainer.predict(context, 'cell_completion')\n",
    "    print(f\"Contexte: {context}\")\n",
    "    print(f\"Prédiction: {prediction}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e9290f3-c705-430d-b6c8-0bdac51331aa",
   "metadata": {},
   "source": [
    "Interface d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da47f6c-a946-435b-85ab-3abf8114e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class ExcelMaskedEvaluator:\n",
    "    \"\"\"Évaluateur pour la tâche de masked prediction avec analyses détaillées\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model: 'MaskedCellPredictor',\n",
    "                 transformer_builder: 'JSONToGraphTransformer'):\n",
    "        self.model = model\n",
    "        self.transformer_builder = transformer_builder\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Stockage des résultats d'évaluation\n",
    "        self.evaluation_results = []\n",
    "        self.aggregated_metrics = defaultdict(list)\n",
    "        \n",
    "    def evaluate_excel_file(self, \n",
    "                           json_data: Dict[str, Any],\n",
    "                           strategies: List[str] = ['random', 'strategic'],\n",
    "                           num_candidates: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Évalue le modèle sur un fichier Excel complet\n",
    "        \n",
    "        Args:\n",
    "            json_data: Données Excel en JSON\n",
    "            strategies: Stratégies de masquage à tester\n",
    "            num_candidates: Nombre de candidats à générer\n",
    "            \n",
    "        Returns:\n",
    "            Résultats détaillés de l'évaluation\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'file_summary': {},\n",
    "            'strategy_results': {},\n",
    "            'cell_analysis': [],\n",
    "            'error_analysis': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Extraire et filtrer les cellules\n",
    "            cells = self.transformer_builder._extract_cells_from_json(json_data)\n",
    "            cells = self.transformer_builder._filter_cells(cells, max_total_cells=100)\n",
    "            \n",
    "            if not cells:\n",
    "                return {'error': 'Aucune cellule trouvée'}\n",
    "            \n",
    "            # Créer le graphe\n",
    "            graph = self.transformer_builder.graph_embedder(cells)\n",
    "            \n",
    "            # Résumé du fichier\n",
    "            results['file_summary'] = {\n",
    "                'total_cells': len(cells),\n",
    "                'num_nodes': graph.num_nodes,\n",
    "                'num_edges': graph.num_edges,\n",
    "                'sheets': list(set(cell.sheet_name for cell in cells)),\n",
    "                'cell_types': {\n",
    "                    'empty': sum(1 for c in cells if c.cell_type == 0),\n",
    "                    'text': sum(1 for c in cells if c.cell_type == 1),\n",
    "                    'number': sum(1 for c in cells if c.cell_type == 2),\n",
    "                    'formula': sum(1 for c in cells if c.cell_type == 3)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Tester chaque stratégie de masquage\n",
    "            for strategy in strategies:\n",
    "                strategy_result = self._evaluate_strategy(\n",
    "                    graph, cells, strategy, num_candidates\n",
    "                )\n",
    "                results['strategy_results'][strategy] = strategy_result\n",
    "                \n",
    "                # Ajouter à l'analyse par cellule\n",
    "                for cell_result in strategy_result['cell_predictions']:\n",
    "                    cell_result['strategy'] = strategy\n",
    "                    cell_result['file_id'] = id(json_data)\n",
    "                    results['cell_analysis'].append(cell_result)\n",
    "            \n",
    "            # Analyse des erreurs\n",
    "            results['error_analysis'] = self._analyze_errors(results['cell_analysis'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            results['error'] = str(e)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _evaluate_strategy(self, \n",
    "                          graph: 'ExcelGraph',\n",
    "                          cells: List['FullCellInfo'],\n",
    "                          strategy: str,\n",
    "                          num_candidates: int) -> Dict[str, Any]:\n",
    "        \"\"\"Évalue une stratégie de masquage spécifique\"\"\"\n",
    "        \n",
    "        # Choisir les cellules à masquer selon la stratégie\n",
    "        if strategy == 'random':\n",
    "            mask_indices = ExcelMaskingStrategy.random_masking(cells, mask_ratio=0.2)\n",
    "        elif strategy == 'strategic':\n",
    "            mask_indices = ExcelMaskingStrategy.strategic_masking(cells)\n",
    "        else:\n",
    "            mask_indices = [0]  # Par défaut\n",
    "        \n",
    "        if not mask_indices:\n",
    "            return {'error': 'Aucune cellule à masquer'}\n",
    "        \n",
    "        # Générer les candidats\n",
    "        candidates = []\n",
    "        ground_truth_info = []\n",
    "        \n",
    "        for mask_idx in mask_indices:\n",
    "            cell = cells[mask_idx]\n",
    "            cell_candidates = generate_candidates(cell, num_candidates)\n",
    "            \n",
    "            # Informations sur la vérité terrain\n",
    "            true_value = str(cell.raw_value) if cell.raw_value else str(cell.formula) if cell.formula else \"\"\n",
    "            true_position = -1\n",
    "            \n",
    "            # Trouver la position de la vraie valeur dans les candidats\n",
    "            for i, candidate in enumerate(cell_candidates):\n",
    "                if candidate == true_value:\n",
    "                    true_position = i\n",
    "                    break\n",
    "            \n",
    "            # Si pas trouvée, l'insérer à une position aléatoire pour le test\n",
    "            if true_position == -1 and true_value:\n",
    "                true_position = 0\n",
    "                cell_candidates[0] = true_value\n",
    "            \n",
    "            candidates.append(cell_candidates)\n",
    "            ground_truth_info.append({\n",
    "                'cell_index': mask_idx,\n",
    "                'true_value': true_value,\n",
    "                'true_position': true_position,\n",
    "                'cell_type': cell.cell_type,\n",
    "                'position': (cell.row, cell.col),\n",
    "                'sheet': cell.sheet_name\n",
    "            })\n",
    "        \n",
    "        # Prédiction du modèle\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model.predict_top_candidates(graph, mask_indices, candidates)\n",
    "        \n",
    "        # Calculer les métriques\n",
    "        metrics = self._calculate_metrics(predictions, ground_truth_info)\n",
    "        \n",
    "        # Analyser chaque prédiction\n",
    "        cell_predictions = []\n",
    "        for i, (prediction, gt_info) in enumerate(zip(predictions, ground_truth_info)):\n",
    "            cell_pred = self._analyze_cell_prediction(prediction, gt_info, candidates[i])\n",
    "            cell_predictions.append(cell_pred)\n",
    "        \n",
    "        return {\n",
    "            'strategy': strategy,\n",
    "            'num_masked': len(mask_indices),\n",
    "            'metrics': metrics,\n",
    "            'cell_predictions': cell_predictions\n",
    "        }\n",
    "    \n",
    "    def _calculate_metrics(self, \n",
    "                          predictions: List[Dict],\n",
    "                          ground_truth_info: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"Calcule les métriques de performance\"\"\"\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy_top1': 0.0,\n",
    "            'accuracy_top3': 0.0,\n",
    "            'accuracy_top5': 0.0,\n",
    "            'mrr': 0.0,  # Mean Reciprocal Rank\n",
    "            'avg_confidence': 0.0,\n",
    "            'by_type': {}\n",
    "        }\n",
    "        \n",
    "        if not predictions:\n",
    "            return metrics\n",
    "        \n",
    "        total = len(predictions)\n",
    "        top1_correct = 0\n",
    "        top3_correct = 0\n",
    "        top5_correct = 0\n",
    "        reciprocal_ranks = []\n",
    "        confidences = []\n",
    "        \n",
    "        # Métriques par type de cellule\n",
    "        type_metrics = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "        \n",
    "        for pred, gt_info in zip(predictions, ground_truth_info):\n",
    "            true_value = gt_info['true_value']\n",
    "            cell_type = gt_info['cell_type']\n",
    "            \n",
    "            # Trouver le rang de la vraie valeur\n",
    "            true_rank = None\n",
    "            for rank, candidate_info in enumerate(pred['candidates_ranked']):\n",
    "                if candidate_info['value'] == true_value:\n",
    "                    true_rank = rank + 1\n",
    "                    break\n",
    "            \n",
    "            if true_rank is not None:\n",
    "                # Accuracy\n",
    "                if true_rank == 1:\n",
    "                    top1_correct += 1\n",
    "                    type_metrics[cell_type]['correct'] += 1\n",
    "                if true_rank <= 3:\n",
    "                    top3_correct += 1\n",
    "                if true_rank <= 5:\n",
    "                    top5_correct += 1\n",
    "                \n",
    "                # MRR\n",
    "                reciprocal_ranks.append(1.0 / true_rank)\n",
    "            else:\n",
    "                reciprocal_ranks.append(0.0)\n",
    "            \n",
    "            type_metrics[cell_type]['total'] += 1\n",
    "            \n",
    "            # Confiance de la prédiction top-1\n",
    "            if pred['candidates_ranked']:\n",
    "                confidences.append(pred['candidates_ranked'][0]['probability'])\n",
    "        \n",
    "        # Calculer les métriques finales\n",
    "        metrics['accuracy_top1'] = top1_correct / total\n",
    "        metrics['accuracy_top3'] = top3_correct / total\n",
    "        metrics['accuracy_top5'] = top5_correct / total\n",
    "        metrics['mrr'] = np.mean(reciprocal_ranks)\n",
    "        metrics['avg_confidence'] = np.mean(confidences) if confidences else 0.0\n",
    "        \n",
    "        # Métriques par type\n",
    "        type_names = {0: 'empty', 1: 'text', 2: 'number', 3: 'formula'}\n",
    "        for cell_type, stats in type_metrics.items():\n",
    "            type_name = type_names.get(cell_type, f'type_{cell_type}')\n",
    "            if stats['total'] > 0:\n",
    "                metrics['by_type'][type_name] = stats['correct'] / stats['total']\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _analyze_cell_prediction(self, \n",
    "                                prediction: Dict,\n",
    "                                gt_info: Dict,\n",
    "                                candidates: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyse détaillée d'une prédiction de cellule\"\"\"\n",
    "        \n",
    "        true_value = gt_info['true_value']\n",
    "        cell_type = gt_info['cell_type']\n",
    "        \n",
    "        # Trouver la vraie valeur dans les prédictions\n",
    "        true_rank = None\n",
    "        true_confidence = 0.0\n",
    "        \n",
    "        for rank, candidate_info in enumerate(prediction['candidates_ranked']):\n",
    "            if candidate_info['value'] == true_value:\n",
    "                true_rank = rank + 1\n",
    "                true_confidence = candidate_info['probability']\n",
    "                break\n",
    "        \n",
    "        # Analyser la distribution des probabilités\n",
    "        probs = [c['probability'] for c in prediction['candidates_ranked']]\n",
    "        prob_analysis = {\n",
    "            'entropy': -sum(p * np.log(p + 1e-10) for p in probs),\n",
    "            'max_prob': max(probs),\n",
    "            'min_prob': min(probs),\n",
    "            'std_prob': np.std(probs)\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'cell_index': gt_info['cell_index'],\n",
    "            'position': gt_info['position'],\n",
    "            'sheet': gt_info['sheet'],\n",
    "            'cell_type': cell_type,\n",
    "            'true_value': true_value,\n",
    "            'predicted_value': prediction['candidates_ranked'][0]['value'],\n",
    "            'predicted_confidence': prediction['candidates_ranked'][0]['probability'],\n",
    "            'true_rank': true_rank,\n",
    "            'true_confidence': true_confidence,\n",
    "            'is_correct': true_rank == 1 if true_rank else False,\n",
    "            'prob_analysis': prob_analysis,\n",
    "            'all_candidates': candidates,\n",
    "            'top5_predictions': prediction['candidates_ranked'][:5]\n",
    "        }\n",
    "    \n",
    "    def _analyze_errors(self, cell_analyses: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyse des erreurs et patterns de performance\"\"\"\n",
    "        \n",
    "        error_analysis = {\n",
    "            'common_errors': defaultdict(int),\n",
    "            'error_by_type': defaultdict(int),\n",
    "            'error_by_position': defaultdict(int),\n",
    "            'confidence_errors': [],\n",
    "            'patterns': {}\n",
    "        }\n",
    "        \n",
    "        correct_predictions = []\n",
    "        incorrect_predictions = []\n",
    "        \n",
    "        for analysis in cell_analyses:\n",
    "            if analysis['is_correct']:\n",
    "                correct_predictions.append(analysis)\n",
    "            else:\n",
    "                incorrect_predictions.append(analysis)\n",
    "                \n",
    "                # Analyser les types d'erreurs\n",
    "                true_val = analysis['true_value']\n",
    "                pred_val = analysis['predicted_value']\n",
    "                cell_type = analysis['cell_type']\n",
    "                \n",
    "                error_analysis['error_by_type'][cell_type] += 1\n",
    "                \n",
    "                # Erreurs de confiance élevée (confiant mais faux)\n",
    "                if analysis['predicted_confidence'] > 0.7:\n",
    "                    error_analysis['confidence_errors'].append(analysis)\n",
    "                \n",
    "                # Patterns d'erreurs communes\n",
    "                if cell_type == 2:  # Nombres\n",
    "                    try:\n",
    "                        true_num = float(true_val) if true_val else 0\n",
    "                        pred_num = float(pred_val) if pred_val else 0\n",
    "                        if abs(true_num - pred_num) < 10:\n",
    "                            error_analysis['common_errors']['close_number'] += 1\n",
    "                        else:\n",
    "                            error_analysis['common_errors']['far_number'] += 1\n",
    "                    except:\n",
    "                        error_analysis['common_errors']['invalid_number'] += 1\n",
    "                \n",
    "                elif cell_type == 1:  # Texte\n",
    "                    if len(true_val) == len(pred_val):\n",
    "                        error_analysis['common_errors']['same_length_text'] += 1\n",
    "                    elif true_val.lower() in pred_val.lower() or pred_val.lower() in true_val.lower():\n",
    "                        error_analysis['common_errors']['partial_text_match'] += 1\n",
    "                    else:\n",
    "                        error_analysis['common_errors']['different_text'] += 1\n",
    "                \n",
    "                elif cell_type == 3:  # Formule\n",
    "                    if '=' in pred_val:\n",
    "                        error_analysis['common_errors']['wrong_formula'] += 1\n",
    "                    else:\n",
    "                        error_analysis['common_errors']['formula_as_value'] += 1\n",
    "        \n",
    "        # Patterns généraux\n",
    "        if correct_predictions and incorrect_predictions:\n",
    "            avg_conf_correct = np.mean([p['predicted_confidence'] for p in correct_predictions])\n",
    "            avg_conf_incorrect = np.mean([p['predicted_confidence'] for p in incorrect_predictions])\n",
    "            \n",
    "            error_analysis['patterns'] = {\n",
    "                'avg_confidence_correct': avg_conf_correct,\n",
    "                'avg_confidence_incorrect': avg_conf_incorrect,\n",
    "                'confidence_separation': avg_conf_correct - avg_conf_incorrect,\n",
    "                'total_errors': len(incorrect_predictions),\n",
    "                'error_rate': len(incorrect_predictions) / len(cell_analyses)\n",
    "            }\n",
    "        \n",
    "        return error_analysis\n",
    "    \n",
    "    def generate_evaluation_report(self, \n",
    "                                  json_files: List[Dict],\n",
    "                                  output_file: str = \"evaluation_report.html\") -> str:\n",
    "        \"\"\"Génère un rapport d'évaluation complet en HTML\"\"\"\n",
    "        \n",
    "        print(\"🔍 Génération du rapport d'évaluation...\")\n",
    "        \n",
    "        all_results = []\n",
    "        aggregated_metrics = defaultdict(list)\n",
    "        \n",
    "        # Évaluer tous les fichiers\n",
    "        for i, json_data in enumerate(json_files):\n",
    "            print(f\"  Évaluation {i+1}/{len(json_files)}\")\n",
    "            \n",
    "            result = self.evaluate_excel_file(json_data)\n",
    "            if 'error' not in result:\n",
    "                all_results.append(result)\n",
    "                \n",
    "                # Agréger les métriques\n",
    "                for strategy, strategy_result in result['strategy_results'].items():\n",
    "                    if 'metrics' in strategy_result:\n",
    "                        for metric, value in strategy_result['metrics'].items():\n",
    "                            if isinstance(value, (int, float)):\n",
    "                                aggregated_metrics[f\"{strategy}_{metric}\"].append(value)\n",
    "        \n",
    "        # Créer le rapport HTML\n",
    "        html_content = self._create_html_report(all_results, aggregated_metrics)\n",
    "        \n",
    "        # Sauvegarder\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"✅ Rapport sauvegardé: {output_file}\")\n",
    "        return output_file\n",
    "    \n",
    "    def _create_html_report(self, \n",
    "                           all_results: List[Dict],\n",
    "                           aggregated_metrics: Dict) -> str:\n",
    "        \"\"\"Crée le contenu HTML du rapport\"\"\"\n",
    "        \n",
    "        # Calculer les statistiques globales\n",
    "        total_files = len(all_results)\n",
    "        total_predictions = sum(\n",
    "            len(result['cell_analysis']) \n",
    "            for result in all_results\n",
    "        )\n",
    "        \n",
    "        # Métriques moyennes\n",
    "        avg_metrics = {}\n",
    "        for metric, values in aggregated_metrics.items():\n",
    "            if values:\n",
    "                avg_metrics[metric] = {\n",
    "                    'mean': np.mean(values),\n",
    "                    'std': np.std(values),\n",
    "                    'min': np.min(values),\n",
    "                    'max': np.max(values)\n",
    "                }\n",
    "        \n",
    "        html = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html lang=\"fr\">\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "            <title>Rapport d'Évaluation - Excel Masked Prediction</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "                .header {{ background: #2c3e50; color: white; padding: 20px; border-radius: 5px; }}\n",
    "                .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}\n",
    "                .metric {{ display: inline-block; margin: 10px; padding: 10px; background: #f8f9fa; border-radius: 3px; }}\n",
    "                .good {{ background: #d4edda; }}\n",
    "                .warning {{ background: #fff3cd; }}\n",
    "                .error {{ background: #f8d7da; }}\n",
    "                table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}\n",
    "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "                th {{ background-color: #f2f2f2; }}\n",
    "                .chart {{ margin: 20px 0; text-align: center; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"header\">\n",
    "                <h1>📊 Rapport d'Évaluation - Excel Masked Prediction</h1>\n",
    "                <p>Analyse de performance du transformer sur {total_files} fichiers Excel</p>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>📈 Résumé Exécutif</h2>\n",
    "                <div class=\"metric good\">\n",
    "                    <strong>Fichiers analysés:</strong> {total_files}\n",
    "                </div>\n",
    "                <div class=\"metric good\">\n",
    "                    <strong>Prédictions totales:</strong> {total_predictions}\n",
    "                </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ajouter les métriques principales\n",
    "        for strategy in ['random', 'strategic']:\n",
    "            acc_key = f\"{strategy}_accuracy_top1\"\n",
    "            if acc_key in avg_metrics:\n",
    "                acc = avg_metrics[acc_key]['mean']\n",
    "                css_class = \"good\" if acc > 0.7 else \"warning\" if acc > 0.4 else \"error\"\n",
    "                html += f\"\"\"\n",
    "                <div class=\"metric {css_class}\">\n",
    "                    <strong>Accuracy {strategy}:</strong> {acc:.1%} ± {avg_metrics[acc_key]['std']:.1%}\n",
    "                </div>\n",
    "                \"\"\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>📊 Métriques Détaillées</h2>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Métrique</th>\n",
    "                        <th>Stratégie</th>\n",
    "                        <th>Moyenne</th>\n",
    "                        <th>Écart-type</th>\n",
    "                        <th>Min</th>\n",
    "                        <th>Max</th>\n",
    "                    </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Table des métriques\n",
    "        metric_names = {\n",
    "            'accuracy_top1': 'Accuracy Top-1',\n",
    "            'accuracy_top3': 'Accuracy Top-3', \n",
    "            'accuracy_top5': 'Accuracy Top-5',\n",
    "            'mrr': 'Mean Reciprocal Rank',\n",
    "            'avg_confidence': 'Confiance Moyenne'\n",
    "        }\n",
    "        \n",
    "        for strategy in ['random', 'strategic']:\n",
    "            for metric, display_name in metric_names.items():\n",
    "                key = f\"{strategy}_{metric}\"\n",
    "                if key in avg_metrics:\n",
    "                    stats = avg_metrics[key]\n",
    "                    html += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td>{display_name}</td>\n",
    "                        <td>{strategy.title()}</td>\n",
    "                        <td>{stats['mean']:.3f}</td>\n",
    "                        <td>{stats['std']:.3f}</td>\n",
    "                        <td>{stats['min']:.3f}</td>\n",
    "                        <td>{stats['max']:.3f}</td>\n",
    "                    </tr>\n",
    "                    \"\"\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "                </table>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>🔍 Analyse des Erreurs</h2>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Analyse des erreurs les plus communes\n",
    "        all_errors = defaultdict(int)\n",
    "        for result in all_results:\n",
    "            if 'error_analysis' in result:\n",
    "                for error_type, count in result['error_analysis']['common_errors'].items():\n",
    "                    all_errors[error_type] += count\n",
    "        \n",
    "        if all_errors:\n",
    "            html += \"<h3>Erreurs les plus fréquentes:</h3><ul>\"\n",
    "            sorted_errors = sorted(all_errors.items(), key=lambda x: x[1], reverse=True)\n",
    "            for error_type, count in sorted_errors[:5]:\n",
    "                html += f\"<li><strong>{error_type}:</strong> {count} occurrences</li>\"\n",
    "            html += \"</ul>\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>📋 Exemples de Prédictions</h2>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Quelques exemples de prédictions\n",
    "        example_count = 0\n",
    "        for result in all_results[:3]:  # Premiers 3 fichiers\n",
    "            if 'cell_analysis' in result:\n",
    "                html += f\"<h3>Fichier {example_count + 1}:</h3>\"\n",
    "                for cell in result['cell_analysis'][:2]:  # 2 cellules par fichier\n",
    "                    status = \"✅ Correct\" if cell['is_correct'] else \"❌ Incorrect\"\n",
    "                    html += f\"\"\"\n",
    "                    <div style=\"margin: 10px 0; padding: 10px; border-left: 3px solid {'green' if cell['is_correct'] else 'red'};\">\n",
    "                        <strong>{status}</strong> - Cellule ({cell['position'][0]}, {cell['position'][1]})<br>\n",
    "                        <strong>Vraie valeur:</strong> \"{cell['true_value']}\"<br>\n",
    "                        <strong>Prédiction:</strong> \"{cell['predicted_value']}\" ({cell['predicted_confidence']:.1%})<br>\n",
    "                        <strong>Type:</strong> {['Vide', 'Texte', 'Nombre', 'Formule'][cell['cell_type']]}\n",
    "                    </div>\n",
    "                    \"\"\"\n",
    "                example_count += 1\n",
    "                if example_count >= 3:\n",
    "                    break\n",
    "        \n",
    "        html += \"\"\"\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>🎯 Recommandations</h2>\n",
    "                <ul>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Générer des recommandations basées sur les résultats\n",
    "        recommendations = []\n",
    "        \n",
    "        # Vérifier la performance globale\n",
    "        if 'random_accuracy_top1' in avg_metrics:\n",
    "            acc = avg_metrics['random_accuracy_top1']['mean']\n",
    "            if acc < 0.5:\n",
    "                recommendations.append(\"🔴 Performance faible (<50%): Augmenter la taille du modèle ou améliorer les données d'entraînement\")\n",
    "            elif acc < 0.7:\n",
    "                recommendations.append(\"🟡 Performance modérée: Optimiser l'architecture ou les hyperparamètres\")\n",
    "            else:\n",
    "                recommendations.append(\"🟢 Performance satisfaisante: Continuer l'entraînement pour améliorer la stabilité\")\n",
    "        \n",
    "        # Vérifier la différence entre stratégies\n",
    "        if ('random_accuracy_top1' in avg_metrics and \n",
    "            'strategic_accuracy_top1' in avg_metrics):\n",
    "            diff = (avg_metrics['strategic_accuracy_top1']['mean'] - \n",
    "                   avg_metrics['random_accuracy_top1']['mean'])\n",
    "            if abs(diff) < 0.05:\n",
    "                recommendations.append(\"⚪ Peu de différence entre stratégies: Le modèle pourrait bénéficier d'un meilleur encodage du contexte\")\n",
    "        \n",
    "        # Analyser la confiance\n",
    "        if 'random_avg_confidence' in avg_metrics:\n",
    "            conf = avg_metrics['random_avg_confidence']['mean']\n",
    "            if conf < 0.3:\n",
    "                recommendations.append(\"🔵 Confiance faible: Revoir la calibration du modèle\")\n",
    "            elif conf > 0.9:\n",
    "                recommendations.append(\"🟠 Confiance très élevée: Risque de sur-confiance, vérifier la diversité des données\")\n",
    "        \n",
    "        if not recommendations:\n",
    "            recommendations.append(\"✨ Aucune recommandation spécifique - Continuer le monitoring\")\n",
    "        \n",
    "        for rec in recommendations:\n",
    "            html += f\"<li>{rec}</li>\"\n",
    "        \n",
    "        html += f\"\"\"\n",
    "                </ul>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>ℹ️ Informations Techniques</h2>\n",
    "                <p><strong>Modèle:</strong> Excel Graph Transformer avec Masked Cell Prediction</p>\n",
    "                <p><strong>Date d'évaluation:</strong> {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "                <p><strong>Nombre de paramètres:</strong> {sum(p.numel() for p in self.model.parameters()):,}</p>\n",
    "                <p><strong>Stratégies testées:</strong> Random masking, Strategic masking</p>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        return html\n",
    "    \n",
    "    def interactive_test(self, json_data: Dict[str, Any]):\n",
    "        \"\"\"Test interactif sur un fichier Excel\"\"\"\n",
    "        \n",
    "        print(\"\\n🎮 MODE INTERACTIF - Test de Prédiction\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Préparer les données\n",
    "            cells = self.transformer_builder._extract_cells_from_json(json_data)\n",
    "            cells = self.transformer_builder._filter_cells(cells, max_total_cells=50)\n",
    "            \n",
    "            if not cells:\n",
    "                print(\"❌ Aucune cellule trouvée dans le fichier\")\n",
    "                return\n",
    "            \n",
    "            graph = self.transformer_builder.graph_embedder(cells)\n",
    "            \n",
    "            print(f\"📊 Fichier chargé: {graph.num_nodes} cellules, {graph.num_edges} relations\")\n",
    "            print(f\"📋 Feuilles: {', '.join(set(cell.sheet_name for cell in cells))}\")\n",
    "            \n",
    "            # Afficher les cellules disponibles\n",
    "            print(\"\\n📱 Cellules disponibles:\")\n",
    "            for i, cell in enumerate(cells[:10]):  # Afficher les 10 premières\n",
    "                content = str(cell.raw_value) if cell.raw_value else str(cell.formula) if cell.formula else \"[vide]\"\n",
    "                type_name = ['Vide', 'Texte', 'Nombre', 'Formule'][cell.cell_type]\n",
    "                print(f\"  {i:2d}. ({cell.row:2d},{cell.col:2d}) {type_name:8s} | {content[:30]}\")\n",
    "            \n",
    "            if len(cells) > 10:\n",
    "                print(f\"  ... et {len(cells) - 10} autres cellules\")\n",
    "            \n",
    "            # Interface interactive\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                choice = input(\"Choisir une action:\\n\"\n",
    "                             \"  1. Masquer une cellule spécifique\\n\"\n",
    "                             \"  2. Masquage aléatoire\\n\"\n",
    "                             \"  3. Masquage stratégique\\n\"\n",
    "                             \"  4. Quitter\\n\"\n",
    "                             \"Votre choix (1-4): \").strip()\n",
    "                \n",
    "                if choice == '4':\n",
    "                    print(\"👋 Au revoir !\")\n",
    "                    break\n",
    "                \n",
    "                elif choice == '1':\n",
    "                    try:\n",
    "                        cell_idx = int(input(f\"Index de la cellule à masquer (0-{len(cells)-1}): \"))\n",
    "                        if 0 <= cell_idx < len(cells):\n",
    "                            mask_indices = [cell_idx]\n",
    "                        else:\n",
    "                            print(\"❌ Index invalide\")\n",
    "                            continue\n",
    "                    except ValueError:\n",
    "                        print(\"❌ Veuillez entrer un nombre\")\n",
    "                        continue\n",
    "                \n",
    "                elif choice == '2':\n",
    "                    mask_indices = ExcelMaskingStrategy.random_masking(cells, mask_ratio=0.1)\n",
    "                    print(f\"🎲 Masquage aléatoire: cellules {mask_indices}\")\n",
    "                \n",
    "                elif choice == '3':\n",
    "                    mask_indices = ExcelMaskingStrategy.strategic_masking(cells)\n",
    "                    print(f\"🎯 Masquage stratégique: cellules {mask_indices}\")\n",
    "                \n",
    "                else:\n",
    "                    print(\"❌ Choix invalide\")\n",
    "                    continue\n",
    "                \n",
    "                if not mask_indices:\n",
    "                    print(\"❌ Aucune cellule à masquer\")\n",
    "                    continue\n",
    "                \n",
    "                # Effectuer la prédiction\n",
    "                candidates = [generate_candidates(cells[idx], 10) for idx in mask_indices]\n",
    "                predictions = self.model.predict_top_candidates(graph, mask_indices, candidates)\n",
    "                \n",
    "                # Afficher les résultats\n",
    "                print(f\"\\n🔮 RÉSULTATS DE PRÉDICTION\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                for i, (mask_idx, prediction) in enumerate(zip(mask_indices, predictions)):\n",
    "                    cell = cells[mask_idx]\n",
    "                    true_value = str(cell.raw_value) if cell.raw_value else str(cell.formula) if cell.formula else \"[vide]\"\n",
    "                    \n",
    "                    print(f\"\\n📍 Cellule {mask_idx} - Position ({cell.row},{cell.col})\")\n",
    "                    print(f\"   Type: {['Vide', 'Texte', 'Nombre', 'Formule'][cell.cell_type]}\")\n",
    "                    print(f\"   Vraie valeur: '{true_value}'\")\n",
    "                    print(f\"   Top 5 prédictions:\")\n",
    "                    \n",
    "                    for rank, candidate in enumerate(prediction['candidates_ranked'][:5]):\n",
    "                        marker = \"🎯\" if candidate['value'] == true_value else f\"{rank+1}.\"\n",
    "                        confidence = candidate['probability']\n",
    "                        bar_length = int(confidence * 20)\n",
    "                        bar = \"█\" * bar_length + \"░\" * (20 - bar_length)\n",
    "                        \n",
    "                        print(f\"     {marker:3s} {candidate['value'][:25]:25s} │{bar}│ {confidence:.1%}\")\n",
    "                    \n",
    "                    # Trouver le rang de la vraie valeur\n",
    "                    true_rank = None\n",
    "                    for rank, candidate in enumerate(prediction['candidates_ranked']):\n",
    "                        if candidate['value'] == true_value:\n",
    "                            true_rank = rank + 1\n",
    "                            break\n",
    "                    \n",
    "                    if true_rank:\n",
    "                        if true_rank == 1:\n",
    "                            print(f\"   ✅ Prédiction correcte ! (rang {true_rank})\")\n",
    "                        elif true_rank <= 3:\n",
    "                            print(f\"   🟡 Dans le top 3 (rang {true_rank})\")\n",
    "                        elif true_rank <= 5:\n",
    "                            print(f\"   🟠 Dans le top 5 (rang {true_rank})\")\n",
    "                        else:\n",
    "                            print(f\"   ❌ Hors du top 5 (rang {true_rank})\")\n",
    "                    else:\n",
    "                        print(f\"   ❌ Vraie valeur non trouvée dans les candidats\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur: {e}\")\n",
    "\n",
    "# Fonction utilitaire pour lancer une évaluation complète\n",
    "def run_complete_evaluation(json_files: List[Dict], \n",
    "                           model: 'MaskedCellPredictor',\n",
    "                           transformer_builder: 'JSONToGraphTransformer') -> str:\n",
    "    \"\"\"Lance une évaluation complète et génère le rapport\"\"\"\n",
    "    \n",
    "    evaluator = ExcelMaskedEvaluator(model, transformer_builder)\n",
    "    \n",
    "    # Générer le rapport\n",
    "    report_file = evaluator.generate_evaluation_report(json_files)\n",
    "    \n",
    "    # Afficher un résumé\n",
    "    print(\"\\n📊 RÉSUMÉ DE L'ÉVALUATION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    sample_result = evaluator.evaluate_excel_file(json_files[0] if json_files else {})\n",
    "    if 'error' not in sample_result and 'strategy_results' in sample_result:\n",
    "        for strategy, result in sample_result['strategy_results'].items():\n",
    "            if 'metrics' in result:\n",
    "                metrics = result['metrics']\n",
    "                print(f\"\\n{strategy.upper()} MASKING:\")\n",
    "                print(f\"  Accuracy Top-1: {metrics.get('accuracy_top1', 0):.1%}\")\n",
    "                print(f\"  Accuracy Top-3: {metrics.get('accuracy_top3', 0):.1%}\")\n",
    "                print(f\"  MRR: {metrics.get('mrr', 0):.3f}\")\n",
    "                print(f\"  Confiance moy.: {metrics.get('avg_confidence', 0):.1%}\")\n",
    "    \n",
    "    return report_file\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"📋 Module d'évaluation Excel Masked Prediction chargé\")\n",
    "    print(\"Utilisez run_complete_evaluation() pour une évaluation complète\")\n",
    "    print(\"Ou ExcelMaskedEvaluator.interactive_test() pour un test interactif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
